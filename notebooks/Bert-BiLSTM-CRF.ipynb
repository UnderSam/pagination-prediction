{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "from urllib.parse import urlparse, urlsplit, parse_qs, parse_qsl\n",
    "\n",
    "import numpy as np\n",
    "import parsel\n",
    "from sklearn_crfsuite.metrics import flat_classification_report, sequence_accuracy_score\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from autopager.storage import Storage\n",
    "from autopager.htmlutils import (get_link_text, get_text_around_selector_list,\n",
    "                                 get_link_href, get_selector_root)\n",
    "from autopager.utils import (\n",
    "    get_domain, normalize_whitespaces, normalize, ngrams, tokenize, ngrams_wb, replace_digits\n",
    ")\n",
    "from autopager.model import link_to_features, _num_tokens_feature, _elem_attr\n",
    "from autopager import AUTOPAGER_LIMITS\n",
    "from autopager.parserutils import (TagParser, MyHTMLParser, draw_scaled_page, position_check, compare_tag, get_first_tag)\n",
    "parser = MyHTMLParser()\n",
    "tagParser = TagParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus)!=0:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPUs visible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = Storage()\n",
    "urls = [rec['Page URL'] for rec in storage.iter_records(contain_button = True, file_type='T')]\n",
    "groups = [get_domain(url) for url in urls]\n",
    "train_groups_set = set(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: Get Page 1 (Encoding: UTF-8)records ... (len: 303)\n",
      "Finish: Get Page 2 (Encoding: UTF-8)records ... (len: 243)\n",
      "Finish: Get Page 3 (Encoding: UTF-8)records ... (len: 119)\n",
      "Finish: Get Page 4 (Encoding: UTF-8)records ... (len: 944)\n",
      "Finish: Get Page 5 (Encoding: UTF-8)records ... (len: 93)\n",
      "Finish: Get Page 6 (Encoding: UTF-8)records ... (len: 994)\n",
      "Finish: Get Page 7 (Encoding: UTF-8)records ... (len: 1014)\n",
      "Finish: Get Page 8 (Encoding: UTF-8)records ... (len: 7)\n",
      "Finish: Get Page 9 (Encoding: UTF-8)records ... (len: 288)\n",
      "Finish: Get Page 10 (Encoding: UTF-8)records ... (len: 678)\n",
      "Finish: Get Page 11 (Encoding: UTF-8)records ... (len: 789)\n",
      "Finish: Get Page 12 (Encoding: UTF-8)records ... (len: 814)\n",
      "Finish: Get Page 13 (Encoding: UTF-8)records ... (len: 814)\n",
      "Finish: Get Page 14 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 15 (Encoding: UTF-8)records ... (len: 168)\n",
      "Finish: Get Page 16 (Encoding: UTF-8)records ... (len: 91)\n",
      "Finish: Get Page 17 (Encoding: UTF-8)records ... (len: 100)\n",
      "Finish: Get Page 18 (Encoding: UTF-8)records ... (len: 102)\n",
      "Finish: Get Page 19 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 20 (Encoding: UTF-8)records ... (len: 215)\n",
      "Finish: Get Page 21 (Encoding: UTF-8)records ... (len: 158)\n",
      "Finish: Get Page 22 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 23 (Encoding: UTF-8)records ... (len: 181)\n",
      "Finish: Get Page 24 (Encoding: UTF-8)records ... (len: 10)\n",
      "Finish: Get Page 25 (Encoding: UTF-8)records ... (len: 165)\n",
      "Finish: Get Page 26 (Encoding: UTF-8)records ... (len: 147)\n",
      "Finish: Get Page 27 (Encoding: UTF-8)records ... (len: 329)\n",
      "Finish: Get Page 28 (Encoding: UTF-8)records ... (len: 268)\n",
      "Finish: Get Page 29 (Encoding: UTF-8)records ... (len: 643)\n",
      "Finish: Get Page 30 (Encoding: UTF-8)records ... (len: 645)\n",
      "Finish: Get Page 31 (Encoding: UTF-8)records ... (len: 647)\n",
      "Finish: Get Page 32 (Encoding: UTF-8)records ... (len: 625)\n",
      "Finish: Get Page 33 (Encoding: UTF-8)records ... (len: 108)\n",
      "Finish: Get Page 34 (Encoding: UTF-8)records ... (len: 109)\n",
      "Finish: Get Page 35 (Encoding: UTF-8)records ... (len: 84)\n",
      "Finish: Get Page 36 (Encoding: UTF-8)records ... (len: 718)\n",
      "Finish: Get Page 37 (Encoding: UTF-8)records ... (len: 723)\n",
      "Finish: Get Page 38 (Encoding: UTF-8)records ... (len: 703)\n",
      "Finish: Get Page 39 (Encoding: UTF-8)records ... (len: 70)\n",
      "Finish: Get Page 40 (Encoding: UTF-8)records ... (len: 102)\n",
      "Finish: Get Page 41 (Encoding: UTF-8)records ... (len: 75)\n",
      "Finish: Get Page 42 (Encoding: UTF-8)records ... (len: 52)\n",
      "Finish: Get Page 43 (Encoding: UTF-8)records ... (len: 198)\n",
      "Finish: Get Page 44 (Encoding: UTF-8)records ... (len: 200)\n",
      "Finish: Get Page 45 (Encoding: UTF-8)records ... (len: 206)\n",
      "Finish: Get Page 46 (Encoding: UTF-8)records ... (len: 82)\n",
      "Finish: Get Page 47 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 48 (Encoding: UTF-8)records ... (len: 34)\n",
      "Finish: Get Page 49 (Encoding: UTF-8)records ... (len: 62)\n",
      "Finish: Get Page 50 (Encoding: UTF-8)records ... (len: 15)\n",
      "Finish: Get Page 51 (Encoding: UTF-8)records ... (len: 266)\n",
      "Finish: Get Page 52 (Encoding: UTF-8)records ... (len: 228)\n",
      "Finish: Get Page 53 (Encoding: UTF-8)records ... (len: 273)\n",
      "Finish: Get Page 54 (Encoding: cp1252)records ... (len: 283)\n",
      "Finish: Get Page 55 (Encoding: cp1252)records ... (len: 285)\n",
      "Finish: Get Page 56 (Encoding: cp1252)records ... (len: 520)\n",
      "Finish: Get Page 57 (Encoding: cp1252)records ... (len: 463)\n",
      "Finish: Get Page 58 (Encoding: UTF-8)records ... (len: 130)\n",
      "Finish: Get Page 59 (Encoding: UTF-8)records ... (len: 58)\n",
      "Finish: Get Page 60 (Encoding: UTF-8)records ... (len: 54)\n",
      "Finish: Get Page 61 (Encoding: UTF-8)records ... (len: 55)\n",
      "Finish: Get Page 62 (Encoding: UTF-8)records ... (len: 55)\n",
      "Finish: Get Page 63 (Encoding: UTF-8)records ... (len: 84)\n",
      "Finish: Get Page 64 (Encoding: UTF-8)records ... (len: 95)\n",
      "Finish: Get Page 65 (Encoding: iso8859-15)records ... (len: 702)\n",
      "Finish: Get Page 66 (Encoding: iso8859-15)records ... (len: 535)\n",
      "Finish: Get Page 67 (Encoding: iso8859-15)records ... (len: 538)\n",
      "Finish: Get Page 68 (Encoding: UTF-8)records ... (len: 37)\n",
      "Finish: Get Page 69 (Encoding: UTF-8)records ... (len: 312)\n",
      "Finish: Get Page 70 (Encoding: UTF-8)records ... (len: 127)\n",
      "Finish: Get Page 71 (Encoding: UTF-8)records ... (len: 104)\n",
      "Finish: Get Page 72 (Encoding: UTF-8)records ... (len: 92)\n",
      "Finish: Get Page 73 (Encoding: UTF-8)records ... (len: 149)\n",
      "Finish: Get Page 74 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 75 (Encoding: UTF-8)records ... (len: 386)\n",
      "Finish: Get Page 76 (Encoding: UTF-8)records ... (len: 319)\n",
      "Finish: Get Page 77 (Encoding: UTF-8)records ... (len: 114)\n",
      "Finish: Get Page 78 (Encoding: UTF-8)records ... (len: 118)\n",
      "Finish: Get Page 79 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 80 (Encoding: cp1251)records ... (len: 266)\n",
      "Finish: Get Page 81 (Encoding: cp1251)records ... (len: 274)\n",
      "Finish: Get Page 82 (Encoding: UTF-8)records ... (len: 139)\n",
      "Finish: Get Page 83 (Encoding: UTF-8)records ... (len: 408)\n",
      "Finish: Get Page 84 (Encoding: UTF-8)records ... (len: 94)\n",
      "Finish: Get Page 85 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 86 (Encoding: UTF-8)records ... (len: 230)\n",
      "Finish: Get Page 87 (Encoding: UTF-8)records ... (len: 106)\n",
      "Finish: Get Page 88 (Encoding: UTF-8)records ... (len: 110)\n",
      "Finish: Get Page 89 (Encoding: UTF-8)records ... (len: 448)\n",
      "Finish: Get Page 90 (Encoding: UTF-8)records ... (len: 452)\n",
      "Finish: Get Page 91 (Encoding: UTF-8)records ... (len: 2389)\n",
      "Finish: Get Page 92 (Encoding: UTF-8)records ... (len: 2379)\n",
      "Finish: Get Page 93 (Encoding: UTF-8)records ... (len: 160)\n",
      "Finish: Get Page 94 (Encoding: UTF-8)records ... (len: 74)\n",
      "Finish: Get Page 95 (Encoding: UTF-8)records ... (len: 143)\n",
      "Finish: Get Page 96 (Encoding: UTF-8)records ... (len: 133)\n",
      "Finish: Get Page 97 (Encoding: UTF-8)records ... (len: 163)\n",
      "Finish: Get Page 98 (Encoding: UTF-8)records ... (len: 378)\n",
      "Finish: Get Page 99 (Encoding: UTF-8)records ... (len: 120)\n",
      "Finish: Get Page 100 (Encoding: UTF-8)records ... (len: 124)\n",
      "Finish: Get Page 101 (Encoding: UTF-8)records ... (len: 122)\n",
      "Finish: Get Page 102 (Encoding: UTF-8)records ... (len: 430)\n",
      "Finish: Get Page 103 (Encoding: UTF-8)records ... (len: 260)\n",
      "Finish: Get Page 104 (Encoding: UTF-8)records ... (len: 252)\n",
      "Finish: Get Page 105 (Encoding: UTF-8)records ... (len: 439)\n",
      "Finish: Get Page 106 (Encoding: UTF-8)records ... (len: 422)\n",
      "Finish: Get Page 107 (Encoding: UTF-8)records ... (len: 423)\n",
      "Finish: Get Page 108 (Encoding: UTF-8)records ... (len: 233)\n",
      "Finish: Get Page 109 (Encoding: cp1252)records ... (len: 155)\n",
      "Finish: Get Page 110 (Encoding: cp1252)records ... (len: 161)\n",
      "Finish: Get Page 111 (Encoding: cp1252)records ... (len: 131)\n",
      "Finish: Get Page 112 (Encoding: cp1252)records ... (len: 131)\n",
      "Finish: Get Page 113 (Encoding: UTF-8)records ... (len: 72)\n",
      "Finish: Get Page 114 (Encoding: UTF-8)records ... (len: 70)\n",
      "Finish: Get Page 115 (Encoding: UTF-8)records ... (len: 126)\n",
      "Finish: Get Page 116 (Encoding: UTF-8)records ... (len: 90)\n",
      "Finish: Get Page 117 (Encoding: UTF-8)records ... (len: 89)\n",
      "Finish: Get Page 118 (Encoding: UTF-8)records ... (len: 79)\n",
      "Finish: Get Page 119 (Encoding: UTF-8)records ... (len: 133)\n",
      "Finish: Get Page 120 (Encoding: UTF-8)records ... (len: 89)\n",
      "Finish: Get Page 121 (Encoding: UTF-8)records ... (len: 81)\n",
      "Finish: Get Page 122 (Encoding: UTF-8)records ... (len: 167)\n",
      "Finish: Get Page 123 (Encoding: UTF-8)records ... (len: 165)\n",
      "Finish: Get Page 124 (Encoding: UTF-8)records ... (len: 124)\n",
      "Finish: Get Page 125 (Encoding: UTF-8)records ... (len: 16)\n",
      "Finish: Get Page 126 (Encoding: UTF-8)records ... (len: 8)\n",
      "Finish: Get Page 127 (Encoding: UTF-8)records ... (len: 154)\n",
      "Finish: Get Page 128 (Encoding: UTF-8)records ... (len: 51)\n",
      "Finish: Get Page 129 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 130 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 131 (Encoding: UTF-8)records ... (len: 53)\n",
      "Finish: Get Page 132 (Encoding: UTF-8)records ... (len: 306)\n",
      "Finish: Get Page 133 (Encoding: UTF-8)records ... (len: 309)\n",
      "Finish: Get Page 134 (Encoding: UTF-8)records ... (len: 103)\n",
      "Finish: Get Page 135 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 136 (Encoding: UTF-8)records ... (len: 159)\n",
      "Finish: Get Page 137 (Encoding: UTF-8)records ... (len: 35)\n",
      "Finish: Get Page 138 (Encoding: UTF-8)records ... (len: 112)\n",
      "Finish: Get Page 139 (Encoding: UTF-8)records ... (len: 117)\n",
      "Finish: Get Page 140 (Encoding: UTF-8)records ... (len: 142)\n",
      "Finish: Get Page 141 (Encoding: UTF-8)records ... (len: 116)\n",
      "Finish: Get Page 142 (Encoding: UTF-8)records ... (len: 31)\n",
      "Finish: Get Page 143 (Encoding: cp1252)records ... (len: 84)\n",
      "Finish: Get Page 144 (Encoding: cp1252)records ... (len: 134)\n",
      "Finish: Get Page 145 (Encoding: cp1252)records ... (len: 139)\n",
      "Finish: Get Page 146 (Encoding: cp1252)records ... (len: 95)\n",
      "Finish: Get Page 147 (Encoding: cp1252)records ... (len: 130)\n",
      "Finish: Get Page 148 (Encoding: UTF-8)records ... (len: 478)\n",
      "Finish: Get Page 149 (Encoding: UTF-8)records ... (len: 365)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: Get Page 150 (Encoding: UTF-8)records ... (len: 368)\n",
      "Finish: Get Page 151 (Encoding: UTF-8)records ... (len: 369)\n",
      "Finish: Get Page 152 (Encoding: cp1252)records ... (len: 294)\n",
      "Finish: Get Page 153 (Encoding: UTF-8)records ... (len: 271)\n",
      "Finish: Get Page 154 (Encoding: UTF-8)records ... (len: 300)\n",
      "Finish: Get Page 155 (Encoding: UTF-8)records ... (len: 314)\n",
      "Finish: Get Page 156 (Encoding: UTF-8)records ... (len: 278)\n",
      "Finish: Get Page 157 (Encoding: UTF-8)records ... (len: 288)\n",
      "Finish: Get Page 158 (Encoding: UTF-8)records ... (len: 178)\n",
      "Finish: Get Page 159 (Encoding: UTF-8)records ... (len: 108)\n",
      "Finish: Get Page 160 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 161 (Encoding: UTF-8)records ... (len: 101)\n",
      "Finish: Get Page 162 (Encoding: UTF-8)records ... (len: 308)\n",
      "Finish: Get Page 163 (Encoding: UTF-8)records ... (len: 298)\n",
      "Finish: Get Page 164 (Encoding: UTF-8)records ... (len: 285)\n",
      "Finish: Get Page 165 (Encoding: UTF-8)records ... (len: 221)\n",
      "Finish: Get Page 166 (Encoding: UTF-8)records ... (len: 21)\n",
      "Finish: Get Page 167 (Encoding: UTF-8)records ... (len: 94)\n",
      "Finish: Get Page 168 (Encoding: UTF-8)records ... (len: 99)\n",
      "Finish: Get Page 169 (Encoding: UTF-8)records ... (len: 103)\n",
      "Finish: Get Page 170 (Encoding: UTF-8)records ... (len: 210)\n",
      "Finish: Get Page 171 (Encoding: UTF-8)records ... (len: 208)\n",
      "Finish: Get Page 172 (Encoding: UTF-8)records ... (len: 179)\n",
      "Finish: Get Page 173 (Encoding: UTF-8)records ... (len: 461)\n",
      "Finish: Get Page 174 (Encoding: UTF-8)records ... (len: 340)\n",
      "Finish: Get Page 175 (Encoding: UTF-8)records ... (len: 188)\n",
      "Finish: Get Page 176 (Encoding: UTF-8)records ... (len: 195)\n",
      "Finish: Get Page 177 (Encoding: UTF-8)records ... (len: 40)\n",
      "Finish: Get Page 178 (Encoding: UTF-8)records ... (len: 256)\n",
      "Finish: Get Page 179 (Encoding: UTF-8)records ... (len: 258)\n",
      "Finish: Get Page 180 (Encoding: UTF-8)records ... (len: 256)\n",
      "Finish: Get Page 181 (Encoding: UTF-8)records ... (len: 227)\n",
      "Finish: Get Page 182 (Encoding: UTF-8)records ... (len: 229)\n",
      "Finish: Get Page 183 (Encoding: UTF-8)records ... (len: 189)\n",
      "Finish: Get Page 184 (Encoding: UTF-8)records ... (len: 401)\n",
      "Finish: Get Page 185 (Encoding: UTF-8)records ... (len: 954)\n",
      "Finish: Get Page 186 (Encoding: UTF-8)records ... (len: 427)\n",
      "Finish: Get Page 187 (Encoding: UTF-8)records ... (len: 426)\n",
      "Finish: Get Page 188 (Encoding: UTF-8)records ... (len: 191)\n",
      "Finish: Get Page 189 (Encoding: UTF-8)records ... (len: 129)\n",
      "Finish: Get Page 190 (Encoding: UTF-8)records ... (len: 254)\n",
      "Finish: Get Page 191 (Encoding: UTF-8)records ... (len: 247)\n",
      "Finish: Get Page 192 (Encoding: UTF-8)records ... (len: 243)\n",
      "Finish: Get Page 193 (Encoding: UTF-8)records ... (len: 89)\n",
      "Finish: Get Page 194 (Encoding: UTF-8)records ... (len: 57)\n",
      "Finish: Get Page 195 (Encoding: UTF-8)records ... (len: 95)\n",
      "Finish: Get Page 196 (Encoding: UTF-8)records ... (len: 238)\n",
      "Finish: Get Page 197 (Encoding: UTF-8)records ... (len: 182)\n",
      "Finish: Get Page 198 (Encoding: UTF-8)records ... (len: 183)\n",
      "Finish: Get Page 199 (Encoding: UTF-8)records ... (len: 148)\n",
      "Finish: Get Page 200 (Encoding: UTF-8)records ... (len: 347)\n",
      "Finish: Get Page 201 (Encoding: UTF-8)records ... (len: 199)\n",
      "Finish: Get Page 202 (Encoding: UTF-8)records ... (len: 200)\n",
      "Finish: Get Page 203 (Encoding: UTF-8)records ... (len: 361)\n",
      "Finish: Get Page 204 (Encoding: UTF-8)records ... (len: 356)\n",
      "Finish: Get Page 205 (Encoding: UTF-8)records ... (len: 360)\n",
      "Finish: Get Page 206 (Encoding: UTF-8)records ... (len: 331)\n",
      "Finish: Get Page 207 (Encoding: UTF-8)records ... (len: 498)\n",
      "Finish: Get Page 208 (Encoding: UTF-8)records ... (len: 499)\n",
      "Finish: Get Page 209 (Encoding: UTF-8)records ... (len: 497)\n",
      "Finish: Get Page 210 (Encoding: cp1252)records ... (len: 47)\n",
      "Finish: Get Page 211 (Encoding: UTF-8)records ... (len: 145)\n",
      "Finish: Get Page 212 (Encoding: UTF-8)records ... (len: 147)\n",
      "Finish: Get Page 213 (Encoding: UTF-8)records ... (len: 111)\n",
      "Finish: Get Page 214 (Encoding: UTF-8)records ... (len: 429)\n",
      "Finish: Get Page 215 (Encoding: UTF-8)records ... (len: 160)\n",
      "Finish: Get Page 216 (Encoding: UTF-8)records ... (len: 162)\n",
      "Finish: Get Page 217 (Encoding: UTF-8)records ... (len: 123)\n",
      "Finish: Get Page 218 (Encoding: UTF-8)records ... (len: 83)\n",
      "Finish: Get Page 219 (Encoding: UTF-8)records ... (len: 84)\n",
      "Finish: Get Page 220 (Encoding: UTF-8)records ... (len: 72)\n",
      "Finish: Get Page 221 (Encoding: UTF-8)records ... (len: 207)\n",
      "Finish: Get Page 222 (Encoding: UTF-8)records ... (len: 202)\n",
      "Finish: Get Page 223 (Encoding: UTF-8)records ... (len: 103)\n",
      "Finish: Get Page 224 (Encoding: cp1252)records ... (len: 59)\n",
      "Finish: Get Page 225 (Encoding: UTF-8)records ... (len: 58)\n",
      "Finish: Get Page 226 (Encoding: UTF-8)records ... (len: 346)\n",
      "Finish: Get Page 227 (Encoding: UTF-8)records ... (len: 281)\n",
      "Finish: Get Page 228 (Encoding: UTF-8)records ... (len: 164)\n",
      "Finish: Get Page 229 (Encoding: UTF-8)records ... (len: 66)\n",
      "Finish: Get Page 230 (Encoding: UTF-8)records ... (len: 69)\n",
      "Finish: Get Page 231 (Encoding: UTF-8)records ... (len: 613)\n",
      "Finish: Get Page 232 (Encoding: cp1252)records ... (len: 74)\n",
      "Finish: Get Page 233 (Encoding: UTF-8)records ... (len: 61)\n",
      "Finish: Get Page 234 (Encoding: UTF-8)records ... (len: 258)\n",
      "Finish: Get Page 235 (Encoding: UTF-8)records ... (len: 369)\n",
      "Finish: Get Page 236 (Encoding: UTF-8)records ... (len: 226)\n",
      "Finish: Get Page 237 (Encoding: UTF-8)records ... (len: 286)\n",
      "Finish: Get Page 238 (Encoding: UTF-8)records ... (len: 190)\n",
      "Finish: Get Page 239 (Encoding: cp1252)records ... (len: 256)\n",
      "Finish: Get Page 240 (Encoding: cp1252)records ... (len: 148)\n",
      "Finish: Get Page 241 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 242 (Encoding: UTF-8)records ... (len: 164)\n",
      "Finish: Get Page 243 (Encoding: UTF-8)records ... (len: 169)\n",
      "Finish: Get Page 244 (Encoding: UTF-8)records ... (len: 139)\n",
      "Finish: Get Page 245 (Encoding: UTF-8)records ... (len: 131)\n",
      "Finish: Get Page 246 (Encoding: cp1252)records ... (len: 528)\n",
      "Finish: Get Page 247 (Encoding: UTF-8)records ... (len: 217)\n",
      "Finish: Get Page 248 (Encoding: UTF-8)records ... (len: 119)\n",
      "Finish: Get Page 249 (Encoding: UTF-8)records ... (len: 61)\n",
      "Finish: Get Page 250 (Encoding: UTF-8)records ... (len: 107)\n",
      "Finish: Get Page 251 (Encoding: cp1252)records ... (len: 467)\n",
      "Finish: Get Page 252 (Encoding: cp1252)records ... (len: 469)\n",
      "Finish: Get Page 253 (Encoding: UTF-8)records ... (len: 144)\n",
      "Finish: Get Page 254 (Encoding: UTF-8)records ... (len: 43)\n",
      "Finish: Get Page 255 (Encoding: UTF-8)records ... (len: 44)\n",
      "Finish: Get Page 257 (Encoding: UTF-8)records ... (len: 385)\n",
      "Finish: Get Page 258 (Encoding: UTF-8)records ... (len: 355)\n",
      "Finish: Get Page 259 (Encoding: UTF-8)records ... (len: 72)\n",
      "Finish: Get Page 260 (Encoding: UTF-8)records ... (len: 73)\n",
      "Finish: Get Page 261 (Encoding: UTF-8)records ... (len: 306)\n",
      "Finish: Get Page 262 (Encoding: UTF-8)records ... (len: 129)\n",
      "Finish: Get Page 263 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 264 (Encoding: UTF-8)records ... (len: 53)\n",
      "Finish: Get Page 265 (Encoding: UTF-8)records ... (len: 199)\n",
      "Finish: Get Page 266 (Encoding: UTF-8)records ... (len: 86)\n",
      "Finish: Get Page 267 (Encoding: UTF-8)records ... (len: 131)\n",
      "Finish: Get Page 268 (Encoding: UTF-8)records ... (len: 175)\n",
      "Finish: Get Page 269 (Encoding: UTF-8)records ... (len: 176)\n",
      "Finish: Get Page 271 (Encoding: UTF-8)records ... (len: 302)\n",
      "Finish: Get Page 272 (Encoding: UTF-8)records ... (len: 304)\n",
      "Finish: Get Page 273 (Encoding: UTF-8)records ... (len: 180)\n",
      "Finish: Get Page 274 (Encoding: UTF-8)records ... (len: 529)\n",
      "Finish: Get Page 275 (Encoding: UTF-8)records ... (len: 310)\n",
      "Finish: Get Page 276 (Encoding: UTF-8)records ... (len: 80)\n",
      "Finish: Get Page 277 (Encoding: UTF-8)records ... (len: 165)\n",
      "Finish: Get Page 278 (Encoding: UTF-8)records ... (len: 337)\n",
      "Finish: Get Page 279 (Encoding: cp1252)records ... (len: 253)\n",
      "Finish: Get Page 280 (Encoding: cp1252)records ... (len: 239)\n",
      "Finish: Get Page 281 (Encoding: cp1252)records ... (len: 243)\n",
      "Finish: Get Page 282 (Encoding: UTF-8)records ... (len: 68)\n",
      "Finish: Get Page 283 (Encoding: UTF-8)records ... (len: 59)\n",
      "Finish: Get Page 284 (Encoding: cp1252)records ... (len: 130)\n",
      "Finish: Get Page 285 (Encoding: UTF-8)records ... (len: 139)\n",
      "Finish: Get Page 286 (Encoding: UTF-8)records ... (len: 166)\n",
      "Finish: Get Page 287 (Encoding: UTF-8)records ... (len: 82)\n",
      "Finish: Get Page 288 (Encoding: UTF-8)records ... (len: 140)\n",
      "Finish: Get Page 289 (Encoding: UTF-8)records ... (len: 44)\n",
      "Finish: Get Page 290 (Encoding: UTF-8)records ... (len: 330)\n",
      "Finish: Get Page 291 (Encoding: UTF-8)records ... (len: 331)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: Get Page 292 (Encoding: UTF-8)records ... (len: 280)\n",
      "Finish: Get Page 293 (Encoding: UTF-8)records ... (len: 74)\n",
      "Finish: Get Page 294 (Encoding: UTF-8)records ... (len: 63)\n",
      "Finish: Get Page 295 (Encoding: UTF-8)records ... (len: 65)\n",
      "Finish: Get Page 296 (Encoding: UTF-8)records ... (len: 20)\n",
      "Finish: Get Page 297 (Encoding: UTF-8)records ... (len: 367)\n",
      "Finish: Get Page 298 (Encoding: UTF-8)records ... (len: 371)\n",
      "Finish: Get Page 299 (Encoding: UTF-8)records ... (len: 361)\n",
      "Finish: Get Page 300 (Encoding: UTF-8)records ... (len: 361)\n",
      "Finish: Get Page 301 (Encoding: UTF-8)records ... (len: 364)\n",
      "Finish: Get Page 302 (Encoding: UTF-8)records ... (len: 170)\n",
      "Finish: Get Page 303 (Encoding: UTF-8)records ... (len: 154)\n",
      "Finish: Get Page 304 (Encoding: cp1252)records ... (len: 117)\n",
      "Finish: Get Page 305 (Encoding: UTF-8)records ... (len: 1987)\n",
      "Finish: Get Page 306 (Encoding: UTF-8)records ... (len: 59)\n",
      "Finish: Get Page 307 (Encoding: UTF-8)records ... (len: 60)\n",
      "Finish: Get Page 308 (Encoding: UTF-8)records ... (len: 60)\n",
      "Finish: Get Page 309 (Encoding: UTF-8)records ... (len: 145)\n",
      "Finish: Get Page 310 (Encoding: UTF-8)records ... (len: 116)\n",
      "Finish: Get Page 311 (Encoding: UTF-8)records ... (len: 119)\n",
      "Finish: Get Page 312 (Encoding: cp1252)records ... (len: 136)\n",
      "Finish: Get Page 313 (Encoding: UTF-8)records ... (len: 383)\n",
      "Finish: Get Page 314 (Encoding: UTF-8)records ... (len: 317)\n",
      "Finish: Get Page 315 (Encoding: cp1252)records ... (len: 314)\n",
      "Finish: Get Page 316 (Encoding: cp1252)records ... (len: 357)\n",
      "Finish: Get Page 317 (Encoding: cp1252)records ... (len: 370)\n",
      "Finish: Get Page 318 (Encoding: UTF-8)records ... (len: 137)\n",
      "Finish: Get Page 319 (Encoding: UTF-8)records ... (len: 281)\n",
      "Finish: Get Page 320 (Encoding: UTF-8)records ... (len: 281)\n",
      "Finish: Get Page 321 (Encoding: UTF-8)records ... (len: 247)\n",
      "Finish: Get Page 322 (Encoding: UTF-8)records ... (len: 248)\n",
      "Finish: Get Page 323 (Encoding: UTF-8)records ... (len: 389)\n",
      "Finish: Get Page 324 (Encoding: UTF-8)records ... (len: 330)\n",
      "Finish: Get Page 325 (Encoding: UTF-8)records ... (len: 291)\n"
     ]
    }
   ],
   "source": [
    "X_raw, y, page_positions = storage.get_Xy(contain_button = True, file_type='T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_page_seq = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_data(x, y, p):\n",
    "    new_tmp_x_array = []\n",
    "    new_tmp_y_array = []\n",
    "    new_tmp_p_array = []\n",
    "    for tmp_x, tmp_y, tmp_p in zip(x, y, p):\n",
    "        new_tmp_x_array.extend(chunks(tmp_x, max_page_seq))\n",
    "        new_tmp_y_array.extend(chunks(tmp_y, max_page_seq))\n",
    "        new_tmp_p_array.extend(chunks(tmp_p, max_page_seq))\n",
    "    return new_tmp_x_array, new_tmp_y_array, new_tmp_p_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_x, chunks_y, chunk_positions = get_chunks_data(X_raw, y, page_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Bert model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import bert\n",
    "from bert import tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "from BertModel import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_short_model = BertModel(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_long_model = BertModel(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbert = bert_short_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 4 µs, total: 10 µs\n",
      "Wall time: 20.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# XXX: these functions should be copy-pasted from autopager/model.py\n",
    "\n",
    "def _as_list(generator, limit=None):\n",
    "    \"\"\"\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2), 0)\n",
    "    []\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2), 2)\n",
    "    ['te', 'ex']\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2))\n",
    "    ['te', 'ex', 'xt']\n",
    "    \"\"\"\n",
    "    return list(generator if limit is None else islice(generator, 0, limit))\n",
    "\n",
    "def feat_to_tokens(feat, tokenizer):\n",
    "    if type(feat) == type([]):\n",
    "        feat = ' '.join(feat)\n",
    "    tokens = tokenizer.tokenize(feat)\n",
    "    return tokens\n",
    "\n",
    "def link_to_features(link):\n",
    "    text = normalize(get_link_text(link))\n",
    "    href = get_link_href(link)\n",
    "    if href is None:\n",
    "        href = \"\"\n",
    "    p = urlsplit(href)\n",
    "    parent = link.xpath('..').extract()\n",
    "    parent = get_first_tag(parser, parent[0])\n",
    "    query_parsed = parse_qsl(p.query) #parse query string from path\n",
    "    query_param_names = [k.lower() for k, v in query_parsed]\n",
    "    query_param_names_ngrams = _as_list(ngrams_wb(\n",
    "        \" \".join([normalize(name) for name in query_param_names]), 3, 5, True\n",
    "    ))\n",
    "\n",
    "    # Classes of link itself and all its children.\n",
    "    # It is common to have e.g. span elements with fontawesome\n",
    "    # arrow icon classes inside <a> links.\n",
    "    self_and_children_classes = ' '.join(_as_list(link.xpath(\".//@class\").extract(), 5))\n",
    "    parent_classes = ' '.join(_as_list(link.xpath('../@class').extract(), 5))\n",
    "    css_classes = normalize(parent_classes + ' ' + self_and_children_classes)\n",
    "#     print(css_classes)\n",
    "    token_feature = {\n",
    "        'text-before': '',\n",
    "        'text-exact': replace_digits(text.strip()[:40].strip()),\n",
    "        'text-after': '',\n",
    "        'class': css_classes,\n",
    "        'query': _as_list(query_param_names, 10),\n",
    "        'parent-tag': parent,\n",
    "    }\n",
    "    tag_feature = {\n",
    "        'isdigit': 1 if text.isdigit() is True else 0,\n",
    "        'isalpha': 1 if text.isalpha() is True else 0,\n",
    "        'has-href': 0 if href is \"\" else 1,\n",
    "        'path-has-page': 1 if 'page' in p.path.lower() else 0,\n",
    "        'path-has-pageXX': 1 if re.search(r'[/-](?:p|page\\w?)/?\\d+', p.path.lower()) is not None else 0,\n",
    "        'path-has-number': 1 if any(part.isdigit() for part in p.path.split('/')) else 0,\n",
    "        'href-has-year': 1 if re.search('20\\d\\d', href) is not None else 0,\n",
    "        'class-has-disabled': 1 if 'disabled' in css_classes else 0,\n",
    "    }\n",
    "    tag_feature = [v for k,v in tag_feature.items()]\n",
    "#     attribute_feature = elem_rel + elem_target\n",
    "    non_token_feature = tag_feature #+ attribute_feature\n",
    "    return [token_feature, non_token_feature]\n",
    "\n",
    "\n",
    "def page_to_features(xseq):\n",
    "    feat_list = [link_to_features(a) for a in xseq]\n",
    "    around = get_text_around_selector_list(xseq, max_length=15)\n",
    "    \n",
    "    # weight is less than 1 because there is a lot of duplicate information\n",
    "    # in these ngrams and so we want to regularize them stronger\n",
    "    # (as if they are a single feature, not many features)\n",
    "    k = 0.2\n",
    "    for feat, (before, after) in zip(feat_list, around):\n",
    "        feat[0]['text-before'] = normalize(before)\n",
    "        feat[0]['text-after'] = normalize(after)\n",
    "        \n",
    "    return feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_tag_features_from_chunks(chunks):\n",
    "    token_features = []\n",
    "    tag_features = []\n",
    "    for page in chunks:\n",
    "        feat_list = page_to_features(page)\n",
    "        token_features.append([node[0] for node in feat_list])\n",
    "        tag_features.append([node[1] for node in feat_list])\n",
    "    return token_features, tag_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens_from_token_features(token_features):\n",
    "    train_tag_feature_token_list = []\n",
    "    for page in token_features:\n",
    "        tmp_page_list = []\n",
    "        for node in page: \n",
    "            tmp_list = []\n",
    "            for k, v in node.items():\n",
    "                if k == 'text-exact':\n",
    "                    continue\n",
    "                else:\n",
    "                    tmp_list.extend(v)\n",
    "            tmp_page_list.append(tmp_list)\n",
    "        train_tag_feature_token_list.append(tmp_page_list)\n",
    "    return train_tag_feature_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_to_two_bert_embeddings(token_features, tokenizer):\n",
    "    text_first_segs = []\n",
    "    text_second_segs = []\n",
    "    for page in token_features:\n",
    "        page_one_features = []\n",
    "        page_two_features = []\n",
    "        for node in page:\n",
    "            text_before = tokenizer.tokenize(node[\"text-before\"])\n",
    "            text_exact = tokenizer.tokenize(node[\"text-exact\"])\n",
    "            text_after = tokenizer.tokenize(node[\"text-after\"])\n",
    "            page_one_features.append([\"[CLS]\"]+text_before+[\"[SEP]\"]+text_exact+[\"[SEP]\"])\n",
    "            page_two_features.append([\"[CLS]\"]+text_exact+[\"[SEP]\"]+text_after+[\"[SEP]\"])\n",
    "        text_first_segs.append(page_one_features)\n",
    "        text_second_segs.append(page_two_features)\n",
    "    print(\"Start encode first seg embeddings\")\n",
    "    first_emb = pbert.page_list_to_bert_embedding_list(text_first_segs, Token=True)\n",
    "    print(\"Start encode second seg embeddings\")\n",
    "    second_emb = pbert.page_list_to_bert_embedding_list(text_second_segs, Token=True)\n",
    "    full_text_emb = [np.concatenate([first_emb[page], second_emb[page]], axis = 1) for page in range(len(token_features))]\n",
    "    return first_emb, second_emb, full_text_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_features, tag_features = get_token_tag_features_from_chunks(chunks_x)\n",
    "# train_tag_feature_token_list = extract_tokens_from_token_features(token_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start encode first seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57292a539545486cb38f88089c769beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=356)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start encode second seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95b096a4a0244f2a6217edcd0bf6ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=356)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_emb, second_emb, full_text_emb = page_to_two_bert_embeddings(token_features, pbert.get_tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/DL/lib/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "np.save('embedding/train/first.npy', first_emb)\n",
    "np.save('embedding/train/second.npy', second_emb)\n",
    "np.save('embedding/train/full_text.npy', full_text_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_emb = np.load('embedding/train/first.npy', allow_pickle=True)\n",
    "second_emb = np.load('embedding/train/second.npy', allow_pickle=True)\n",
    "full_text_emb = np.load('embedding/train/full_text.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First emb:(303, 768)\n",
      "Second emb:(303, 768)\n",
      "Full_text emb:(303, 1536)\n"
     ]
    }
   ],
   "source": [
    "print(f\"First emb:{first_emb[0].shape}\")\n",
    "print(f\"Second emb:{second_emb[0].shape}\")\n",
    "print(f\"Full_text emb:{full_text_emb[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  10.835698937066166\n",
      "Max_node:  63\n"
     ]
    }
   ],
   "source": [
    "max_node = -1\n",
    "page_sum = 0\n",
    "for page in train_token_features:\n",
    "    sum = 0\n",
    "    for node in page:\n",
    "        sum+=len(node)\n",
    "        if len(node) > max_node:\n",
    "            max_node = len(node)\n",
    "    page_sum+=sum/len(page)\n",
    "print(\"Average: \", page_sum/len(train_token_features))\n",
    "print(\"Max_node: \",max_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_info_list = tag_features #features which only have tag true/false information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text-exact feature in token_features\n",
    "train_text_list = [[ data['text-exact'] for data in x] for x in token_features]\n",
    "train_text_before_list = [[ data['text-before'] for data in x] for x in token_features]\n",
    "train_text_after_list = [[ data['text-after'] for data in x] for x in token_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_text(before, mid, after):\n",
    "    res = \"\"\n",
    "    if before != \"\":\n",
    "        res+=before + \",\"\n",
    "    if mid != \"\":\n",
    "        res+=mid + \",\"\n",
    "    if after == \"\":\n",
    "        res = res[:-1]\n",
    "    else:\n",
    "        res+=after\n",
    "    return res\n",
    "# Extract text-exact feature in token_features\n",
    "train_full_text_list = [[ concat_text(data['text-before'],data['text-exact'],data['text-after']) for data in x] for x in token_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbert.max_seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature List\n",
    "    * train_tag_feature_token_list => Tag Attributes tokens\n",
    "    * train_tag_info_list => Tag information\n",
    "    * train_text_emb => Only Text node => Bert Text embedding\n",
    "    * train_tag_emb => Text-before Text Text-after [SEP] Other Attributes => Bert Text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks_text_emb = train_text_emb # text\n",
    "# chunks_text_emb = train_tag_emb # tag\n",
    "chunks_text_emb = full_text_emb # full text embedding (two bert)\n",
    "\n",
    "chunks_tag_infos = train_tag_info_list\n",
    "chunks_filtered_y = chunks_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding to fixed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_pad_to_npdata(embedding):\n",
    "    dataset = Dataset.from_generator(lambda: iter(embedding), tf.float32)\n",
    "    dataset = dataset.padded_batch(1, padded_shapes= (max_page_seq, len(embedding[0][0])), padding_values=-1.,drop_remainder=False)\n",
    "    after_pad = np.array([ data[0] for data in list(dataset.as_numpy_iterator())])\n",
    "    return after_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tag_token = feature_pad_to_npdata(chunks_tag_tokens)\n",
    "train_text_emb_x = feature_pad_to_npdata(chunks_text_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_x = feature_pad_to_npdata(chunks_tag_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_positions_x = feature_pad_to_npdata(chunk_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_x = np.concatenate([train_tag_x, train_positions_x], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info_x = np.concatenate([train_text_emb_x, train_tag_x], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"O\", \"PREV\", \"PAGE\", \"NEXT\", \"[PAD]\"]\n",
    "tag2idx = { label:idx for idx,label in enumerate(labels)}\n",
    "idx2tag = { idx:label for idx,label in enumerate(labels)}\n",
    "num_tags = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pad_sequences([[tag2idx.get(l) for l in lab] for lab in chunks_filtered_y], maxlen=max_page_seq, padding=\"post\", truncating=\"post\", value=tag2idx[\"[PAD]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Shape:\n",
      "train_text_emb_x: (356, 512, 1536)\n",
      "train_tag_x: (356, 512, 10)\n",
      "train_info_x: (356, 512, 1546)\n",
      "train_y: (356, 512)\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Shape:\")\n",
    "print(f\"train_text_emb_x: {train_text_emb_x.shape}\")\n",
    "print(f\"train_tag_x: {train_tag_x.shape}\")\n",
    "print(f\"train_info_x: {train_info_x.shape}\")\n",
    "print(f\"train_y: {train_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = train_text_emb_x\n",
    "train_x = train_info_x\n",
    "# train_x = train_tag_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356, 512, 1546)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build BERT-BiLSTM-CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.layers.crf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME_STAMP: 512\n",
      "HIDDEN_UNITS: 200\n",
      "DROPOUT_RATE: 0.1\n",
      "NUM_CLASS: 5\n"
     ]
    }
   ],
   "source": [
    "TIME_STAMPS = max_page_seq\n",
    "HIDDEN_UNITS = 200\n",
    "DROPOUT_RATE = 0.1\n",
    "# NUM_CLASS = 5\n",
    "NUM_CLASS = num_tags\n",
    "print(f\"TIME_STAMP: {TIME_STAMPS}\")\n",
    "print(f\"HIDDEN_UNITS: {HIDDEN_UNITS}\")\n",
    "print(f\"DROPOUT_RATE: {DROPOUT_RATE}\")\n",
    "print(f\"NUM_CLASS: {NUM_CLASS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BERT_BILSTM_CRF(SHAPE, numtags):\n",
    "    print(f\"SHAPE: {SHAPE}\")\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=SHAPE))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=HIDDEN_UNITS, return_sequences=True)))\n",
    "    crf=CRF(numtags,name='crf_layer')\n",
    "    model.add(crf)\n",
    "    model.compile('adam',loss={'crf_layer': crf.get_loss})\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BERT_FFN_CRF(SHAPE, numtags):\n",
    "    print(f\"SHAPE: {SHAPE}\")\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=SHAPE))\n",
    "    model.add(tf.keras.layers.Dense(units = 768, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units = 324, activation = 'relu'))\n",
    "    model.add(tf.keras.layers.Dense(units = 162, activation = 'relu'))\n",
    "#     model.add(tf.keras.layers.Dense(units = 81, activation = 'relu'))\n",
    "    crf=CRF(numtags,name='crf_layer')\n",
    "    model.add(crf)\n",
    "    model.compile('adam',loss={'crf_layer': crf.get_loss})\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_BERT_BILSTM_SOFTMAX(SHAPE, numtags):\n",
    "    print(f\"SHAPE: {SHAPE}\")\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=SHAPE))\n",
    "    model.add(tf.keras.layers.Masking(input_shape=SHAPE, mask_value=-1.))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=HIDDEN_UNITS, return_sequences=True)))\n",
    "    model.add(tf.keras.layers.Dense(units = numtags, activation='softmax'))\n",
    "    model.compile('adam',loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE: (512, 1546)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512, 768)          1188096   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512, 324)          249156    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512, 162)          52650     \n",
      "_________________________________________________________________\n",
      "crf_layer (CRF)              (None, 512)               850       \n",
      "=================================================================\n",
      "Total params: 1,490,752\n",
      "Trainable params: 1,490,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "FFN_CRF = get_BERT_FFN_CRF(train_x.shape[1:], num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE: (512, 1546)\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_4 (Bidirection (None, 512, 400)          2795200   \n",
      "_________________________________________________________________\n",
      "crf_layer (CRF)              (None, 512)               2040      \n",
      "=================================================================\n",
      "Total params: 2,797,240\n",
      "Trainable params: 2,797,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "SHAPE: (512, 1546)\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_2 (Masking)          (None, 512, 1546)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 512, 400)          2795200   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512, 5)            2005      \n",
      "=================================================================\n",
      "Total params: 2,797,205\n",
      "Trainable params: 2,797,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CRF_model = get_BERT_BILSTM_CRF(train_x.shape[1:], num_tags)\n",
    "\n",
    "Softmax_model = get_BERT_BILSTM_SOFTMAX(train_x.shape[1:], num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356, 512, 1546)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "3/3 [==============================] - 1s 499ms/step - loss: 483.9219 - val_loss: 57.2484\n",
      "Epoch 2/1000\n",
      "3/3 [==============================] - 1s 377ms/step - loss: 45.1123 - val_loss: 66.4676\n",
      "Epoch 3/1000\n",
      "3/3 [==============================] - 1s 358ms/step - loss: 46.8816 - val_loss: 51.1406\n",
      "Epoch 4/1000\n",
      "3/3 [==============================] - 1s 376ms/step - loss: 35.1875 - val_loss: 47.4464\n",
      "Epoch 5/1000\n",
      "3/3 [==============================] - 1s 362ms/step - loss: 35.8825 - val_loss: 42.7081\n",
      "Epoch 6/1000\n",
      "3/3 [==============================] - 1s 388ms/step - loss: 32.2482 - val_loss: 45.0902\n",
      "Epoch 7/1000\n",
      "3/3 [==============================] - 1s 379ms/step - loss: 32.9311 - val_loss: 41.6077\n",
      "Epoch 8/1000\n",
      "3/3 [==============================] - 1s 378ms/step - loss: 30.5672 - val_loss: 39.1452\n",
      "Epoch 9/1000\n",
      "3/3 [==============================] - 1s 371ms/step - loss: 29.4342 - val_loss: 37.7583\n",
      "Epoch 10/1000\n",
      "3/3 [==============================] - 1s 376ms/step - loss: 28.4592 - val_loss: 36.0805\n",
      "Epoch 11/1000\n",
      "3/3 [==============================] - 1s 385ms/step - loss: 26.4747 - val_loss: 34.4987\n",
      "Epoch 12/1000\n",
      "3/3 [==============================] - 1s 366ms/step - loss: 25.0525 - val_loss: 32.2516\n",
      "Epoch 13/1000\n",
      "3/3 [==============================] - 1s 374ms/step - loss: 22.9714 - val_loss: 29.8455\n",
      "Epoch 14/1000\n",
      "3/3 [==============================] - 1s 351ms/step - loss: 20.9670 - val_loss: 28.5316\n",
      "Epoch 15/1000\n",
      "3/3 [==============================] - 1s 369ms/step - loss: 19.3960 - val_loss: 26.9751\n",
      "Epoch 16/1000\n",
      "3/3 [==============================] - 1s 358ms/step - loss: 17.8267 - val_loss: 26.3665\n",
      "Epoch 17/1000\n",
      "3/3 [==============================] - 1s 388ms/step - loss: 17.2372 - val_loss: 25.6146\n",
      "Epoch 18/1000\n",
      "3/3 [==============================] - 1s 384ms/step - loss: 16.6157 - val_loss: 24.8183\n",
      "Epoch 19/1000\n",
      "3/3 [==============================] - 1s 379ms/step - loss: 16.5028 - val_loss: 24.4672\n",
      "Epoch 20/1000\n",
      "3/3 [==============================] - 1s 371ms/step - loss: 16.5923 - val_loss: 26.9410\n",
      "Epoch 21/1000\n",
      "3/3 [==============================] - 1s 372ms/step - loss: 17.1949 - val_loss: 24.7200\n",
      "Epoch 22/1000\n",
      "3/3 [==============================] - 1s 383ms/step - loss: 15.7100 - val_loss: 23.7024\n",
      "Epoch 23/1000\n",
      "3/3 [==============================] - 1s 349ms/step - loss: 15.0914 - val_loss: 24.3209\n",
      "Epoch 24/1000\n",
      "3/3 [==============================] - 1s 363ms/step - loss: 15.1974 - val_loss: 24.5227\n",
      "Epoch 25/1000\n",
      "3/3 [==============================] - 1s 369ms/step - loss: 15.7022 - val_loss: 23.1530\n",
      "Epoch 26/1000\n",
      "3/3 [==============================] - 1s 365ms/step - loss: 14.4742 - val_loss: 23.3155\n",
      "Epoch 27/1000\n",
      "3/3 [==============================] - 1s 360ms/step - loss: 14.2298 - val_loss: 24.4160\n",
      "Epoch 28/1000\n",
      "3/3 [==============================] - 1s 379ms/step - loss: 14.2548 - val_loss: 23.7896\n",
      "Epoch 29/1000\n",
      "3/3 [==============================] - 1s 385ms/step - loss: 14.1974 - val_loss: 25.1179\n",
      "Epoch 30/1000\n",
      "3/3 [==============================] - 1s 358ms/step - loss: 14.4158 - val_loss: 23.5646\n",
      "Epoch 31/1000\n",
      "3/3 [==============================] - 1s 366ms/step - loss: 13.7056 - val_loss: 23.9415\n",
      "Epoch 32/1000\n",
      "3/3 [==============================] - 1s 381ms/step - loss: 13.7151 - val_loss: 23.8285\n",
      "Epoch 33/1000\n",
      "3/3 [==============================] - 1s 360ms/step - loss: 13.8554 - val_loss: 22.8789\n",
      "Epoch 34/1000\n",
      "3/3 [==============================] - 1s 377ms/step - loss: 13.3939 - val_loss: 22.6326\n",
      "Epoch 35/1000\n",
      "3/3 [==============================] - 1s 388ms/step - loss: 13.6977 - val_loss: 22.5972\n",
      "Epoch 36/1000\n",
      "3/3 [==============================] - 1s 384ms/step - loss: 12.8840 - val_loss: 22.5587\n",
      "Epoch 37/1000\n",
      "3/3 [==============================] - 1s 373ms/step - loss: 12.6027 - val_loss: 22.3186\n",
      "Epoch 38/1000\n",
      "3/3 [==============================] - 1s 376ms/step - loss: 12.5449 - val_loss: 22.7571\n",
      "Epoch 39/1000\n",
      "3/3 [==============================] - 1s 380ms/step - loss: 12.7566 - val_loss: 22.5332\n",
      "Epoch 40/1000\n",
      "3/3 [==============================] - 1s 361ms/step - loss: 12.1555 - val_loss: 22.9991\n",
      "Epoch 41/1000\n",
      "3/3 [==============================] - 1s 379ms/step - loss: 12.1303 - val_loss: 21.6310\n",
      "Epoch 42/1000\n",
      "3/3 [==============================] - 1s 366ms/step - loss: 12.4872 - val_loss: 22.4457\n",
      "Epoch 43/1000\n",
      "3/3 [==============================] - 1s 392ms/step - loss: 12.9354 - val_loss: 24.4181\n",
      "Epoch 44/1000\n",
      "3/3 [==============================] - 1s 374ms/step - loss: 12.3949 - val_loss: 25.6883\n",
      "Epoch 45/1000\n",
      "3/3 [==============================] - 1s 375ms/step - loss: 14.8477 - val_loss: 33.0200\n",
      "Epoch 46/1000\n",
      "3/3 [==============================] - 1s 379ms/step - loss: 18.3628 - val_loss: 30.8308\n",
      "Epoch 47/1000\n",
      "3/3 [==============================] - 1s 398ms/step - loss: 16.4005 - val_loss: 31.8544\n",
      "Epoch 48/1000\n",
      "3/3 [==============================] - 1s 383ms/step - loss: 15.7801 - val_loss: 22.8868\n",
      "Epoch 49/1000\n",
      "3/3 [==============================] - 1s 377ms/step - loss: 13.5422 - val_loss: 20.7973\n",
      "Epoch 50/1000\n",
      "3/3 [==============================] - 1s 394ms/step - loss: 13.1774 - val_loss: 20.5885\n",
      "Epoch 51/1000\n",
      "3/3 [==============================] - 1s 360ms/step - loss: 12.8668 - val_loss: 21.1805\n",
      "Epoch 52/1000\n",
      "3/3 [==============================] - 1s 363ms/step - loss: 12.0146 - val_loss: 22.6081\n",
      "Epoch 53/1000\n",
      "3/3 [==============================] - 1s 374ms/step - loss: 12.5063 - val_loss: 21.3557\n",
      "Epoch 54/1000\n",
      "3/3 [==============================] - 1s 384ms/step - loss: 11.7147 - val_loss: 20.7787\n",
      "Epoch 55/1000\n",
      "3/3 [==============================] - 1s 385ms/step - loss: 11.4655 - val_loss: 19.9807\n",
      "Epoch 56/1000\n",
      "3/3 [==============================] - 1s 383ms/step - loss: 11.1628 - val_loss: 19.8876\n",
      "Epoch 57/1000\n",
      "3/3 [==============================] - 1s 372ms/step - loss: 11.1569 - val_loss: 20.2111\n",
      "Epoch 58/1000\n",
      "3/3 [==============================] - 1s 369ms/step - loss: 10.9575 - val_loss: 20.3341\n",
      "Epoch 59/1000\n",
      "3/3 [==============================] - 1s 378ms/step - loss: 10.8372 - val_loss: 20.5973\n",
      "Epoch 60/1000\n",
      "3/3 [==============================] - 1s 379ms/step - loss: 10.6446 - val_loss: 20.6549\n",
      "Epoch 61/1000\n",
      "3/3 [==============================] - 1s 396ms/step - loss: 10.7636 - val_loss: 20.7994\n",
      "Epoch 62/1000\n",
      "3/3 [==============================] - 1s 373ms/step - loss: 10.6408 - val_loss: 20.6691\n",
      "Epoch 63/1000\n",
      "3/3 [==============================] - 1s 404ms/step - loss: 10.4834 - val_loss: 20.3070\n",
      "Epoch 64/1000\n",
      "3/3 [==============================] - 1s 387ms/step - loss: 10.2463 - val_loss: 19.9610\n",
      "Epoch 65/1000\n",
      "3/3 [==============================] - 1s 395ms/step - loss: 10.0985 - val_loss: 19.7427\n",
      "Epoch 66/1000\n",
      "3/3 [==============================] - 1s 393ms/step - loss: 10.1891 - val_loss: 19.9311\n",
      "Epoch 67/1000\n",
      "3/3 [==============================] - 1s 375ms/step - loss: 10.0803 - val_loss: 19.6608\n",
      "Epoch 68/1000\n",
      "3/3 [==============================] - 1s 368ms/step - loss: 9.8647 - val_loss: 19.8622\n",
      "Epoch 69/1000\n",
      "3/3 [==============================] - 1s 380ms/step - loss: 9.8346 - val_loss: 19.9454\n",
      "Epoch 70/1000\n",
      "3/3 [==============================] - 1s 374ms/step - loss: 9.8185 - val_loss: 19.9427\n",
      "Epoch 71/1000\n",
      "3/3 [==============================] - 1s 387ms/step - loss: 10.1517 - val_loss: 22.8991\n",
      "Epoch 72/1000\n",
      "3/3 [==============================] - 1s 399ms/step - loss: 11.0364 - val_loss: 22.0063\n",
      "Epoch 73/1000\n",
      "3/3 [==============================] - 1s 367ms/step - loss: 10.9359 - val_loss: 22.6863\n",
      "Epoch 74/1000\n",
      "3/3 [==============================] - 1s 376ms/step - loss: 10.2937 - val_loss: 20.9354\n",
      "Epoch 75/1000\n",
      "3/3 [==============================] - 1s 381ms/step - loss: 10.6139 - val_loss: 21.5300\n",
      "Epoch 76/1000\n",
      "3/3 [==============================] - 1s 356ms/step - loss: 10.2793 - val_loss: 21.2412\n",
      "Epoch 77/1000\n",
      "3/3 [==============================] - 1s 387ms/step - loss: 11.7461 - val_loss: 21.2497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa38f1b47f0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FFN_CRF.fit(train_x, train_y, batch_size=128, epochs=1000, validation_split=0.1, verbose=1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "3/3 [==============================] - 2s 642ms/step - loss: 465.3257 - val_loss: 57.2520\n",
      "Epoch 2/1000\n",
      "3/3 [==============================] - 1s 458ms/step - loss: 43.1996 - val_loss: 55.1223\n",
      "Epoch 3/1000\n",
      "3/3 [==============================] - 1s 464ms/step - loss: 39.2618 - val_loss: 44.9216\n",
      "Epoch 4/1000\n",
      "3/3 [==============================] - 1s 481ms/step - loss: 32.0852 - val_loss: 37.6443\n",
      "Epoch 5/1000\n",
      "3/3 [==============================] - 1s 437ms/step - loss: 29.2268 - val_loss: 40.9559\n",
      "Epoch 6/1000\n",
      "3/3 [==============================] - 1s 437ms/step - loss: 30.3761 - val_loss: 37.2785\n",
      "Epoch 7/1000\n",
      "3/3 [==============================] - 1s 441ms/step - loss: 27.5271 - val_loss: 37.0146\n",
      "Epoch 8/1000\n",
      "3/3 [==============================] - 1s 458ms/step - loss: 27.6929 - val_loss: 37.4914\n",
      "Epoch 9/1000\n",
      "3/3 [==============================] - 1s 451ms/step - loss: 27.5586 - val_loss: 36.5734\n",
      "Epoch 10/1000\n",
      "3/3 [==============================] - 1s 455ms/step - loss: 26.6348 - val_loss: 35.4552\n",
      "Epoch 11/1000\n",
      "3/3 [==============================] - 1s 440ms/step - loss: 25.7825 - val_loss: 35.2300\n",
      "Epoch 12/1000\n",
      "3/3 [==============================] - 1s 449ms/step - loss: 25.4779 - val_loss: 34.7649\n",
      "Epoch 13/1000\n",
      "3/3 [==============================] - 1s 429ms/step - loss: 24.9927 - val_loss: 33.9179\n",
      "Epoch 14/1000\n",
      "3/3 [==============================] - 1s 438ms/step - loss: 24.3605 - val_loss: 33.5561\n",
      "Epoch 15/1000\n",
      "3/3 [==============================] - 1s 443ms/step - loss: 23.9629 - val_loss: 33.0586\n",
      "Epoch 16/1000\n",
      "3/3 [==============================] - 1s 445ms/step - loss: 23.4451 - val_loss: 32.3905\n",
      "Epoch 17/1000\n",
      "3/3 [==============================] - 1s 451ms/step - loss: 22.9454 - val_loss: 31.9634\n",
      "Epoch 18/1000\n",
      "3/3 [==============================] - 1s 459ms/step - loss: 22.5757 - val_loss: 31.6408\n",
      "Epoch 19/1000\n",
      "3/3 [==============================] - 1s 450ms/step - loss: 22.0625 - val_loss: 31.1940\n",
      "Epoch 20/1000\n",
      "3/3 [==============================] - 1s 466ms/step - loss: 21.5590 - val_loss: 30.6257\n",
      "Epoch 21/1000\n",
      "3/3 [==============================] - 1s 441ms/step - loss: 21.0874 - val_loss: 29.9444\n",
      "Epoch 22/1000\n",
      "3/3 [==============================] - 1s 441ms/step - loss: 20.5513 - val_loss: 29.1643\n",
      "Epoch 23/1000\n",
      "3/3 [==============================] - 1s 440ms/step - loss: 20.0722 - val_loss: 28.5254\n",
      "Epoch 24/1000\n",
      "3/3 [==============================] - 1s 443ms/step - loss: 19.5497 - val_loss: 27.9115\n",
      "Epoch 25/1000\n",
      "3/3 [==============================] - 1s 444ms/step - loss: 19.0211 - val_loss: 27.2003\n",
      "Epoch 26/1000\n",
      "3/3 [==============================] - 1s 447ms/step - loss: 18.5265 - val_loss: 26.5453\n",
      "Epoch 27/1000\n",
      "3/3 [==============================] - 1s 431ms/step - loss: 18.0165 - val_loss: 25.8458\n",
      "Epoch 28/1000\n",
      "3/3 [==============================] - 1s 447ms/step - loss: 17.5396 - val_loss: 25.0721\n",
      "Epoch 29/1000\n",
      "3/3 [==============================] - 1s 443ms/step - loss: 17.0397 - val_loss: 24.3343\n",
      "Epoch 30/1000\n",
      "3/3 [==============================] - 1s 446ms/step - loss: 16.5047 - val_loss: 23.6487\n",
      "Epoch 31/1000\n",
      "3/3 [==============================] - 1s 447ms/step - loss: 16.0710 - val_loss: 23.5433\n",
      "Epoch 32/1000\n",
      "3/3 [==============================] - 1s 458ms/step - loss: 15.7541 - val_loss: 22.5834\n",
      "Epoch 33/1000\n",
      "3/3 [==============================] - 1s 443ms/step - loss: 15.2496 - val_loss: 22.4907\n",
      "Epoch 34/1000\n",
      "3/3 [==============================] - 1s 449ms/step - loss: 14.7824 - val_loss: 21.6314\n",
      "Epoch 35/1000\n",
      "3/3 [==============================] - 1s 443ms/step - loss: 14.3179 - val_loss: 22.6333\n",
      "Epoch 36/1000\n",
      "3/3 [==============================] - 1s 449ms/step - loss: 14.1234 - val_loss: 20.9383\n",
      "Epoch 37/1000\n",
      "3/3 [==============================] - 1s 451ms/step - loss: 13.8243 - val_loss: 21.1325\n",
      "Epoch 38/1000\n",
      "3/3 [==============================] - 1s 429ms/step - loss: 13.4396 - val_loss: 20.7512\n",
      "Epoch 39/1000\n",
      "3/3 [==============================] - 1s 436ms/step - loss: 13.0710 - val_loss: 20.3859\n",
      "Epoch 40/1000\n",
      "3/3 [==============================] - 1s 459ms/step - loss: 12.8658 - val_loss: 21.3639\n",
      "Epoch 41/1000\n",
      "3/3 [==============================] - 1s 447ms/step - loss: 12.6290 - val_loss: 20.4287\n",
      "Epoch 42/1000\n",
      "3/3 [==============================] - 1s 446ms/step - loss: 12.4029 - val_loss: 19.7104\n",
      "Epoch 43/1000\n",
      "3/3 [==============================] - 1s 455ms/step - loss: 12.0827 - val_loss: 21.7830\n",
      "Epoch 44/1000\n",
      "3/3 [==============================] - 1s 462ms/step - loss: 12.0371 - val_loss: 19.7737\n",
      "Epoch 45/1000\n",
      "3/3 [==============================] - 1s 460ms/step - loss: 11.9265 - val_loss: 19.0985\n",
      "Epoch 46/1000\n",
      "3/3 [==============================] - 1s 456ms/step - loss: 11.6087 - val_loss: 20.4301\n",
      "Epoch 47/1000\n",
      "3/3 [==============================] - 1s 455ms/step - loss: 11.1900 - val_loss: 18.8750\n",
      "Epoch 48/1000\n",
      "3/3 [==============================] - 1s 441ms/step - loss: 11.0912 - val_loss: 20.6200\n",
      "Epoch 49/1000\n",
      "3/3 [==============================] - 1s 449ms/step - loss: 10.7749 - val_loss: 18.6849\n",
      "Epoch 50/1000\n",
      "3/3 [==============================] - 1s 446ms/step - loss: 10.7609 - val_loss: 18.7120\n",
      "Epoch 51/1000\n",
      "3/3 [==============================] - 1s 448ms/step - loss: 10.4683 - val_loss: 20.1077\n",
      "Epoch 52/1000\n",
      "3/3 [==============================] - 1s 440ms/step - loss: 10.2433 - val_loss: 17.9554\n",
      "Epoch 53/1000\n",
      "3/3 [==============================] - 1s 436ms/step - loss: 10.5061 - val_loss: 19.6163\n",
      "Epoch 54/1000\n",
      "3/3 [==============================] - 1s 454ms/step - loss: 9.9340 - val_loss: 17.7468\n",
      "Epoch 55/1000\n",
      "3/3 [==============================] - 1s 445ms/step - loss: 9.7146 - val_loss: 19.8626\n",
      "Epoch 56/1000\n",
      "3/3 [==============================] - 1s 454ms/step - loss: 9.5836 - val_loss: 17.9996\n",
      "Epoch 57/1000\n",
      "3/3 [==============================] - 1s 426ms/step - loss: 9.4905 - val_loss: 19.3443\n",
      "Epoch 58/1000\n",
      "3/3 [==============================] - 1s 424ms/step - loss: 9.2731 - val_loss: 19.9724\n",
      "Epoch 59/1000\n",
      "3/3 [==============================] - 1s 444ms/step - loss: 9.1835 - val_loss: 17.0709\n",
      "Epoch 60/1000\n",
      "3/3 [==============================] - 1s 461ms/step - loss: 8.9414 - val_loss: 21.5657\n",
      "Epoch 61/1000\n",
      "3/3 [==============================] - 1s 436ms/step - loss: 9.0066 - val_loss: 18.3977\n",
      "Epoch 62/1000\n",
      "3/3 [==============================] - 1s 467ms/step - loss: 9.1358 - val_loss: 16.9815\n",
      "Epoch 63/1000\n",
      "3/3 [==============================] - 1s 435ms/step - loss: 8.8207 - val_loss: 18.3337\n",
      "Epoch 64/1000\n",
      "3/3 [==============================] - 1s 456ms/step - loss: 8.4445 - val_loss: 17.5388\n",
      "Epoch 65/1000\n",
      "3/3 [==============================] - 1s 451ms/step - loss: 8.2259 - val_loss: 16.8696\n",
      "Epoch 66/1000\n",
      "3/3 [==============================] - 1s 446ms/step - loss: 8.4537 - val_loss: 19.1448\n",
      "Epoch 67/1000\n",
      "3/3 [==============================] - 1s 444ms/step - loss: 8.3564 - val_loss: 16.8745\n",
      "Epoch 68/1000\n",
      "3/3 [==============================] - 1s 439ms/step - loss: 8.0973 - val_loss: 17.2783\n",
      "Epoch 69/1000\n",
      "3/3 [==============================] - 1s 445ms/step - loss: 8.1161 - val_loss: 20.3621\n",
      "Epoch 70/1000\n",
      "3/3 [==============================] - 1s 438ms/step - loss: 8.4263 - val_loss: 17.0981\n",
      "Epoch 71/1000\n",
      "3/3 [==============================] - 1s 439ms/step - loss: 8.0577 - val_loss: 19.9366\n",
      "Epoch 72/1000\n",
      "3/3 [==============================] - 1s 439ms/step - loss: 7.6726 - val_loss: 17.6727\n",
      "Epoch 73/1000\n",
      "3/3 [==============================] - 1s 429ms/step - loss: 7.6187 - val_loss: 18.5019\n",
      "Epoch 74/1000\n",
      "3/3 [==============================] - 1s 437ms/step - loss: 7.4266 - val_loss: 18.1101\n",
      "Epoch 75/1000\n",
      "3/3 [==============================] - 1s 439ms/step - loss: 7.2652 - val_loss: 19.1268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa38e8e1320>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRF_model.fit(train_x, train_y, batch_size=128, epochs=1000, validation_split=0.1, verbose=1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "3/3 [==============================] - 2s 569ms/step - loss: 0.3605 - val_loss: 0.1384\n",
      "Epoch 2/1000\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 0.1014 - val_loss: 0.1250\n",
      "Epoch 3/1000\n",
      "3/3 [==============================] - 0s 163ms/step - loss: 0.0888 - val_loss: 0.1033\n",
      "Epoch 4/1000\n",
      "3/3 [==============================] - 0s 165ms/step - loss: 0.0714 - val_loss: 0.0899\n",
      "Epoch 5/1000\n",
      "3/3 [==============================] - 1s 168ms/step - loss: 0.0652 - val_loss: 0.0916\n",
      "Epoch 6/1000\n",
      "3/3 [==============================] - 1s 167ms/step - loss: 0.0680 - val_loss: 0.0876\n",
      "Epoch 7/1000\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 0.0632 - val_loss: 0.0824\n",
      "Epoch 8/1000\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 0.0603 - val_loss: 0.0824\n",
      "Epoch 9/1000\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 0.0604 - val_loss: 0.0813\n",
      "Epoch 10/1000\n",
      "3/3 [==============================] - 1s 169ms/step - loss: 0.0594 - val_loss: 0.0781\n",
      "Epoch 11/1000\n",
      "3/3 [==============================] - 1s 168ms/step - loss: 0.0574 - val_loss: 0.0750\n",
      "Epoch 12/1000\n",
      "3/3 [==============================] - 1s 167ms/step - loss: 0.0560 - val_loss: 0.0732\n",
      "Epoch 13/1000\n",
      "3/3 [==============================] - 1s 169ms/step - loss: 0.0550 - val_loss: 0.0718\n",
      "Epoch 14/1000\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 0.0537 - val_loss: 0.0700\n",
      "Epoch 15/1000\n",
      "3/3 [==============================] - 1s 168ms/step - loss: 0.0521 - val_loss: 0.0680\n",
      "Epoch 16/1000\n",
      "3/3 [==============================] - 1s 167ms/step - loss: 0.0506 - val_loss: 0.0663\n",
      "Epoch 17/1000\n",
      "3/3 [==============================] - 1s 167ms/step - loss: 0.0492 - val_loss: 0.0644\n",
      "Epoch 18/1000\n",
      "3/3 [==============================] - 1s 169ms/step - loss: 0.0477 - val_loss: 0.0627\n",
      "Epoch 19/1000\n",
      "3/3 [==============================] - 1s 170ms/step - loss: 0.0462 - val_loss: 0.0611\n",
      "Epoch 20/1000\n",
      "3/3 [==============================] - 1s 170ms/step - loss: 0.0447 - val_loss: 0.0590\n",
      "Epoch 21/1000\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 0.0431 - val_loss: 0.0573\n",
      "Epoch 22/1000\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 0.0415 - val_loss: 0.0556\n",
      "Epoch 23/1000\n",
      "3/3 [==============================] - 1s 168ms/step - loss: 0.0401 - val_loss: 0.0540\n",
      "Epoch 24/1000\n",
      "3/3 [==============================] - 1s 167ms/step - loss: 0.0388 - val_loss: 0.0524\n",
      "Epoch 25/1000\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 0.0372 - val_loss: 0.0518\n",
      "Epoch 26/1000\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 0.0360 - val_loss: 0.0499\n",
      "Epoch 27/1000\n",
      "3/3 [==============================] - 1s 169ms/step - loss: 0.0348 - val_loss: 0.0495\n",
      "Epoch 28/1000\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 0.0338 - val_loss: 0.0482\n",
      "Epoch 29/1000\n",
      "3/3 [==============================] - 1s 169ms/step - loss: 0.0328 - val_loss: 0.0480\n",
      "Epoch 30/1000\n",
      "3/3 [==============================] - 1s 171ms/step - loss: 0.0318 - val_loss: 0.0470\n",
      "Epoch 31/1000\n",
      "3/3 [==============================] - 1s 170ms/step - loss: 0.0310 - val_loss: 0.0467\n",
      "Epoch 32/1000\n",
      "3/3 [==============================] - 0s 165ms/step - loss: 0.0303 - val_loss: 0.0473\n",
      "Epoch 33/1000\n",
      "3/3 [==============================] - 0s 165ms/step - loss: 0.0297 - val_loss: 0.0490\n",
      "Epoch 34/1000\n",
      "3/3 [==============================] - 0s 165ms/step - loss: 0.0291 - val_loss: 0.0442\n",
      "Epoch 35/1000\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 0.0286 - val_loss: 0.0444\n",
      "Epoch 36/1000\n",
      "3/3 [==============================] - 1s 171ms/step - loss: 0.0277 - val_loss: 0.0461\n",
      "Epoch 37/1000\n",
      "3/3 [==============================] - 1s 167ms/step - loss: 0.0271 - val_loss: 0.0427\n",
      "Epoch 38/1000\n",
      "3/3 [==============================] - 1s 168ms/step - loss: 0.0265 - val_loss: 0.0448\n",
      "Epoch 39/1000\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 0.0259 - val_loss: 0.0465\n",
      "Epoch 40/1000\n",
      "3/3 [==============================] - 1s 170ms/step - loss: 0.0255 - val_loss: 0.0416\n",
      "Epoch 41/1000\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 0.0247 - val_loss: 0.0460\n",
      "Epoch 42/1000\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 0.0245 - val_loss: 0.0422\n",
      "Epoch 43/1000\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 0.0244 - val_loss: 0.0468\n",
      "Epoch 44/1000\n",
      "3/3 [==============================] - 0s 165ms/step - loss: 0.0231 - val_loss: 0.0404\n",
      "Epoch 45/1000\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 0.0227 - val_loss: 0.0408\n",
      "Epoch 46/1000\n",
      "3/3 [==============================] - 0s 162ms/step - loss: 0.0221 - val_loss: 0.0431\n",
      "Epoch 47/1000\n",
      "3/3 [==============================] - 0s 165ms/step - loss: 0.0218 - val_loss: 0.0412\n",
      "Epoch 48/1000\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 0.0211 - val_loss: 0.0388\n",
      "Epoch 49/1000\n",
      "3/3 [==============================] - 0s 165ms/step - loss: 0.0208 - val_loss: 0.0429\n",
      "Epoch 50/1000\n",
      "3/3 [==============================] - 1s 168ms/step - loss: 0.0201 - val_loss: 0.0398\n",
      "Epoch 51/1000\n",
      "3/3 [==============================] - 1s 170ms/step - loss: 0.0198 - val_loss: 0.0417\n",
      "Epoch 52/1000\n",
      "3/3 [==============================] - 1s 170ms/step - loss: 0.0194 - val_loss: 0.0384\n",
      "Epoch 53/1000\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 0.0190 - val_loss: 0.0386\n",
      "Epoch 54/1000\n",
      "3/3 [==============================] - 0s 162ms/step - loss: 0.0189 - val_loss: 0.0459\n",
      "Epoch 55/1000\n",
      "3/3 [==============================] - 0s 163ms/step - loss: 0.0186 - val_loss: 0.0408\n",
      "Epoch 56/1000\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 0.0184 - val_loss: 0.0376\n",
      "Epoch 57/1000\n",
      "3/3 [==============================] - 0s 165ms/step - loss: 0.0202 - val_loss: 0.0470\n",
      "Epoch 58/1000\n",
      "3/3 [==============================] - 0s 165ms/step - loss: 0.0196 - val_loss: 0.0404\n",
      "Epoch 59/1000\n",
      "3/3 [==============================] - 0s 165ms/step - loss: 0.0201 - val_loss: 0.0427\n",
      "Epoch 60/1000\n",
      "3/3 [==============================] - 0s 165ms/step - loss: 0.0185 - val_loss: 0.0388\n",
      "Epoch 61/1000\n",
      "3/3 [==============================] - 0s 167ms/step - loss: 0.0173 - val_loss: 0.0367\n",
      "Epoch 62/1000\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 0.0177 - val_loss: 0.0441\n",
      "Epoch 63/1000\n",
      "3/3 [==============================] - 0s 163ms/step - loss: 0.0170 - val_loss: 0.0406\n",
      "Epoch 64/1000\n",
      "3/3 [==============================] - 1s 168ms/step - loss: 0.0167 - val_loss: 0.0406\n",
      "Epoch 65/1000\n",
      "3/3 [==============================] - 1s 170ms/step - loss: 0.0157 - val_loss: 0.0408\n",
      "Epoch 66/1000\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 0.0154 - val_loss: 0.0399\n",
      "Epoch 67/1000\n",
      "3/3 [==============================] - 1s 168ms/step - loss: 0.0154 - val_loss: 0.0391\n",
      "Epoch 68/1000\n",
      "3/3 [==============================] - 0s 165ms/step - loss: 0.0150 - val_loss: 0.0398\n",
      "Epoch 69/1000\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 0.0145 - val_loss: 0.0415\n",
      "Epoch 70/1000\n",
      "3/3 [==============================] - 0s 166ms/step - loss: 0.0145 - val_loss: 0.0399\n",
      "Epoch 71/1000\n",
      "3/3 [==============================] - 1s 172ms/step - loss: 0.0143 - val_loss: 0.0404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa382b6a710>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Softmax_model.fit(train_x, train_y, batch_size=128, epochs=1000, validation_split=0.1, verbose=1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODELS = [FFN_CRF, CRF_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_MODELS = [Softmax_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(type=None, scaled_page='normal'):\n",
    "    if type is None:\n",
    "        print(\"Please assign type of test_data\")\n",
    "        return\n",
    "    if type != 'EVENT_SOURCE':\n",
    "        storage.test_file = 'NORMAL'\n",
    "        test_urls = [rec['Page URL'] for rec in storage.iter_test_records()]\n",
    "        test_X_one, test_y_one, test_page_positions_one = storage.get_test_Xy(validate=False, scaled_page=scaled_page)\n",
    "        print(\"pages: {}  domains: {}\".format(len(test_urls), len({get_domain(url) for url in test_urls})))\n",
    "        if type == 'NORMAL':\n",
    "            return test_X_one, test_y_one, test_page_positions_one\n",
    "    if type != 'NORMAL':\n",
    "        storage.test_file = 'EVENT_SOURCE'\n",
    "        test_urls = [rec['Page URL'] for rec in storage.iter_test_records()]\n",
    "        test_X_two, test_y_two, test_page_positions_two = storage.get_test_Xy(validate=False, scaled_page=scaled_page)\n",
    "        print(\"pages: {}  domains: {}\".format(len(test_urls), len({get_domain(url) for url in test_urls})))\n",
    "        if type == 'EVENT_SOURCE':\n",
    "            return test_X_two, test_y_two, test_page_positions_two\n",
    "    test_X_raw = test_X_one + test_X_two\n",
    "    test_y = test_y_one + test_y_two\n",
    "    test_positions = test_page_positions_one + test_page_positions_two\n",
    "    return test_X_raw, test_y, test_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distribution_to_label(predict_y):\n",
    "    if len(predict_y.shape) != 3:\n",
    "        return predict_y\n",
    "    label_y = list()\n",
    "    for page in predict_y:\n",
    "        tmp = list()\n",
    "        for lab in page:\n",
    "            lab = lab.tolist()\n",
    "            tmp.append(lab.index(max(lab)))\n",
    "        label_y.append(tmp)\n",
    "    return label_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_predict_and_evaluate(models, x_test, y_test, evaluate_labels):\n",
    "    for idx, model in enumerate(models):\n",
    "        print(f\"Start predict model {idx}\")\n",
    "        print(model.summary())\n",
    "        print(\"--------------------------\")\n",
    "        predict_y = model.predict(x_test)\n",
    "        predict_y = label_distribution_to_label(predict_y)\n",
    "        predict_y = np.asarray([[idx2tag.get(lab) for lab in page] for page in predict_y])\n",
    "        print(flat_classification_report(y_test, predict_y, labels=evaluate_labels, digits=len(evaluate_labels)))\n",
    "        print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages: 100  domains: 58\n"
     ]
    }
   ],
   "source": [
    "# test_X_raw, test_y, test_page_positions = get_test_data('EVENT_SOURCE')\n",
    "test_X_raw, test_y, test_page_positions = get_test_data('NORMAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_urls = [rec['Page URL'] for rec in storage.iter_test_records()]\n",
    "test_groups = set([get_domain(url) for url in test_urls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_test_x, chunks_test_y, chunks_test_positions = get_chunks_data(test_X_raw, test_y, test_page_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_features, test_tag_features = get_token_tag_features_from_chunks(chunks_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start encode first seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fe8a1ee60342cd84ebe45c02eee210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=125)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start encode second seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00002f3bc8d741af847dd91915fdb308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=125)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_first_emb, test_second_emb, test_full_text_emb = page_to_two_bert_embeddings(test_token_features, pbert.get_tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/DL/lib/python3.6/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "np.save('embedding/test/first.npy', test_first_emb)\n",
    "np.save('embedding/test/second.npy', test_second_emb)\n",
    "np.save('embedding/test/full_text.npy', test_full_text_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_first_emb = np.load('embedding/test/first.npy', allow_pickle=True)\n",
    "test_second_emb = np.load('embedding/test/second.npy', allow_pickle=True)\n",
    "test_full_text_emb = np.load('embedding/test/full_text.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First emb:(185, 768)\n",
      "Second emb:(185, 768)\n",
      "Full_text emb:(185, 1536)\n"
     ]
    }
   ],
   "source": [
    "print(f\"First emb:{test_first_emb[0].shape}\")\n",
    "print(f\"Second emb:{test_second_emb[0].shape}\")\n",
    "print(f\"Full_text emb:{test_full_text_emb[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tag_info_list = test_tag_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  1.0638181757827387\n",
      "Max_node:  59\n"
     ]
    }
   ],
   "source": [
    "max_node = -1\n",
    "page_sum = 0\n",
    "for page in test_tag_emb_features:\n",
    "    sum = 0\n",
    "    for node in page:\n",
    "        sum+=len(node)\n",
    "        if len(node) > max_node:\n",
    "            max_node = len(node)\n",
    "    page_sum+=sum/len(page)\n",
    "print(\"Average: \", page_sum/len(train_token_features))\n",
    "print(\"Max_node: \",max_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_emb_x = feature_pad_to_npdata(test_full_text_emb) # full text emb / two-bert emb\n",
    "# test_text_emb_x = feature_pad_to_npdata(test_text_emb) # text emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tag_x = feature_pad_to_npdata(test_tag_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_positions_x = feature_pad_to_npdata(chunks_test_positions)\n",
    "test_tag_x = np.concatenate([test_tag_x, test_positions_x], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_info_x = np.concatenate([test_text_emb_x, test_tag_x], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_info_x\n",
    "# x_test = test_text_emb_x\n",
    "# x_test = test_tag_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 512, 1546)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pad_sequences([[tag2idx.get(l) for l in lab] for lab in chunks_test_y], maxlen=max_page_seq, padding=\"post\", truncating=\"post\", value=tag2idx[\"[PAD]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [[idx2tag.get(lab) for lab in page] for page in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_labels = ['PREV', 'PAGE', 'NEXT', '[PAD]', 'O']\n",
    "evaluate_labels = ['PAGE', 'NEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predict model 0\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_2 (Masking)          (None, 512, 1546)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 512, 400)          2795200   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512, 5)            2005      \n",
      "=================================================================\n",
      "Total params: 2,797,205\n",
      "Trainable params: 2,797,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/DL/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass labels=['PAGE', 'NEXT'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PAGE       0.68      0.74      0.71       279\n",
      "        NEXT       0.67      0.04      0.08        49\n",
      "\n",
      "   micro avg       0.68      0.63      0.66       328\n",
      "   macro avg       0.68      0.39      0.39       328\n",
      "weighted avg       0.68      0.63      0.62       328\n",
      "\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "recursive_predict_and_evaluate(TEST_MODELS, x_test, y_test, evaluate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
