{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "from urllib.parse import urlparse, urlsplit, parse_qs, parse_qsl\n",
    "\n",
    "import numpy as np\n",
    "import parsel\n",
    "from sklearn_crfsuite.metrics import flat_classification_report, sequence_accuracy_score\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from autopager.storage import Storage\n",
    "from autopager.htmlutils import (get_link_text, get_text_around_selector_list,\n",
    "                                 get_link_href, get_selector_root)\n",
    "from autopager.utils import (\n",
    "    get_domain, normalize_whitespaces, normalize, ngrams, tokenize, ngrams_wb, replace_digits\n",
    ")\n",
    "from autopager.model import link_to_features, _num_tokens_feature, _elem_attr\n",
    "from autopager import AUTOPAGER_LIMITS\n",
    "from autopager.parserutils import (TagParser, MyHTMLParser, draw_scaled_page, position_check, compare_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus)!=0:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPUs visible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = Storage()\n",
    "urls = [rec['Page URL'] for rec in storage.iter_records(contain_button = True, file_type='T')]\n",
    "groups = [get_domain(url) for url in urls]\n",
    "train_groups_set = set(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: Get Page 1 (Encoding: UTF-8)records ... (len: 303)\n",
      "Finish: Get Page 2 (Encoding: UTF-8)records ... (len: 243)\n",
      "Finish: Get Page 3 (Encoding: UTF-8)records ... (len: 119)\n",
      "Finish: Get Page 4 (Encoding: UTF-8)records ... (len: 944)\n",
      "Finish: Get Page 5 (Encoding: UTF-8)records ... (len: 93)\n",
      "Finish: Get Page 6 (Encoding: UTF-8)records ... (len: 994)\n",
      "Finish: Get Page 7 (Encoding: UTF-8)records ... (len: 1014)\n",
      "Finish: Get Page 8 (Encoding: UTF-8)records ... (len: 7)\n",
      "Finish: Get Page 9 (Encoding: UTF-8)records ... (len: 288)\n",
      "Finish: Get Page 10 (Encoding: UTF-8)records ... (len: 678)\n",
      "Finish: Get Page 11 (Encoding: UTF-8)records ... (len: 789)\n",
      "Finish: Get Page 12 (Encoding: UTF-8)records ... (len: 814)\n",
      "Finish: Get Page 13 (Encoding: UTF-8)records ... (len: 814)\n",
      "Finish: Get Page 14 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 15 (Encoding: UTF-8)records ... (len: 168)\n",
      "Finish: Get Page 16 (Encoding: UTF-8)records ... (len: 91)\n",
      "Finish: Get Page 17 (Encoding: UTF-8)records ... (len: 100)\n",
      "Finish: Get Page 18 (Encoding: UTF-8)records ... (len: 102)\n",
      "Finish: Get Page 19 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 20 (Encoding: UTF-8)records ... (len: 215)\n",
      "Finish: Get Page 21 (Encoding: UTF-8)records ... (len: 158)\n",
      "Finish: Get Page 22 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 23 (Encoding: UTF-8)records ... (len: 181)\n",
      "Finish: Get Page 24 (Encoding: UTF-8)records ... (len: 10)\n",
      "Finish: Get Page 25 (Encoding: UTF-8)records ... (len: 165)\n",
      "Finish: Get Page 26 (Encoding: UTF-8)records ... (len: 147)\n",
      "Finish: Get Page 27 (Encoding: UTF-8)records ... (len: 329)\n",
      "Finish: Get Page 28 (Encoding: UTF-8)records ... (len: 268)\n",
      "Finish: Get Page 29 (Encoding: UTF-8)records ... (len: 643)\n",
      "Finish: Get Page 30 (Encoding: UTF-8)records ... (len: 645)\n",
      "Finish: Get Page 31 (Encoding: UTF-8)records ... (len: 647)\n",
      "Finish: Get Page 32 (Encoding: UTF-8)records ... (len: 625)\n",
      "Finish: Get Page 33 (Encoding: UTF-8)records ... (len: 108)\n",
      "Finish: Get Page 34 (Encoding: UTF-8)records ... (len: 109)\n",
      "Finish: Get Page 35 (Encoding: UTF-8)records ... (len: 84)\n",
      "Finish: Get Page 36 (Encoding: UTF-8)records ... (len: 718)\n",
      "Finish: Get Page 37 (Encoding: UTF-8)records ... (len: 723)\n",
      "Finish: Get Page 38 (Encoding: UTF-8)records ... (len: 703)\n",
      "Finish: Get Page 39 (Encoding: UTF-8)records ... (len: 70)\n",
      "Finish: Get Page 40 (Encoding: UTF-8)records ... (len: 102)\n",
      "Finish: Get Page 41 (Encoding: UTF-8)records ... (len: 75)\n",
      "Finish: Get Page 42 (Encoding: UTF-8)records ... (len: 52)\n",
      "Finish: Get Page 43 (Encoding: UTF-8)records ... (len: 198)\n",
      "Finish: Get Page 44 (Encoding: UTF-8)records ... (len: 200)\n",
      "Finish: Get Page 45 (Encoding: UTF-8)records ... (len: 206)\n",
      "Finish: Get Page 46 (Encoding: UTF-8)records ... (len: 82)\n",
      "Finish: Get Page 47 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 48 (Encoding: UTF-8)records ... (len: 34)\n",
      "Finish: Get Page 49 (Encoding: UTF-8)records ... (len: 62)\n",
      "Finish: Get Page 50 (Encoding: UTF-8)records ... (len: 15)\n",
      "Finish: Get Page 51 (Encoding: UTF-8)records ... (len: 266)\n",
      "Finish: Get Page 52 (Encoding: UTF-8)records ... (len: 228)\n",
      "Finish: Get Page 53 (Encoding: UTF-8)records ... (len: 273)\n",
      "Finish: Get Page 54 (Encoding: cp1252)records ... (len: 283)\n",
      "Finish: Get Page 55 (Encoding: cp1252)records ... (len: 285)\n",
      "Finish: Get Page 56 (Encoding: cp1252)records ... (len: 520)\n",
      "Finish: Get Page 57 (Encoding: cp1252)records ... (len: 463)\n",
      "Finish: Get Page 58 (Encoding: UTF-8)records ... (len: 130)\n",
      "Finish: Get Page 59 (Encoding: UTF-8)records ... (len: 58)\n",
      "Finish: Get Page 60 (Encoding: UTF-8)records ... (len: 54)\n",
      "Finish: Get Page 61 (Encoding: UTF-8)records ... (len: 55)\n",
      "Finish: Get Page 62 (Encoding: UTF-8)records ... (len: 55)\n",
      "Finish: Get Page 63 (Encoding: UTF-8)records ... (len: 84)\n",
      "Finish: Get Page 64 (Encoding: UTF-8)records ... (len: 95)\n",
      "Finish: Get Page 65 (Encoding: iso8859-15)records ... (len: 702)\n",
      "Finish: Get Page 66 (Encoding: iso8859-15)records ... (len: 535)\n",
      "Finish: Get Page 67 (Encoding: iso8859-15)records ... (len: 538)\n",
      "Finish: Get Page 68 (Encoding: UTF-8)records ... (len: 37)\n",
      "Finish: Get Page 69 (Encoding: UTF-8)records ... (len: 312)\n",
      "Finish: Get Page 70 (Encoding: UTF-8)records ... (len: 127)\n",
      "Finish: Get Page 71 (Encoding: UTF-8)records ... (len: 104)\n",
      "Finish: Get Page 72 (Encoding: UTF-8)records ... (len: 92)\n",
      "Finish: Get Page 73 (Encoding: UTF-8)records ... (len: 149)\n",
      "Finish: Get Page 74 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 75 (Encoding: UTF-8)records ... (len: 386)\n",
      "Finish: Get Page 76 (Encoding: UTF-8)records ... (len: 319)\n",
      "Finish: Get Page 77 (Encoding: UTF-8)records ... (len: 114)\n",
      "Finish: Get Page 78 (Encoding: UTF-8)records ... (len: 118)\n",
      "Finish: Get Page 79 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 80 (Encoding: cp1251)records ... (len: 266)\n",
      "Finish: Get Page 81 (Encoding: cp1251)records ... (len: 274)\n",
      "Finish: Get Page 82 (Encoding: UTF-8)records ... (len: 139)\n",
      "Finish: Get Page 83 (Encoding: UTF-8)records ... (len: 408)\n",
      "Finish: Get Page 84 (Encoding: UTF-8)records ... (len: 94)\n",
      "Finish: Get Page 85 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 86 (Encoding: UTF-8)records ... (len: 230)\n",
      "Finish: Get Page 87 (Encoding: UTF-8)records ... (len: 106)\n",
      "Finish: Get Page 88 (Encoding: UTF-8)records ... (len: 110)\n",
      "Finish: Get Page 89 (Encoding: UTF-8)records ... (len: 448)\n",
      "Finish: Get Page 90 (Encoding: UTF-8)records ... (len: 452)\n",
      "Finish: Get Page 91 (Encoding: UTF-8)records ... (len: 2389)\n",
      "Finish: Get Page 92 (Encoding: UTF-8)records ... (len: 2379)\n",
      "Finish: Get Page 93 (Encoding: UTF-8)records ... (len: 160)\n",
      "Finish: Get Page 94 (Encoding: UTF-8)records ... (len: 74)\n",
      "Finish: Get Page 95 (Encoding: UTF-8)records ... (len: 143)\n",
      "Finish: Get Page 96 (Encoding: UTF-8)records ... (len: 133)\n",
      "Finish: Get Page 97 (Encoding: UTF-8)records ... (len: 163)\n",
      "Finish: Get Page 98 (Encoding: UTF-8)records ... (len: 378)\n",
      "Finish: Get Page 99 (Encoding: UTF-8)records ... (len: 120)\n",
      "Finish: Get Page 100 (Encoding: UTF-8)records ... (len: 124)\n",
      "Finish: Get Page 101 (Encoding: UTF-8)records ... (len: 122)\n",
      "Finish: Get Page 102 (Encoding: UTF-8)records ... (len: 430)\n",
      "Finish: Get Page 103 (Encoding: UTF-8)records ... (len: 260)\n",
      "Finish: Get Page 104 (Encoding: UTF-8)records ... (len: 252)\n",
      "Finish: Get Page 105 (Encoding: UTF-8)records ... (len: 439)\n",
      "Finish: Get Page 106 (Encoding: UTF-8)records ... (len: 422)\n",
      "Finish: Get Page 107 (Encoding: UTF-8)records ... (len: 423)\n",
      "Finish: Get Page 108 (Encoding: UTF-8)records ... (len: 233)\n",
      "Finish: Get Page 109 (Encoding: cp1252)records ... (len: 155)\n",
      "Finish: Get Page 110 (Encoding: cp1252)records ... (len: 161)\n",
      "Finish: Get Page 111 (Encoding: cp1252)records ... (len: 131)\n",
      "Finish: Get Page 112 (Encoding: cp1252)records ... (len: 131)\n",
      "Finish: Get Page 113 (Encoding: UTF-8)records ... (len: 72)\n",
      "Finish: Get Page 114 (Encoding: UTF-8)records ... (len: 70)\n",
      "Finish: Get Page 115 (Encoding: UTF-8)records ... (len: 126)\n",
      "Finish: Get Page 116 (Encoding: UTF-8)records ... (len: 90)\n",
      "Finish: Get Page 117 (Encoding: UTF-8)records ... (len: 89)\n",
      "Finish: Get Page 118 (Encoding: UTF-8)records ... (len: 79)\n",
      "Finish: Get Page 119 (Encoding: UTF-8)records ... (len: 133)\n",
      "Finish: Get Page 120 (Encoding: UTF-8)records ... (len: 89)\n",
      "Finish: Get Page 121 (Encoding: UTF-8)records ... (len: 81)\n",
      "Finish: Get Page 122 (Encoding: UTF-8)records ... (len: 167)\n",
      "Finish: Get Page 123 (Encoding: UTF-8)records ... (len: 165)\n",
      "Finish: Get Page 124 (Encoding: UTF-8)records ... (len: 124)\n",
      "Finish: Get Page 125 (Encoding: UTF-8)records ... (len: 16)\n",
      "Finish: Get Page 126 (Encoding: UTF-8)records ... (len: 8)\n",
      "Finish: Get Page 127 (Encoding: UTF-8)records ... (len: 154)\n",
      "Finish: Get Page 128 (Encoding: UTF-8)records ... (len: 51)\n",
      "Finish: Get Page 129 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 130 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 131 (Encoding: UTF-8)records ... (len: 53)\n",
      "Finish: Get Page 132 (Encoding: UTF-8)records ... (len: 306)\n",
      "Finish: Get Page 133 (Encoding: UTF-8)records ... (len: 309)\n",
      "Finish: Get Page 134 (Encoding: UTF-8)records ... (len: 103)\n",
      "Finish: Get Page 135 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 136 (Encoding: UTF-8)records ... (len: 159)\n",
      "Finish: Get Page 137 (Encoding: UTF-8)records ... (len: 35)\n",
      "Finish: Get Page 138 (Encoding: UTF-8)records ... (len: 112)\n",
      "Finish: Get Page 139 (Encoding: UTF-8)records ... (len: 117)\n",
      "Finish: Get Page 140 (Encoding: UTF-8)records ... (len: 142)\n",
      "Finish: Get Page 141 (Encoding: UTF-8)records ... (len: 116)\n",
      "Finish: Get Page 142 (Encoding: UTF-8)records ... (len: 31)\n",
      "Finish: Get Page 143 (Encoding: cp1252)records ... (len: 84)\n",
      "Finish: Get Page 144 (Encoding: cp1252)records ... (len: 134)\n",
      "Finish: Get Page 145 (Encoding: cp1252)records ... (len: 139)\n",
      "Finish: Get Page 146 (Encoding: cp1252)records ... (len: 95)\n",
      "Finish: Get Page 147 (Encoding: cp1252)records ... (len: 130)\n",
      "Finish: Get Page 148 (Encoding: UTF-8)records ... (len: 478)\n",
      "Finish: Get Page 149 (Encoding: UTF-8)records ... (len: 365)\n",
      "Finish: Get Page 150 (Encoding: UTF-8)records ... (len: 368)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: Get Page 151 (Encoding: UTF-8)records ... (len: 369)\n",
      "Finish: Get Page 152 (Encoding: cp1252)records ... (len: 294)\n",
      "Finish: Get Page 153 (Encoding: UTF-8)records ... (len: 271)\n",
      "Finish: Get Page 154 (Encoding: UTF-8)records ... (len: 300)\n",
      "Finish: Get Page 155 (Encoding: UTF-8)records ... (len: 314)\n",
      "Finish: Get Page 156 (Encoding: UTF-8)records ... (len: 278)\n",
      "Finish: Get Page 157 (Encoding: UTF-8)records ... (len: 288)\n",
      "Finish: Get Page 158 (Encoding: UTF-8)records ... (len: 178)\n",
      "Finish: Get Page 159 (Encoding: UTF-8)records ... (len: 108)\n",
      "Finish: Get Page 160 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 161 (Encoding: UTF-8)records ... (len: 101)\n",
      "Finish: Get Page 162 (Encoding: UTF-8)records ... (len: 308)\n",
      "Finish: Get Page 163 (Encoding: UTF-8)records ... (len: 298)\n",
      "Finish: Get Page 164 (Encoding: UTF-8)records ... (len: 285)\n",
      "Finish: Get Page 165 (Encoding: UTF-8)records ... (len: 221)\n",
      "Finish: Get Page 166 (Encoding: UTF-8)records ... (len: 21)\n",
      "Finish: Get Page 167 (Encoding: UTF-8)records ... (len: 94)\n",
      "Finish: Get Page 168 (Encoding: UTF-8)records ... (len: 99)\n",
      "Finish: Get Page 169 (Encoding: UTF-8)records ... (len: 103)\n",
      "Finish: Get Page 170 (Encoding: UTF-8)records ... (len: 210)\n",
      "Finish: Get Page 171 (Encoding: UTF-8)records ... (len: 208)\n",
      "Finish: Get Page 172 (Encoding: UTF-8)records ... (len: 179)\n",
      "Finish: Get Page 173 (Encoding: UTF-8)records ... (len: 461)\n",
      "Finish: Get Page 174 (Encoding: UTF-8)records ... (len: 340)\n",
      "Finish: Get Page 175 (Encoding: UTF-8)records ... (len: 188)\n",
      "Finish: Get Page 176 (Encoding: UTF-8)records ... (len: 195)\n",
      "Finish: Get Page 177 (Encoding: UTF-8)records ... (len: 40)\n",
      "Finish: Get Page 178 (Encoding: UTF-8)records ... (len: 256)\n",
      "Finish: Get Page 179 (Encoding: UTF-8)records ... (len: 258)\n",
      "Finish: Get Page 180 (Encoding: UTF-8)records ... (len: 256)\n",
      "Finish: Get Page 181 (Encoding: UTF-8)records ... (len: 227)\n",
      "Finish: Get Page 182 (Encoding: UTF-8)records ... (len: 229)\n",
      "Finish: Get Page 183 (Encoding: UTF-8)records ... (len: 189)\n",
      "Finish: Get Page 184 (Encoding: UTF-8)records ... (len: 401)\n",
      "Finish: Get Page 185 (Encoding: UTF-8)records ... (len: 954)\n",
      "Finish: Get Page 186 (Encoding: UTF-8)records ... (len: 427)\n",
      "Finish: Get Page 187 (Encoding: UTF-8)records ... (len: 426)\n",
      "Finish: Get Page 188 (Encoding: UTF-8)records ... (len: 191)\n",
      "Finish: Get Page 189 (Encoding: UTF-8)records ... (len: 129)\n",
      "Finish: Get Page 190 (Encoding: UTF-8)records ... (len: 254)\n",
      "Finish: Get Page 191 (Encoding: UTF-8)records ... (len: 247)\n",
      "Finish: Get Page 192 (Encoding: UTF-8)records ... (len: 243)\n",
      "Finish: Get Page 193 (Encoding: UTF-8)records ... (len: 89)\n",
      "Finish: Get Page 194 (Encoding: UTF-8)records ... (len: 57)\n",
      "Finish: Get Page 195 (Encoding: UTF-8)records ... (len: 95)\n",
      "Finish: Get Page 196 (Encoding: UTF-8)records ... (len: 238)\n",
      "Finish: Get Page 197 (Encoding: UTF-8)records ... (len: 182)\n",
      "Finish: Get Page 198 (Encoding: UTF-8)records ... (len: 183)\n",
      "Finish: Get Page 199 (Encoding: UTF-8)records ... (len: 148)\n",
      "Finish: Get Page 200 (Encoding: UTF-8)records ... (len: 347)\n",
      "Finish: Get Page 201 (Encoding: UTF-8)records ... (len: 199)\n",
      "Finish: Get Page 202 (Encoding: UTF-8)records ... (len: 200)\n",
      "Finish: Get Page 203 (Encoding: UTF-8)records ... (len: 361)\n",
      "Finish: Get Page 204 (Encoding: UTF-8)records ... (len: 356)\n",
      "Finish: Get Page 205 (Encoding: UTF-8)records ... (len: 360)\n",
      "Finish: Get Page 206 (Encoding: UTF-8)records ... (len: 331)\n",
      "Finish: Get Page 207 (Encoding: UTF-8)records ... (len: 498)\n",
      "Finish: Get Page 208 (Encoding: UTF-8)records ... (len: 499)\n",
      "Finish: Get Page 209 (Encoding: UTF-8)records ... (len: 497)\n",
      "Finish: Get Page 210 (Encoding: cp1252)records ... (len: 47)\n",
      "Finish: Get Page 211 (Encoding: UTF-8)records ... (len: 145)\n",
      "Finish: Get Page 212 (Encoding: UTF-8)records ... (len: 147)\n",
      "Finish: Get Page 213 (Encoding: UTF-8)records ... (len: 111)\n",
      "Finish: Get Page 214 (Encoding: UTF-8)records ... (len: 429)\n",
      "Finish: Get Page 215 (Encoding: UTF-8)records ... (len: 160)\n",
      "Finish: Get Page 216 (Encoding: UTF-8)records ... (len: 162)\n",
      "Finish: Get Page 217 (Encoding: UTF-8)records ... (len: 123)\n",
      "Finish: Get Page 218 (Encoding: UTF-8)records ... (len: 83)\n",
      "Finish: Get Page 219 (Encoding: UTF-8)records ... (len: 84)\n",
      "Finish: Get Page 220 (Encoding: UTF-8)records ... (len: 72)\n",
      "Finish: Get Page 221 (Encoding: UTF-8)records ... (len: 207)\n",
      "Finish: Get Page 222 (Encoding: UTF-8)records ... (len: 202)\n",
      "Finish: Get Page 223 (Encoding: UTF-8)records ... (len: 103)\n",
      "Finish: Get Page 224 (Encoding: cp1252)records ... (len: 59)\n",
      "Finish: Get Page 225 (Encoding: UTF-8)records ... (len: 58)\n",
      "Finish: Get Page 226 (Encoding: UTF-8)records ... (len: 346)\n",
      "Finish: Get Page 227 (Encoding: UTF-8)records ... (len: 281)\n",
      "Finish: Get Page 228 (Encoding: UTF-8)records ... (len: 164)\n",
      "Finish: Get Page 229 (Encoding: UTF-8)records ... (len: 66)\n",
      "Finish: Get Page 230 (Encoding: UTF-8)records ... (len: 69)\n",
      "Finish: Get Page 231 (Encoding: UTF-8)records ... (len: 613)\n",
      "Finish: Get Page 232 (Encoding: cp1252)records ... (len: 74)\n",
      "Finish: Get Page 233 (Encoding: UTF-8)records ... (len: 61)\n",
      "Finish: Get Page 234 (Encoding: UTF-8)records ... (len: 258)\n",
      "Finish: Get Page 235 (Encoding: UTF-8)records ... (len: 369)\n",
      "Finish: Get Page 236 (Encoding: UTF-8)records ... (len: 226)\n",
      "Finish: Get Page 237 (Encoding: UTF-8)records ... (len: 286)\n",
      "Finish: Get Page 238 (Encoding: UTF-8)records ... (len: 190)\n",
      "Finish: Get Page 239 (Encoding: cp1252)records ... (len: 256)\n",
      "Finish: Get Page 240 (Encoding: cp1252)records ... (len: 148)\n",
      "Finish: Get Page 241 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 242 (Encoding: UTF-8)records ... (len: 164)\n",
      "Finish: Get Page 243 (Encoding: UTF-8)records ... (len: 169)\n",
      "Finish: Get Page 244 (Encoding: UTF-8)records ... (len: 139)\n",
      "Finish: Get Page 245 (Encoding: UTF-8)records ... (len: 131)\n",
      "Finish: Get Page 246 (Encoding: cp1252)records ... (len: 528)\n",
      "Finish: Get Page 247 (Encoding: UTF-8)records ... (len: 217)\n",
      "Finish: Get Page 248 (Encoding: UTF-8)records ... (len: 119)\n",
      "Finish: Get Page 249 (Encoding: UTF-8)records ... (len: 61)\n",
      "Finish: Get Page 250 (Encoding: UTF-8)records ... (len: 107)\n",
      "Finish: Get Page 251 (Encoding: cp1252)records ... (len: 467)\n",
      "Finish: Get Page 252 (Encoding: cp1252)records ... (len: 469)\n",
      "Finish: Get Page 253 (Encoding: UTF-8)records ... (len: 144)\n",
      "Finish: Get Page 254 (Encoding: UTF-8)records ... (len: 43)\n",
      "Finish: Get Page 255 (Encoding: UTF-8)records ... (len: 44)\n",
      "Finish: Get Page 257 (Encoding: UTF-8)records ... (len: 385)\n",
      "Finish: Get Page 258 (Encoding: UTF-8)records ... (len: 355)\n",
      "Finish: Get Page 259 (Encoding: UTF-8)records ... (len: 72)\n",
      "Finish: Get Page 260 (Encoding: UTF-8)records ... (len: 73)\n",
      "Finish: Get Page 261 (Encoding: UTF-8)records ... (len: 306)\n",
      "Finish: Get Page 262 (Encoding: UTF-8)records ... (len: 129)\n",
      "Finish: Get Page 263 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 264 (Encoding: UTF-8)records ... (len: 53)\n",
      "Finish: Get Page 265 (Encoding: UTF-8)records ... (len: 199)\n",
      "Finish: Get Page 266 (Encoding: UTF-8)records ... (len: 86)\n",
      "Finish: Get Page 267 (Encoding: UTF-8)records ... (len: 131)\n",
      "Finish: Get Page 268 (Encoding: UTF-8)records ... (len: 175)\n",
      "Finish: Get Page 269 (Encoding: UTF-8)records ... (len: 176)\n",
      "Finish: Get Page 271 (Encoding: UTF-8)records ... (len: 302)\n",
      "Finish: Get Page 272 (Encoding: UTF-8)records ... (len: 304)\n",
      "Finish: Get Page 273 (Encoding: UTF-8)records ... (len: 180)\n",
      "Finish: Get Page 274 (Encoding: UTF-8)records ... (len: 529)\n",
      "Finish: Get Page 275 (Encoding: UTF-8)records ... (len: 310)\n",
      "Finish: Get Page 276 (Encoding: UTF-8)records ... (len: 80)\n",
      "Finish: Get Page 277 (Encoding: UTF-8)records ... (len: 165)\n",
      "Finish: Get Page 278 (Encoding: UTF-8)records ... (len: 337)\n",
      "Finish: Get Page 279 (Encoding: cp1252)records ... (len: 253)\n",
      "Finish: Get Page 280 (Encoding: cp1252)records ... (len: 239)\n",
      "Finish: Get Page 281 (Encoding: cp1252)records ... (len: 243)\n",
      "Finish: Get Page 282 (Encoding: UTF-8)records ... (len: 68)\n",
      "Finish: Get Page 283 (Encoding: UTF-8)records ... (len: 59)\n",
      "Finish: Get Page 284 (Encoding: cp1252)records ... (len: 130)\n",
      "Finish: Get Page 285 (Encoding: UTF-8)records ... (len: 139)\n",
      "Finish: Get Page 286 (Encoding: UTF-8)records ... (len: 166)\n",
      "Finish: Get Page 287 (Encoding: UTF-8)records ... (len: 82)\n",
      "Finish: Get Page 288 (Encoding: UTF-8)records ... (len: 140)\n",
      "Finish: Get Page 289 (Encoding: UTF-8)records ... (len: 44)\n",
      "Finish: Get Page 290 (Encoding: UTF-8)records ... (len: 330)\n",
      "Finish: Get Page 291 (Encoding: UTF-8)records ... (len: 331)\n",
      "Finish: Get Page 292 (Encoding: UTF-8)records ... (len: 280)\n",
      "Finish: Get Page 293 (Encoding: UTF-8)records ... (len: 74)\n",
      "Finish: Get Page 294 (Encoding: UTF-8)records ... (len: 63)\n",
      "Finish: Get Page 295 (Encoding: UTF-8)records ... (len: 65)\n",
      "Finish: Get Page 296 (Encoding: UTF-8)records ... (len: 20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: Get Page 297 (Encoding: UTF-8)records ... (len: 367)\n",
      "Finish: Get Page 298 (Encoding: UTF-8)records ... (len: 371)\n",
      "Finish: Get Page 299 (Encoding: UTF-8)records ... (len: 361)\n",
      "Finish: Get Page 300 (Encoding: UTF-8)records ... (len: 361)\n",
      "Finish: Get Page 301 (Encoding: UTF-8)records ... (len: 364)\n",
      "Finish: Get Page 302 (Encoding: UTF-8)records ... (len: 170)\n",
      "Finish: Get Page 303 (Encoding: UTF-8)records ... (len: 154)\n",
      "Finish: Get Page 304 (Encoding: cp1252)records ... (len: 117)\n",
      "Finish: Get Page 305 (Encoding: UTF-8)records ... (len: 1987)\n",
      "Finish: Get Page 306 (Encoding: UTF-8)records ... (len: 59)\n",
      "Finish: Get Page 307 (Encoding: UTF-8)records ... (len: 60)\n",
      "Finish: Get Page 308 (Encoding: UTF-8)records ... (len: 60)\n",
      "Finish: Get Page 309 (Encoding: UTF-8)records ... (len: 145)\n",
      "Finish: Get Page 310 (Encoding: UTF-8)records ... (len: 116)\n",
      "Finish: Get Page 311 (Encoding: UTF-8)records ... (len: 119)\n",
      "Finish: Get Page 312 (Encoding: cp1252)records ... (len: 136)\n",
      "Finish: Get Page 313 (Encoding: UTF-8)records ... (len: 383)\n",
      "Finish: Get Page 314 (Encoding: UTF-8)records ... (len: 317)\n",
      "Finish: Get Page 315 (Encoding: cp1252)records ... (len: 314)\n",
      "Finish: Get Page 316 (Encoding: cp1252)records ... (len: 357)\n",
      "Finish: Get Page 317 (Encoding: cp1252)records ... (len: 370)\n",
      "Finish: Get Page 318 (Encoding: UTF-8)records ... (len: 137)\n",
      "Finish: Get Page 319 (Encoding: UTF-8)records ... (len: 281)\n",
      "Finish: Get Page 320 (Encoding: UTF-8)records ... (len: 281)\n",
      "Finish: Get Page 321 (Encoding: UTF-8)records ... (len: 247)\n",
      "Finish: Get Page 322 (Encoding: UTF-8)records ... (len: 248)\n",
      "Finish: Get Page 323 (Encoding: UTF-8)records ... (len: 389)\n",
      "Finish: Get Page 324 (Encoding: UTF-8)records ... (len: 330)\n"
     ]
    }
   ],
   "source": [
    "X_raw, y, page_positions = storage.get_Xy(contain_button = True, file_type='T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_page_seq = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_data(x, y, p):\n",
    "    new_tmp_x_array = []\n",
    "    new_tmp_y_array = []\n",
    "    new_tmp_p_array = []\n",
    "    for tmp_x, tmp_y, tmp_p in zip(x, y, p):\n",
    "        new_tmp_x_array.extend(chunks(tmp_x, max_page_seq))\n",
    "        new_tmp_y_array.extend(chunks(tmp_y, max_page_seq))\n",
    "        new_tmp_p_array.extend(chunks(tmp_p, max_page_seq))\n",
    "    return new_tmp_x_array, new_tmp_y_array, new_tmp_p_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_x, chunks_y, chunk_positions = get_chunks_data(X_raw, y, page_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Bert model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import bert\n",
    "from bert import tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "from BertModel import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_short_model = BertModel(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_long_model = BertModel(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbert = bert_short_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 µs, sys: 0 ns, total: 16 µs\n",
      "Wall time: 24.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# XXX: these functions should be copy-pasted from autopager/model.py\n",
    "\n",
    "def _as_list(generator, limit=None):\n",
    "    \"\"\"\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2), 0)\n",
    "    []\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2), 2)\n",
    "    ['te', 'ex']\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2))\n",
    "    ['te', 'ex', 'xt']\n",
    "    \"\"\"\n",
    "    return list(generator if limit is None else islice(generator, 0, limit))\n",
    "\n",
    "def feat_to_tokens(feat, tokenizer):\n",
    "    if type(feat) == type([]):\n",
    "        feat = ' '.join(feat)\n",
    "    tokens = tokenizer.tokenize(feat)\n",
    "    return tokens\n",
    "\n",
    "def link_to_features(link):\n",
    "    text = normalize(get_link_text(link))\n",
    "    href = get_link_href(link)\n",
    "    if href is None:\n",
    "        href = \"\"\n",
    "    p = urlsplit(href)\n",
    "    query_parsed = parse_qsl(p.query) #parse query string from path\n",
    "    query_param_names = [k.lower() for k, v in query_parsed]\n",
    "    query_param_names_ngrams = _as_list(ngrams_wb(\n",
    "        \" \".join([normalize(name) for name in query_param_names]), 3, 5, True\n",
    "    ))\n",
    "\n",
    "    # Classes of link itself and all its children.\n",
    "    # It is common to have e.g. span elements with fontawesome\n",
    "    # arrow icon classes inside <a> links.\n",
    "    self_and_children_classes = ' '.join(_as_list(link.xpath(\".//@class\").extract(), 5))\n",
    "    parent_classes = ' '.join(_as_list(link.xpath('../@class').extract(), 5))\n",
    "    css_classes = normalize(parent_classes + ' ' + self_and_children_classes)\n",
    "#     print(css_classes)\n",
    "    token_feature = {\n",
    "        'text-before': '',\n",
    "        'text-exact': replace_digits(text.strip()[:40].strip()),\n",
    "        'text-after': '',\n",
    "        'class': css_classes,\n",
    "        'query': _as_list(query_param_names, 10)\n",
    "    }\n",
    "    tag_feature = {\n",
    "        'isdigit': 1 if text.isdigit() is True else 0,\n",
    "        'isalpha': 1 if text.isalpha() is True else 0,\n",
    "        'has-href': 0 if href is \"\" else 1,\n",
    "        'path-has-page': 1 if 'page' in p.path.lower() else 0,\n",
    "        'path-has-pageXX': 1 if re.search(r'[/-](?:p|page\\w?)/?\\d+', p.path.lower()) is not None else 0,\n",
    "        'path-has-number': 1 if any(part.isdigit() for part in p.path.split('/')) else 0,\n",
    "        'href-has-year': 1 if re.search('20\\d\\d', href) is not None else 0,\n",
    "        'class-has-disabled': 1 if 'disabled' in css_classes else 0,\n",
    "    }\n",
    "    tag_feature = [v for k,v in tag_feature.items()]\n",
    "#     attribute_feature = elem_rel + elem_target\n",
    "    non_token_feature = tag_feature #+ attribute_feature\n",
    "    return [token_feature, non_token_feature]\n",
    "\n",
    "\n",
    "def page_to_features(xseq):\n",
    "    feat_list = [link_to_features(a) for a in xseq]\n",
    "    around = get_text_around_selector_list(xseq, max_length=15)\n",
    "    \n",
    "    # weight is less than 1 because there is a lot of duplicate information\n",
    "    # in these ngrams and so we want to regularize them stronger\n",
    "    # (as if they are a single feature, not many features)\n",
    "    k = 0.2\n",
    "    for feat, (before, after) in zip(feat_list, around):\n",
    "        feat[0]['text-before'] = normalize(before)\n",
    "        feat[0]['text-after'] = normalize(after)\n",
    "        \n",
    "    return feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_tag_features_from_chunks(chunks):\n",
    "    token_features = []\n",
    "    tag_features = []\n",
    "    for page in chunks:\n",
    "        feat_list = page_to_features(page)\n",
    "        token_features.append([node[0] for node in feat_list])\n",
    "        tag_features.append([node[1] for node in feat_list])\n",
    "    return token_features, tag_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens_from_token_features(token_features):\n",
    "    train_tag_feature_token_list = []\n",
    "    for page in token_features:\n",
    "        tmp_page_list = []\n",
    "        for node in page: \n",
    "            tmp_list = []\n",
    "            for k, v in node.items():\n",
    "                if k == 'text-exact':\n",
    "                    continue\n",
    "                else:\n",
    "                    tmp_list.extend(v)\n",
    "            tmp_page_list.append(tmp_list)\n",
    "        train_tag_feature_token_list.append(tmp_page_list)\n",
    "    return train_tag_feature_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_features_bert_preprocessing(token_features, tokenizer, type = None, addNone = False, only_text = False):\n",
    "    train_token_features = []\n",
    "    if type == None or type not in ['single','multi','multi-two']:\n",
    "        print(\"Must Given a type of pre-processing\")\n",
    "        return\n",
    "    for page in token_features:\n",
    "        page_features = []\n",
    "        for node in page:\n",
    "            node_features = [\"[CLS]\"]\n",
    "            sep_two = False\n",
    "            for k,v in node.items():\n",
    "                value_tokens = feat_to_tokens(v, tokenizer)\n",
    "                if addNone == True and len(value_tokens) == 0:\n",
    "                    value_tokens = [\"None\"]\n",
    "                if type == 'single':\n",
    "                    node_features = node_features + value_tokens\n",
    "                    if k == 'text-after':\n",
    "                        node_features = node_features + [\"[SEP]\"]\n",
    "                elif type == 'multi':\n",
    "                    if k == 'text-after' or sep_two is True:\n",
    "                        sep_two = True\n",
    "                        node_features = node_features + value_tokens + [\"[SEP]\"]\n",
    "                    else:\n",
    "                        node_features = node_features + value_tokens\n",
    "                elif type == 'multi-two':\n",
    "                    node_features = node_features + value_tokens + [\"[SEP]\"]\n",
    "                    if k == 'text-after' and only_text is True:\n",
    "                        break\n",
    "            if 'multi' in type:\n",
    "                node_features = node_features[:-1]\n",
    "            page_features.append(node_features)\n",
    "        train_token_features.append(page_features)\n",
    "    return train_token_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_to_two_bert_embeddings(token_features, tokenizer):\n",
    "    text_first_segs = []\n",
    "    text_second_segs = []\n",
    "    for page in token_features:\n",
    "        page_one_features = []\n",
    "        page_two_features = []\n",
    "        for node in page:\n",
    "            text_before = tokenizer.tokenize(node[\"text-before\"])\n",
    "            text_exact = tokenizer.tokenize(node[\"text-exact\"])\n",
    "            text_after = tokenizer.tokenize(node[\"text-after\"])\n",
    "            page_one_features.append([\"[CLS]\"]+text_before+[\"[SEP]\"]+text_exact+[\"[SEP]\"])\n",
    "            page_two_features.append([\"[CLS]\"]+text_exact+[\"[SEP]\"]+text_after+[\"[SEP]\"])\n",
    "        text_first_segs.append(page_one_features)\n",
    "        text_second_segs.append(page_two_features)\n",
    "    print(\"Start encode first seg embeddings\")\n",
    "    first_emb = pbert.page_list_to_bert_embedding_list(text_first_segs, Token=True)\n",
    "    print(\"Start encode second seg embeddings\")\n",
    "    second_emb = pbert.page_list_to_bert_embedding_list(text_second_segs, Token=True)\n",
    "    full_text_emb = [np.concatenate([first_emb[page], second_emb[page]], axis = 1) for page in range(len(token_features))]\n",
    "    return full_text_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_features, tag_features = get_token_tag_features_from_chunks(chunks_x)\n",
    "# train_tag_feature_token_list = extract_tokens_from_token_features(token_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start encode first seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fcb4fa32df9431ea172f435bc5cdca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=355)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start encode second seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8c0d3bf18b4e519bc43aa2de354761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=355)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_text_emb = page_to_two_bert_embeddings(token_features, pbert.get_tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 1536)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text_emb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_token_features = token_features_bert_preprocessing(token_features,type = 'single')\n",
    "# train_token_features = token_features_bert_preprocessing(token_features,type = 'multi')\n",
    "train_token_features = token_features_bert_preprocessing(token_features, pbert.get_tokenizer(), type = 'multi-two', only_text = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  10.835698937066166\n",
      "Max_node:  63\n"
     ]
    }
   ],
   "source": [
    "max_node = -1\n",
    "page_sum = 0\n",
    "for page in train_token_features:\n",
    "    sum = 0\n",
    "    for node in page:\n",
    "        sum+=len(node)\n",
    "        if len(node) > max_node:\n",
    "            max_node = len(node)\n",
    "    page_sum+=sum/len(page)\n",
    "print(\"Average: \", page_sum/len(train_token_features))\n",
    "print(\"Max_node: \",max_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_info_list = tag_features #features which only have tag true/false information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text-exact feature in token_features\n",
    "train_text_list = [[ data['text-exact'] for data in x] for x in token_features]\n",
    "train_text_before_list = [[ data['text-before'] for data in x] for x in token_features]\n",
    "train_text_after_list = [[ data['text-after'] for data in x] for x in token_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_text(before, mid, after):\n",
    "    res = \"\"\n",
    "    if before != \"\":\n",
    "        res+=before + \",\"\n",
    "    if mid != \"\":\n",
    "        res+=mid + \",\"\n",
    "    if after == \"\":\n",
    "        res = res[:-1]\n",
    "    else:\n",
    "        res+=after\n",
    "    return res\n",
    "# Extract text-exact feature in token_features\n",
    "train_full_text_list = [[ concat_text(data['text-before'],data['text-exact'],data['text-after']) for data in x] for x in token_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pbert.max_seq_length"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "train_text_emb = pbert.page_list_to_bert_embedding_list(train_text_list, Token=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "train_text_before_emb = pbert.page_list_to_bert_embedding_list(train_text_before_list, Token=False)\n",
    "train_text_after_emb = pbert.page_list_to_bert_embedding_list(train_text_after_list, Token=False)\n",
    "train_concat_text_emb = [np.concatenate([train_text_emb[page], train_text_before_emb[page], train_text_after_emb[page]], axis = 1) for page in range(len(token_features))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use custom Token: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d98195a5e8149b7a5752a8d139fa7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=353)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_full_text_emb = pbert.page_list_to_bert_embedding_list(train_full_text_list, Token=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdda8c6d87f4fe393d38f6fa0778d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=353)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tag_emb = pbert.page_list_to_bert_embedding_list(train_token_features, Token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tag_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 768)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tag_emb[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_text_emb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-330d2f846a23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_text_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_text_emb' is not defined"
     ]
    }
   ],
   "source": [
    "train_text_emb[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split chunks into three type of training data\n",
    "    (1) Every chunks\n",
    "    (2) Chunks only have tag informations + Chunks only have Other\n",
    "\n",
    "### Feature List\n",
    "    * train_tag_feature_token_list => Tag Attributes tokens\n",
    "    * train_tag_info_list => Tag information\n",
    "    * train_text_emb => Only Text node => Bert Text embedding\n",
    "    * train_tag_emb => Text-before Text Text-after [SEP] Other Attributes => Bert Text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyHavaOther(y):\n",
    "    for tag in y:\n",
    "        if tag != 'O':\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_only_data_idx = [idx for idx, row_y in enumerate(chunks_y) if not onlyHavaOther(row_y)]\n",
    "chunks_only_other_idx = [idx for idx, row_y in enumerate(chunks_y) if onlyHavaOther(row_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags data:  228\n",
      "Other data:  125\n"
     ]
    }
   ],
   "source": [
    "print(\"Tags data: \", len(chunks_only_data_idx))\n",
    "print(\"Other data: \", len(chunks_only_other_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilterChunks(chunks, filterIdx):\n",
    "    # chunks: pages\n",
    "    # filterIdx: Idx of list which indicate the return data\n",
    "    return [page for idx, page in enumerate(chunks) if idx in filterIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingData(types = None):\n",
    "    if types == None:\n",
    "        if len(train_text_emb) != len(train_tag_info_list) or len(train_tag_info_list) != len(chunks_y) or len(train_tag_info_list) != len(train_tag_feature_token_list):\n",
    "            raise Exception('Every chunks should have equal size')\n",
    "        print(f\"return {len(chunks_y)} data.\")\n",
    "        return (train_text_emb, train_tag_feature_token_list, train_tag_info_list, chunks_y)\n",
    "    if types == 'Tags':\n",
    "        chunks_text_x = getFilterChunks(train_text_emb, chunks_only_data_idx)\n",
    "        chunks_token_x = getFilterChunks(train_tag_feature_token_list, chunks_only_data_idx)\n",
    "        chunks_tag_x = getFilterChunks(train_tag_info_list, chunks_only_data_idx)\n",
    "        chunks_filtered_y = getFilterChunks(chunks_y, chunks_only_data_idx)\n",
    "        if len(chunks_text_x) != len(chunks_tag_x) or len(chunks_tag_x) != len(chunks_filtered_y) or len(chunks_token_x) != len(chunks_tag_x):\n",
    "            raise Exception('Every chunks should have equal size')\n",
    "        print(f\"return {len(chunks_filtered_y)} data.\")\n",
    "        return (chunks_text_x, chunks_token_x, chunks_tag_x, chunks_filtered_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks_text_emb = train_concat_text_emb # text concat\n",
    "# chunks_text_emb = train_text_emb # text\n",
    "# chunks_text_emb = train_full_text_emb # full text\n",
    "# chunks_text_emb = train_tag_emb # tag\n",
    "chunks_text_emb = full_text_emb # full text embedding (two bert)\n",
    "\n",
    "chunks_tag_infos = train_tag_info_list\n",
    "chunks_filtered_y = chunks_y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "chunks_text_emb = getFilterChunks(train_tag_emb, chunks_only_data_idx)\n",
    "chunks_tag_infos = getFilterChunks(train_tag_info_list, chunks_only_data_idx)\n",
    "chunks_filtered_y = getFilterChunks(chunks_y, chunks_only_data_idx)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "chunks_text_emb, chunks_tag_tokens, chunks_tag_infos, chunks_filtered_y = getTrainingData()\n",
    "# chunks_text_emb, chunks_tag_tokens, chunks_tag_infos, chunks_filtered_y = getTrainingData(types = 'Tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding to fixed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_pad_to_npdata(embedding):\n",
    "    dataset = Dataset.from_generator(lambda: iter(embedding), tf.float32)\n",
    "    dataset = dataset.padded_batch(1, padded_shapes= (max_page_seq, len(embedding[0][0])), padding_values=-1.,drop_remainder=False)\n",
    "    after_pad = np.array([ data[0] for data in list(dataset.as_numpy_iterator())])\n",
    "    return after_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tag_token = feature_pad_to_npdata(chunks_tag_tokens)\n",
    "train_text_emb_x = feature_pad_to_npdata(chunks_text_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_x = feature_pad_to_npdata(chunks_tag_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_positions_x = feature_pad_to_npdata(chunk_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_x = np.concatenate([train_tag_x, train_positions_x], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info_x = np.concatenate([train_text_emb_x, train_tag_x], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"O\", \"PREV\", \"PAGE\", \"NEXT\", \"[PAD]\"]\n",
    "tag2idx = { label:idx for idx,label in enumerate(labels)}\n",
    "idx2tag = { idx:label for idx,label in enumerate(labels)}\n",
    "num_tags = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pad_sequences([[tag2idx.get(l) for l in lab] for lab in chunks_filtered_y], maxlen=max_page_seq, padding=\"post\", truncating=\"post\", value=tag2idx[\"[PAD]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Shape:\n",
      "train_text_emb_x: (355, 512, 1536)\n",
      "train_tag_x: (355, 512, 10)\n",
      "train_info_x: (355, 512, 1546)\n",
      "train_y: (355, 512)\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Shape:\")\n",
    "print(f\"train_text_emb_x: {train_text_emb_x.shape}\")\n",
    "print(f\"train_tag_x: {train_tag_x.shape}\")\n",
    "print(f\"train_info_x: {train_info_x.shape}\")\n",
    "print(f\"train_y: {train_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = train_text_emb_x\n",
    "train_x = train_info_x\n",
    "# train_x = train_tag_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(355, 512, 1546)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build BERT-BiLSTM-CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.layers.crf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME_STAMP: 512\n",
      "HIDDEN_UNITS: 200\n",
      "DROPOUT_RATE: 0.1\n",
      "NUM_CLASS: 5\n"
     ]
    }
   ],
   "source": [
    "TIME_STAMPS = max_page_seq\n",
    "HIDDEN_UNITS = 200\n",
    "DROPOUT_RATE = 0.1\n",
    "# NUM_CLASS = 5\n",
    "NUM_CLASS = num_tags\n",
    "print(f\"TIME_STAMP: {TIME_STAMPS}\")\n",
    "print(f\"HIDDEN_UNITS: {HIDDEN_UNITS}\")\n",
    "print(f\"DROPOUT_RATE: {DROPOUT_RATE}\")\n",
    "print(f\"NUM_CLASS: {NUM_CLASS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CRF(SHAPE, numtags):\n",
    "    print(f\"SHAPE: {SHAPE}\")\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=SHAPE))\n",
    "    crf=CRF(numtags,name='crf_layer')\n",
    "    model.add(crf)\n",
    "    model.compile('adam',loss={'crf_layer': crf.get_loss})\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BERT_BILSTM_CRF(SHAPE, numtags):\n",
    "    print(f\"SHAPE: {SHAPE}\")\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=SHAPE))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=HIDDEN_UNITS, return_sequences=True)))\n",
    "    crf=CRF(numtags,name='crf_layer')\n",
    "    model.add(crf)\n",
    "    model.compile('adam',loss={'crf_layer': crf.get_loss})\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_BERT_BILSTM_SOFTMAX(SHAPE, numtags):\n",
    "    print(f\"SHAPE: {SHAPE}\")\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=SHAPE))\n",
    "#     model.add(tf.keras.layers.Masking(input_shape=SHAPE, mask_value=-1.))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=HIDDEN_UNITS, return_sequences=True)))\n",
    "    model.add(tf.keras.layers.Dense(units = numtags, activation='softmax'))\n",
    "    model.compile('adam',loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE: (512, 1546)\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_6 (Bidirection (None, 512, 400)          2795200   \n",
      "_________________________________________________________________\n",
      "crf_layer (CRF)              (None, 512)               2040      \n",
      "=================================================================\n",
      "Total params: 2,797,240\n",
      "Trainable params: 2,797,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "SHAPE: (512, 1546)\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_7 (Bidirection (None, 512, 400)          2795200   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512, 5)            2005      \n",
      "=================================================================\n",
      "Total params: 2,797,205\n",
      "Trainable params: 2,797,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# N_CRF = get_CRF(train_x.shape[1:], num_tags)\n",
    "\n",
    "CRF_model = get_BERT_BILSTM_CRF(train_x.shape[1:], num_tags)\n",
    "\n",
    "Softmax_model = get_BERT_BILSTM_SOFTMAX(train_x.shape[1:], num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(355, 512, 1546)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "NCRF_history = N_CRF.fit(train_x, train_y, batch_size=128, epochs=1000, validation_split=0.2, verbose=1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "3/3 [==============================] - 2s 658ms/step - loss: 280.1509 - val_loss: 43.4971\n",
      "Epoch 2/1000\n",
      "3/3 [==============================] - 1s 474ms/step - loss: 38.4417 - val_loss: 36.7814\n",
      "Epoch 3/1000\n",
      "3/3 [==============================] - 1s 474ms/step - loss: 31.7120 - val_loss: 29.1110\n",
      "Epoch 4/1000\n",
      "3/3 [==============================] - 1s 469ms/step - loss: 26.5059 - val_loss: 29.9663\n",
      "Epoch 5/1000\n",
      "3/3 [==============================] - 1s 465ms/step - loss: 27.3002 - val_loss: 27.7817\n",
      "Epoch 6/1000\n",
      "3/3 [==============================] - 1s 463ms/step - loss: 25.1764 - val_loss: 27.5550\n",
      "Epoch 7/1000\n",
      "3/3 [==============================] - 1s 458ms/step - loss: 24.8505 - val_loss: 27.1648\n",
      "Epoch 8/1000\n",
      "3/3 [==============================] - 1s 476ms/step - loss: 24.3794 - val_loss: 26.3130\n",
      "Epoch 9/1000\n",
      "3/3 [==============================] - 1s 458ms/step - loss: 23.5649 - val_loss: 25.5790\n",
      "Epoch 10/1000\n",
      "3/3 [==============================] - 1s 458ms/step - loss: 22.9391 - val_loss: 25.2387\n",
      "Epoch 11/1000\n",
      "3/3 [==============================] - 1s 474ms/step - loss: 22.5368 - val_loss: 24.6239\n",
      "Epoch 12/1000\n",
      "3/3 [==============================] - 1s 468ms/step - loss: 21.9234 - val_loss: 24.0904\n",
      "Epoch 13/1000\n",
      "3/3 [==============================] - 1s 463ms/step - loss: 21.4077 - val_loss: 23.5230\n",
      "Epoch 14/1000\n",
      "3/3 [==============================] - 1s 478ms/step - loss: 20.9188 - val_loss: 23.0989\n",
      "Epoch 15/1000\n",
      "3/3 [==============================] - 1s 470ms/step - loss: 20.2568 - val_loss: 22.5651\n",
      "Epoch 16/1000\n",
      "3/3 [==============================] - 1s 471ms/step - loss: 19.6143 - val_loss: 22.2572\n",
      "Epoch 17/1000\n",
      "3/3 [==============================] - 1s 464ms/step - loss: 19.1334 - val_loss: 21.5425\n",
      "Epoch 18/1000\n",
      "3/3 [==============================] - 1s 476ms/step - loss: 18.4098 - val_loss: 21.1601\n",
      "Epoch 19/1000\n",
      "3/3 [==============================] - 1s 463ms/step - loss: 17.8317 - val_loss: 20.7898\n",
      "Epoch 20/1000\n",
      "3/3 [==============================] - 1s 483ms/step - loss: 17.3499 - val_loss: 20.1890\n",
      "Epoch 21/1000\n",
      "3/3 [==============================] - 1s 474ms/step - loss: 16.6836 - val_loss: 19.9797\n",
      "Epoch 22/1000\n",
      "3/3 [==============================] - 1s 486ms/step - loss: 16.4412 - val_loss: 19.4167\n",
      "Epoch 23/1000\n",
      "3/3 [==============================] - 1s 471ms/step - loss: 16.1550 - val_loss: 20.0718\n",
      "Epoch 24/1000\n",
      "3/3 [==============================] - 1s 461ms/step - loss: 16.5513 - val_loss: 18.3666\n",
      "Epoch 25/1000\n",
      "3/3 [==============================] - 1s 481ms/step - loss: 15.2481 - val_loss: 18.0988\n",
      "Epoch 26/1000\n",
      "3/3 [==============================] - 1s 472ms/step - loss: 14.6558 - val_loss: 19.3793\n",
      "Epoch 27/1000\n",
      "3/3 [==============================] - 1s 461ms/step - loss: 14.4327 - val_loss: 17.6986\n",
      "Epoch 28/1000\n",
      "3/3 [==============================] - 1s 470ms/step - loss: 14.0305 - val_loss: 17.6487\n",
      "Epoch 29/1000\n",
      "3/3 [==============================] - 1s 467ms/step - loss: 13.4718 - val_loss: 17.6336\n",
      "Epoch 30/1000\n",
      "3/3 [==============================] - 1s 474ms/step - loss: 13.0661 - val_loss: 16.8792\n",
      "Epoch 31/1000\n",
      "3/3 [==============================] - 1s 476ms/step - loss: 12.8466 - val_loss: 18.2972\n",
      "Epoch 32/1000\n",
      "3/3 [==============================] - 1s 464ms/step - loss: 12.9741 - val_loss: 17.0805\n",
      "Epoch 33/1000\n",
      "3/3 [==============================] - 1s 464ms/step - loss: 13.0867 - val_loss: 17.7072\n",
      "Epoch 34/1000\n",
      "3/3 [==============================] - 1s 471ms/step - loss: 12.7892 - val_loss: 17.2928\n",
      "Epoch 35/1000\n",
      "3/3 [==============================] - 1s 453ms/step - loss: 12.0671 - val_loss: 16.1375\n",
      "Epoch 36/1000\n",
      "3/3 [==============================] - 1s 455ms/step - loss: 12.0609 - val_loss: 17.4961\n",
      "Epoch 37/1000\n",
      "3/3 [==============================] - 1s 475ms/step - loss: 11.9731 - val_loss: 16.2235\n",
      "Epoch 38/1000\n",
      "3/3 [==============================] - 1s 458ms/step - loss: 11.6431 - val_loss: 16.6667\n",
      "Epoch 39/1000\n",
      "3/3 [==============================] - 1s 466ms/step - loss: 11.3778 - val_loss: 18.0298\n",
      "Epoch 40/1000\n",
      "3/3 [==============================] - 1s 471ms/step - loss: 11.5457 - val_loss: 16.8214\n",
      "Epoch 41/1000\n",
      "3/3 [==============================] - 1s 471ms/step - loss: 11.1694 - val_loss: 17.4949\n",
      "Epoch 42/1000\n",
      "3/3 [==============================] - 1s 461ms/step - loss: 11.0831 - val_loss: 15.7153\n",
      "Epoch 43/1000\n",
      "3/3 [==============================] - 1s 463ms/step - loss: 11.0613 - val_loss: 17.2674\n",
      "Epoch 44/1000\n",
      "3/3 [==============================] - 1s 457ms/step - loss: 10.9065 - val_loss: 16.2846\n",
      "Epoch 45/1000\n",
      "3/3 [==============================] - 1s 481ms/step - loss: 10.3803 - val_loss: 15.9690\n",
      "Epoch 46/1000\n",
      "3/3 [==============================] - 1s 460ms/step - loss: 10.3712 - val_loss: 16.8797\n",
      "Epoch 47/1000\n",
      "3/3 [==============================] - 1s 459ms/step - loss: 10.1205 - val_loss: 15.5287\n",
      "Epoch 48/1000\n",
      "3/3 [==============================] - 1s 464ms/step - loss: 10.1319 - val_loss: 16.3751\n",
      "Epoch 49/1000\n",
      "3/3 [==============================] - 1s 466ms/step - loss: 9.8085 - val_loss: 16.5737\n",
      "Epoch 50/1000\n",
      "3/3 [==============================] - 1s 478ms/step - loss: 9.5517 - val_loss: 15.3711\n",
      "Epoch 51/1000\n",
      "3/3 [==============================] - 1s 477ms/step - loss: 9.5393 - val_loss: 16.5182\n",
      "Epoch 52/1000\n",
      "3/3 [==============================] - 1s 469ms/step - loss: 9.3353 - val_loss: 15.2868\n",
      "Epoch 53/1000\n",
      "3/3 [==============================] - 1s 459ms/step - loss: 9.3454 - val_loss: 15.5581\n",
      "Epoch 54/1000\n",
      "3/3 [==============================] - 1s 483ms/step - loss: 9.1495 - val_loss: 15.6463\n",
      "Epoch 55/1000\n",
      "3/3 [==============================] - 1s 453ms/step - loss: 9.0251 - val_loss: 16.1387\n",
      "Epoch 56/1000\n",
      "3/3 [==============================] - 1s 450ms/step - loss: 8.9465 - val_loss: 15.6169\n",
      "Epoch 57/1000\n",
      "3/3 [==============================] - 1s 461ms/step - loss: 9.2509 - val_loss: 15.7556\n",
      "Epoch 58/1000\n",
      "3/3 [==============================] - 1s 463ms/step - loss: 8.7515 - val_loss: 16.8339\n",
      "Epoch 59/1000\n",
      "3/3 [==============================] - 1s 459ms/step - loss: 8.6127 - val_loss: 14.5285\n",
      "Epoch 60/1000\n",
      "3/3 [==============================] - 1s 474ms/step - loss: 8.9117 - val_loss: 17.0465\n",
      "Epoch 61/1000\n",
      "3/3 [==============================] - 1s 485ms/step - loss: 8.3974 - val_loss: 14.0164\n",
      "Epoch 62/1000\n",
      "3/3 [==============================] - 1s 459ms/step - loss: 8.4342 - val_loss: 18.5649\n",
      "Epoch 63/1000\n",
      "3/3 [==============================] - 1s 477ms/step - loss: 8.6504 - val_loss: 14.7729\n",
      "Epoch 64/1000\n",
      "3/3 [==============================] - 1s 471ms/step - loss: 8.1992 - val_loss: 15.7028\n",
      "Epoch 65/1000\n",
      "3/3 [==============================] - 1s 470ms/step - loss: 8.1865 - val_loss: 18.2289\n",
      "Epoch 66/1000\n",
      "3/3 [==============================] - 1s 471ms/step - loss: 8.0267 - val_loss: 15.0694\n",
      "Epoch 67/1000\n",
      "3/3 [==============================] - 1s 460ms/step - loss: 8.1237 - val_loss: 17.3887\n",
      "Epoch 68/1000\n",
      "3/3 [==============================] - 1s 463ms/step - loss: 7.5964 - val_loss: 15.2120\n",
      "Epoch 69/1000\n",
      "3/3 [==============================] - 1s 456ms/step - loss: 7.6218 - val_loss: 16.2109\n",
      "Epoch 70/1000\n",
      "3/3 [==============================] - 1s 459ms/step - loss: 7.3646 - val_loss: 14.4961\n",
      "Epoch 71/1000\n",
      "3/3 [==============================] - 1s 463ms/step - loss: 7.4741 - val_loss: 16.5814\n"
     ]
    }
   ],
   "source": [
    "CRF_history = CRF_model.fit(train_x, train_y, batch_size=128, epochs=1000, validation_split=0.2, verbose=1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "3/3 [==============================] - 1s 319ms/step - loss: 0.8240 - val_loss: 0.0993\n",
      "Epoch 2/1000\n",
      "3/3 [==============================] - 1s 184ms/step - loss: 0.0908 - val_loss: 0.0947\n",
      "Epoch 3/1000\n",
      "3/3 [==============================] - 1s 189ms/step - loss: 0.0840 - val_loss: 0.0795\n",
      "Epoch 4/1000\n",
      "3/3 [==============================] - 1s 187ms/step - loss: 0.0702 - val_loss: 0.0718\n",
      "Epoch 5/1000\n",
      "3/3 [==============================] - 1s 178ms/step - loss: 0.0668 - val_loss: 0.0769\n",
      "Epoch 6/1000\n",
      "3/3 [==============================] - 1s 190ms/step - loss: 0.0707 - val_loss: 0.0726\n",
      "Epoch 7/1000\n",
      "3/3 [==============================] - 1s 186ms/step - loss: 0.0655 - val_loss: 0.0698\n",
      "Epoch 8/1000\n",
      "3/3 [==============================] - 1s 187ms/step - loss: 0.0633 - val_loss: 0.0703\n",
      "Epoch 9/1000\n",
      "3/3 [==============================] - 1s 188ms/step - loss: 0.0634 - val_loss: 0.0700\n",
      "Epoch 10/1000\n",
      "3/3 [==============================] - 1s 185ms/step - loss: 0.0626 - val_loss: 0.0684\n",
      "Epoch 11/1000\n",
      "3/3 [==============================] - 1s 182ms/step - loss: 0.0611 - val_loss: 0.0674\n",
      "Epoch 12/1000\n",
      "3/3 [==============================] - 1s 188ms/step - loss: 0.0598 - val_loss: 0.0671\n",
      "Epoch 13/1000\n",
      "3/3 [==============================] - 1s 182ms/step - loss: 0.0594 - val_loss: 0.0665\n",
      "Epoch 14/1000\n",
      "3/3 [==============================] - 1s 186ms/step - loss: 0.0584 - val_loss: 0.0655\n",
      "Epoch 15/1000\n",
      "3/3 [==============================] - 1s 197ms/step - loss: 0.0571 - val_loss: 0.0644\n",
      "Epoch 16/1000\n",
      "3/3 [==============================] - 1s 196ms/step - loss: 0.0561 - val_loss: 0.0638\n",
      "Epoch 17/1000\n",
      "3/3 [==============================] - 1s 181ms/step - loss: 0.0554 - val_loss: 0.0629\n",
      "Epoch 18/1000\n",
      "3/3 [==============================] - 1s 189ms/step - loss: 0.0541 - val_loss: 0.0617\n",
      "Epoch 19/1000\n",
      "3/3 [==============================] - 1s 181ms/step - loss: 0.0534 - val_loss: 0.0613\n",
      "Epoch 20/1000\n",
      "3/3 [==============================] - 1s 190ms/step - loss: 0.0522 - val_loss: 0.0609\n",
      "Epoch 21/1000\n",
      "3/3 [==============================] - 1s 189ms/step - loss: 0.0514 - val_loss: 0.0599\n",
      "Epoch 22/1000\n",
      "3/3 [==============================] - 1s 186ms/step - loss: 0.0502 - val_loss: 0.0588\n",
      "Epoch 23/1000\n",
      "3/3 [==============================] - 1s 189ms/step - loss: 0.0491 - val_loss: 0.0576\n",
      "Epoch 24/1000\n",
      "3/3 [==============================] - 1s 200ms/step - loss: 0.0480 - val_loss: 0.0565\n",
      "Epoch 25/1000\n",
      "3/3 [==============================] - 1s 193ms/step - loss: 0.0467 - val_loss: 0.0553\n",
      "Epoch 26/1000\n",
      "3/3 [==============================] - 1s 185ms/step - loss: 0.0458 - val_loss: 0.0538\n",
      "Epoch 27/1000\n",
      "3/3 [==============================] - 1s 188ms/step - loss: 0.0442 - val_loss: 0.0530\n",
      "Epoch 28/1000\n",
      "3/3 [==============================] - 1s 190ms/step - loss: 0.0433 - val_loss: 0.0514\n",
      "Epoch 29/1000\n",
      "3/3 [==============================] - 1s 186ms/step - loss: 0.0416 - val_loss: 0.0501\n",
      "Epoch 30/1000\n",
      "3/3 [==============================] - 1s 192ms/step - loss: 0.0402 - val_loss: 0.0488\n",
      "Epoch 31/1000\n",
      "3/3 [==============================] - 1s 183ms/step - loss: 0.0386 - val_loss: 0.0478\n",
      "Epoch 32/1000\n",
      "3/3 [==============================] - 1s 189ms/step - loss: 0.0372 - val_loss: 0.0467\n",
      "Epoch 33/1000\n",
      "3/3 [==============================] - 1s 190ms/step - loss: 0.0358 - val_loss: 0.0473\n",
      "Epoch 34/1000\n",
      "3/3 [==============================] - 1s 191ms/step - loss: 0.0347 - val_loss: 0.0438\n",
      "Epoch 35/1000\n",
      "3/3 [==============================] - 1s 180ms/step - loss: 0.0335 - val_loss: 0.0447\n",
      "Epoch 36/1000\n",
      "3/3 [==============================] - 1s 195ms/step - loss: 0.0323 - val_loss: 0.0423\n",
      "Epoch 37/1000\n",
      "3/3 [==============================] - 1s 189ms/step - loss: 0.0318 - val_loss: 0.0489\n",
      "Epoch 38/1000\n",
      "3/3 [==============================] - 1s 185ms/step - loss: 0.0316 - val_loss: 0.0412\n",
      "Epoch 39/1000\n",
      "3/3 [==============================] - 1s 182ms/step - loss: 0.0305 - val_loss: 0.0472\n",
      "Epoch 40/1000\n",
      "3/3 [==============================] - 1s 189ms/step - loss: 0.0304 - val_loss: 0.0395\n",
      "Epoch 41/1000\n",
      "3/3 [==============================] - 1s 187ms/step - loss: 0.0291 - val_loss: 0.0449\n",
      "Epoch 42/1000\n",
      "3/3 [==============================] - 1s 194ms/step - loss: 0.0288 - val_loss: 0.0388\n",
      "Epoch 43/1000\n",
      "3/3 [==============================] - 1s 187ms/step - loss: 0.0275 - val_loss: 0.0410\n",
      "Epoch 44/1000\n",
      "3/3 [==============================] - 1s 187ms/step - loss: 0.0270 - val_loss: 0.0382\n",
      "Epoch 45/1000\n",
      "3/3 [==============================] - 1s 187ms/step - loss: 0.0259 - val_loss: 0.0389\n",
      "Epoch 46/1000\n",
      "3/3 [==============================] - 1s 185ms/step - loss: 0.0253 - val_loss: 0.0379\n",
      "Epoch 47/1000\n",
      "3/3 [==============================] - 1s 180ms/step - loss: 0.0253 - val_loss: 0.0399\n",
      "Epoch 48/1000\n",
      "3/3 [==============================] - 1s 188ms/step - loss: 0.0254 - val_loss: 0.0365\n",
      "Epoch 49/1000\n",
      "3/3 [==============================] - 1s 183ms/step - loss: 0.0248 - val_loss: 0.0392\n",
      "Epoch 50/1000\n",
      "3/3 [==============================] - 1s 193ms/step - loss: 0.0242 - val_loss: 0.0366\n",
      "Epoch 51/1000\n",
      "3/3 [==============================] - 1s 192ms/step - loss: 0.0243 - val_loss: 0.0375\n",
      "Epoch 52/1000\n",
      "3/3 [==============================] - 1s 189ms/step - loss: 0.0233 - val_loss: 0.0363\n",
      "Epoch 53/1000\n",
      "3/3 [==============================] - 1s 180ms/step - loss: 0.0235 - val_loss: 0.0351\n",
      "Epoch 54/1000\n",
      "3/3 [==============================] - 1s 189ms/step - loss: 0.0228 - val_loss: 0.0415\n",
      "Epoch 55/1000\n",
      "3/3 [==============================] - 1s 184ms/step - loss: 0.0228 - val_loss: 0.0354\n",
      "Epoch 56/1000\n",
      "3/3 [==============================] - 1s 183ms/step - loss: 0.0218 - val_loss: 0.0403\n",
      "Epoch 57/1000\n",
      "3/3 [==============================] - 1s 190ms/step - loss: 0.0218 - val_loss: 0.0358\n",
      "Epoch 58/1000\n",
      "3/3 [==============================] - 1s 185ms/step - loss: 0.0216 - val_loss: 0.0392\n",
      "Epoch 59/1000\n",
      "3/3 [==============================] - 1s 188ms/step - loss: 0.0207 - val_loss: 0.0388\n",
      "Epoch 60/1000\n",
      "3/3 [==============================] - 1s 186ms/step - loss: 0.0205 - val_loss: 0.0387\n",
      "Epoch 61/1000\n",
      "3/3 [==============================] - 1s 179ms/step - loss: 0.0202 - val_loss: 0.0366\n",
      "Epoch 62/1000\n",
      "3/3 [==============================] - 1s 183ms/step - loss: 0.0199 - val_loss: 0.0348\n",
      "Epoch 63/1000\n",
      "3/3 [==============================] - 1s 185ms/step - loss: 0.0193 - val_loss: 0.0368\n",
      "Epoch 64/1000\n",
      "3/3 [==============================] - 1s 186ms/step - loss: 0.0187 - val_loss: 0.0376\n",
      "Epoch 65/1000\n",
      "3/3 [==============================] - 1s 184ms/step - loss: 0.0188 - val_loss: 0.0371\n",
      "Epoch 66/1000\n",
      "3/3 [==============================] - 1s 192ms/step - loss: 0.0183 - val_loss: 0.0393\n",
      "Epoch 67/1000\n",
      "3/3 [==============================] - 1s 186ms/step - loss: 0.0181 - val_loss: 0.0329\n",
      "Epoch 68/1000\n",
      "3/3 [==============================] - 1s 184ms/step - loss: 0.0183 - val_loss: 0.0472\n",
      "Epoch 69/1000\n",
      "3/3 [==============================] - 1s 184ms/step - loss: 0.0197 - val_loss: 0.0343\n",
      "Epoch 70/1000\n",
      "3/3 [==============================] - 1s 188ms/step - loss: 0.0173 - val_loss: 0.0421\n",
      "Epoch 71/1000\n",
      "3/3 [==============================] - 1s 192ms/step - loss: 0.0172 - val_loss: 0.0341\n",
      "Epoch 72/1000\n",
      "3/3 [==============================] - 1s 187ms/step - loss: 0.0172 - val_loss: 0.0404\n",
      "Epoch 73/1000\n",
      "3/3 [==============================] - 1s 183ms/step - loss: 0.0168 - val_loss: 0.0328\n",
      "Epoch 74/1000\n",
      "3/3 [==============================] - 1s 182ms/step - loss: 0.0164 - val_loss: 0.0397\n",
      "Epoch 75/1000\n",
      "3/3 [==============================] - 1s 186ms/step - loss: 0.0161 - val_loss: 0.0368\n",
      "Epoch 76/1000\n",
      "3/3 [==============================] - 1s 181ms/step - loss: 0.0159 - val_loss: 0.0368\n",
      "Epoch 77/1000\n",
      "3/3 [==============================] - 1s 182ms/step - loss: 0.0154 - val_loss: 0.0363\n",
      "Epoch 78/1000\n",
      "3/3 [==============================] - 1s 193ms/step - loss: 0.0153 - val_loss: 0.0370\n",
      "Epoch 79/1000\n",
      "3/3 [==============================] - 1s 192ms/step - loss: 0.0146 - val_loss: 0.0406\n",
      "Epoch 80/1000\n",
      "3/3 [==============================] - 1s 187ms/step - loss: 0.0145 - val_loss: 0.0408\n",
      "Epoch 81/1000\n",
      "3/3 [==============================] - 1s 184ms/step - loss: 0.0143 - val_loss: 0.0446\n",
      "Epoch 82/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 185ms/step - loss: 0.0142 - val_loss: 0.0380\n",
      "Epoch 83/1000\n",
      "3/3 [==============================] - 1s 183ms/step - loss: 0.0138 - val_loss: 0.0390\n"
     ]
    }
   ],
   "source": [
    "Softmax_history = Softmax_model.fit(train_x, train_y, batch_size=128, epochs=1000, validation_split=0.2, verbose=1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[CRF]\n",
    "[All]\n",
    "[train_text_emb] \n",
    "[0]: epo165 loss: 16.5179 - val_loss: 20.5074\n",
    "[1]: epo183 loss: 13.5592 - val_loss: 16.4887\n",
    "[train_info_concat]\n",
    "[0]: epo153 loss: 10.0395 - val_loss: 14.2559\n",
    "[1]: epo128 loss: 10.1085 - val_loss: 14.6777\n",
    "[train_tag_emb]\n",
    "[0]: epo126 loss: 6.9019 - val_loss: 15.7222\n",
    "[train_tag_info_concat]\n",
    "[0]: epo114 loss: 5.3638 - val_loss: 12.2225\n",
    "[1]: epo116 loss: 4.9694 - val_loss: 11.9932\n",
    "[train_tag_two_info_concat]\n",
    "[0]: epo134 loss: 9.6451 - val_loss: 12.6221\n",
    "[1]: epo185 loss: 6.7996 - val_loss: 12.2248\n",
    "\n",
    "[TagOnly]\n",
    "[train_text_emb] \n",
    "[0]: epo185 loss: 34.0454 - val_loss: 38.6243\n",
    "[1]: epo149 loss: 26.8772 - val_loss: 30.1547\n",
    "[2]: epo167 loss: 25.4310 - val_loss: 31.7712\n",
    "[train_tag_info_concat]\n",
    "[0]: epo147 loss: 8.5670 - val_loss: 22.5889\n",
    "[1]: epo139 loss: 9.2346 - val_loss: 22.0598\n",
    "[train_tag_two_info_concat]\n",
    "[0]: epo172 loss: 11.0723 - val_loss: 16.6598\n",
    "[1]: epo116 loss: 16.7534 - val_loss: 19.8089\n",
    "loss: 13.1938 - val_loss: 19.0995"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[Softmax]\n",
    "[All]\n",
    "[train_text_emb] \n",
    "\n",
    "[train_info_concat]\n",
    "[0]: epo100 loss: 0.0267 - val_loss: 0.0290\n",
    "[train_tag_emb]\n",
    "[train_tag_info_concat]\n",
    "[0]: epo117 loss: 0.0107 - val_loss: 0.0273\n",
    "[1]: epo119 loss: 0.0108 - val_loss: 0.0280\n",
    "[train_tag_two_info_concat]\n",
    "[0]: epo146 loss: 0.0186 - val_loss: 0.0246\n",
    "[1]: epo143 loss: 0.0183 - val_loss: 0.0255\n",
    "\n",
    "[TagOnly]\n",
    "[train_text_emb] \n",
    "[train_info_concat]\n",
    "[train_tag_emb]\n",
    "[train_tag_info_concat]\n",
    "loss: 0.0180 - val_loss: 0.0467\n",
    "loss: 0.0154 - val_loss: 0.0474\n",
    "[train_tag_two_info_concat]\n",
    "loss: 0.0253 - val_loss: 0.0442\n",
    "loss: 0.0205 - val_loss: 0.0411\n",
    "loss: 0.0203 - val_loss: 0.0375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupKfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.36 ms, sys: 0 ns, total: 2.36 ms\n",
      "Wall time: 2.37 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "groups = [get_domain(url) for url in urls]\n",
    "N_SPLITs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_group_dataset(X_data,y_data, groups, n_splits):\n",
    "\n",
    "    def gen():\n",
    "        for train_index, test_index in GroupKFold(n_splits).split(X_data, y_data, groups):\n",
    "            X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "            y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "            yield X_train,y_train,X_test,y_test\n",
    "\n",
    "    return tf.data.Dataset.from_generator(gen, (tf.float64,tf.float64,tf.float64,tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = make_group_dataset(train_x, train_y, groups, N_SPLITs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_cross_val(dataset):\n",
    "    count = 1\n",
    "    for X_train,y_train,X_test,y_test in dataset:\n",
    "        print(f\"Start fold {count}\")\n",
    "        earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "        model = get_BILSTM_SOFTMAX(X_train.shape[1:], num_tags)\n",
    "        history = model.fit(X_train, y_train, batch_size=32, epochs=500, validation_split=0.2, verbose=0, callbacks=[earlyStopping])\n",
    "        predict_y = model.predict(X_test)\n",
    "        predict_y = label_distribution_to_label(predict_y)\n",
    "        predict_y = [[idx2tag.get(lab) for lab in page] for page in predict_y]\n",
    "        y_test = [[idx2tag.get(lab) for lab in page] for page in y_test.numpy()]\n",
    "        evaluate_labels = ['PREV', 'PAGE', 'NEXT']\n",
    "        print(flat_classification_report(y_test, predict_y, labels=evaluate_labels, digits=len(evaluate_labels)))\n",
    "        count+=1\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(type=None):\n",
    "    if type is None:\n",
    "        print(\"Please assign type of test_data\")\n",
    "        return\n",
    "    if type != 'EVENT_SOURCE':\n",
    "        storage.test_file = 'NORMAL'\n",
    "        test_urls = [rec['Page URL'] for rec in storage.iter_test_records()]\n",
    "        test_X_one, test_y_one, test_page_positions_one = storage.get_test_Xy(validate=False)\n",
    "        print(\"pages: {}  domains: {}\".format(len(test_urls), len({get_domain(url) for url in test_urls})))\n",
    "        if type == 'NORMAL':\n",
    "            return test_X_one, test_y_one, test_page_positions_one\n",
    "    if type != 'NORMAL':\n",
    "        storage.test_file = 'EVENT_SOURCE'\n",
    "        test_urls = [rec['Page URL'] for rec in storage.iter_test_records()]\n",
    "        test_X_two, test_y_two, test_page_positions_two = storage.get_test_Xy(validate=False)\n",
    "        print(\"pages: {}  domains: {}\".format(len(test_urls), len({get_domain(url) for url in test_urls})))\n",
    "        if type == 'EVENT_SOURCE':\n",
    "            return test_X_two, test_y_two, test_page_positions_two\n",
    "    test_X_raw = test_X_one + test_X_two\n",
    "    test_y = test_y_one + test_y_two\n",
    "    test_positions = test_page_positions_one + test_page_positions_two\n",
    "    return test_X_raw, test_y, test_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distribution_to_label(predict_y):\n",
    "    if len(predict_y.shape) != 3:\n",
    "        return predict_y\n",
    "    label_y = list()\n",
    "    for page in predict_y:\n",
    "        tmp = list()\n",
    "        for lab in page:\n",
    "            lab = lab.tolist()\n",
    "            tmp.append(lab.index(max(lab)))\n",
    "        label_y.append(tmp)\n",
    "    return label_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages: 100  domains: 54\n"
     ]
    }
   ],
   "source": [
    "# test_X_raw, test_y, test_page_positions = get_test_data('EVENT_SOURCE')\n",
    "test_X_raw, test_y, test_page_positions = get_test_data('NORMAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_urls = [rec['Page URL'] for rec in storage.iter_test_records()]\n",
    "test_groups = set([get_domain(url) for url in test_urls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups exist: musicarts\n",
      "Groups exist: mobile01\n"
     ]
    }
   ],
   "source": [
    "for group in test_groups:\n",
    "    if group in train_groups_set:\n",
    "        print(f\"Groups exist: {group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_test_x, chunks_test_y, chunks_test_positions = get_chunks_data(test_X_raw, test_y, test_page_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_features, test_tag_features = get_token_tag_features_from_chunks(chunks_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start encode first seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85899af2d654870a9fb78b31818e046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=127)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start encode second seg embeddings\n",
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24efda93f9f2407eac974497213224c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=127)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_full_text_emb = page_to_two_bert_embeddings(test_token_features, pbert.get_tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tag_emb_features = token_features_bert_preprocessing(test_token_features, 'single')\n",
    "# test_tag_emb_features = token_features_bert_preprocessing(test_token_features, 'multi')\n",
    "# test_tag_emb_features = token_features_bert_preprocessing(test_token_features, 'multi-two')\n",
    "test_tag_emb_features = token_features_bert_preprocessing(test_token_features, pbert.get_tokenizer(), type = 'multi-two', only_text = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tag_info_list = test_tag_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  1.0638181757827387\n",
      "Max_node:  59\n"
     ]
    }
   ],
   "source": [
    "max_node = -1\n",
    "page_sum = 0\n",
    "for page in test_tag_emb_features:\n",
    "    sum = 0\n",
    "    for node in page:\n",
    "        sum+=len(node)\n",
    "        if len(node) > max_node:\n",
    "            max_node = len(node)\n",
    "    page_sum+=sum/len(page)\n",
    "print(\"Average: \", page_sum/len(train_token_features))\n",
    "print(\"Max_node: \",max_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use custom Token: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55f04e6a77f4eaead6111045c52ab84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=42)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_text_list = [[data[\"text-exact\"] for data in page ] for page in test_token_features]\n",
    "test_text_emb = pbert.page_list_to_bert_embedding_list(test_text_list, Token=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Concat three bert embeddings\n",
    "test_text_before_list = [[data[\"text-before\"] for data in page ] for page in test_token_features]\n",
    "test_text_before_emb = pbert.page_list_to_bert_embedding_list(test_text_before_list, Token=False)\n",
    "test_text_after_list = [[data[\"text-after\"] for data in page ] for page in test_token_features]\n",
    "test_text_after_emb = pbert.page_list_to_bert_embedding_list(test_text_after_list, Token=False)\n",
    "test_concat_text_emb = [np.concatenate([test_text_emb[page], test_text_before_emb[page], test_text_after_emb[page]], axis = 1) for page in range(len(test_token_features))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ec064191a34522b5488383ac7e47ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=42)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_tag_emb = pbert.page_list_to_bert_embedding_list(test_tag_emb_features, Token=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Extract text-exact feature in token_features\n",
    "test_full_text_list = [[ concat_text(data['text-before'],data['text-exact'],data['text-after']) for data in x] for x in test_token_features]\n",
    "test_full_text_emb = pbert.page_list_to_bert_embedding_list(test_full_text_list, Token=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_text_emb_x = feature_pad_to_npdata(test_concat_text_emb) # full text concat emb\n",
    "test_text_emb_x = feature_pad_to_npdata(test_full_text_emb) # full text emb / two-bert emb\n",
    "# test_text_emb_x = feature_pad_to_npdata(test_text_emb) # text emb\n",
    "# test_text_emb_x = feature_pad_to_npdata(test_tag_emb) # tag emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tag_x = feature_pad_to_npdata(test_tag_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_positions_x = feature_pad_to_npdata(chunks_test_positions)\n",
    "test_tag_x = np.concatenate([test_tag_x, test_positions_x], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_info_x = np.concatenate([test_text_emb_x, test_tag_x], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_info_x\n",
    "# x_test = test_text_emb_x\n",
    "# x_test = test_tag_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 512, 1546)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pad_sequences([[tag2idx.get(l) for l in lab] for lab in chunks_test_y], maxlen=max_page_seq, padding=\"post\", truncating=\"post\", value=tag2idx[\"[PAD]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_crf_y = CRF_model.predict(x_test)\n",
    "predict_softmax_y = Softmax_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_crf_y = label_distribution_to_label(predict_crf_y)\n",
    "predict_softmax_y = label_distribution_to_label(predict_softmax_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_crf_y = np.asarray([[idx2tag.get(lab) for lab in page] for page in predict_crf_y])\n",
    "predict_softmax_y = np.asarray([[idx2tag.get(lab) for lab in page] for page in predict_softmax_y])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Baseline CRF\n",
    "predict_ncrf_y = N_CRF.predict(x_test)\n",
    "predict_ncrf_y = label_distribution_to_label(predict_ncrf_y)\n",
    "predict_ncrf_y = np.asarray([[idx2tag.get(lab) for lab in page] for page in predict_ncrf_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [[idx2tag.get(lab) for lab in page] for page in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_labels = ['PREV', 'PAGE', 'NEXT', '[PAD]', 'O']\n",
    "evaluate_labels = ['PREV', 'PAGE', 'NEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PREV      0.000     0.000     0.000        27\n",
      "        PAGE      0.740     0.750     0.745       300\n",
      "        NEXT      0.750     0.115     0.200        52\n",
      "\n",
      "   micro avg      0.729     0.609     0.664       379\n",
      "   macro avg      0.497     0.288     0.315       379\n",
      "weighted avg      0.689     0.609     0.617       379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(flat_classification_report(y_test, predict_crf_y, labels=evaluate_labels, digits=len(evaluate_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PREV      0.000     0.000     0.000        27\n",
      "        PAGE      0.755     0.780     0.767       300\n",
      "        NEXT      1.000     0.038     0.074        52\n",
      "\n",
      "   micro avg      0.756     0.623     0.683       379\n",
      "   macro avg      0.585     0.273     0.280       379\n",
      "weighted avg      0.735     0.623     0.617       379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(flat_classification_report(y_test, predict_softmax_y, labels=evaluate_labels, digits=len(evaluate_labels)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(flat_classification_report(y_test, predict_ncrf_y, labels=evaluate_labels, digits=len(evaluate_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
