{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../autopager/data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "from urllib.parse import urlparse, urlsplit, parse_qs, parse_qsl\n",
    "\n",
    "import numpy as np\n",
    "import parsel\n",
    "from sklearn_crfsuite.metrics import flat_classification_report, sequence_accuracy_score\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from autopager.storage import Storage\n",
    "from autopager.htmlutils import (get_link_text, get_text_around_selector_list,\n",
    "                                 get_link_href, get_selector_root)\n",
    "from autopager.utils import (\n",
    "    get_domain, normalize_whitespaces, normalize, ngrams, tokenize, ngrams_wb, replace_digits\n",
    ")\n",
    "from autopager.model import link_to_features, _num_tokens_feature, _elem_attr\n",
    "from autopager import AUTOPAGER_LIMITS\n",
    "storage = Storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "    except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [rec['Page URL'] for rec in storage.iter_records(contain_button = True, file_type='T')]\n",
    "groups = [get_domain(url) for url in urls]\n",
    "train_groups_set = set(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: Get Page 1 (Encoding: UTF-8)records ... (len: 303)\n",
      "Finish: Get Page 2 (Encoding: UTF-8)records ... (len: 243)\n",
      "Finish: Get Page 3 (Encoding: UTF-8)records ... (len: 119)\n",
      "Finish: Get Page 4 (Encoding: UTF-8)records ... (len: 944)\n",
      "Finish: Get Page 5 (Encoding: UTF-8)records ... (len: 93)\n",
      "Finish: Get Page 6 (Encoding: UTF-8)records ... (len: 994)\n",
      "Finish: Get Page 7 (Encoding: UTF-8)records ... (len: 1014)\n",
      "Finish: Get Page 8 (Encoding: UTF-8)records ... (len: 7)\n",
      "Finish: Get Page 9 (Encoding: UTF-8)records ... (len: 288)\n",
      "Finish: Get Page 10 (Encoding: UTF-8)records ... (len: 678)\n",
      "Finish: Get Page 11 (Encoding: UTF-8)records ... (len: 789)\n",
      "Finish: Get Page 12 (Encoding: UTF-8)records ... (len: 814)\n",
      "Finish: Get Page 13 (Encoding: UTF-8)records ... (len: 814)\n",
      "Finish: Get Page 14 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 15 (Encoding: UTF-8)records ... (len: 168)\n",
      "Finish: Get Page 16 (Encoding: UTF-8)records ... (len: 91)\n",
      "Finish: Get Page 17 (Encoding: UTF-8)records ... (len: 100)\n",
      "Finish: Get Page 18 (Encoding: UTF-8)records ... (len: 102)\n",
      "Finish: Get Page 19 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 20 (Encoding: UTF-8)records ... (len: 215)\n",
      "Finish: Get Page 21 (Encoding: UTF-8)records ... (len: 158)\n",
      "Finish: Get Page 22 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 23 (Encoding: UTF-8)records ... (len: 181)\n",
      "Finish: Get Page 24 (Encoding: UTF-8)records ... (len: 10)\n",
      "Finish: Get Page 25 (Encoding: UTF-8)records ... (len: 165)\n",
      "Finish: Get Page 26 (Encoding: UTF-8)records ... (len: 147)\n",
      "Finish: Get Page 27 (Encoding: UTF-8)records ... (len: 329)\n",
      "Finish: Get Page 28 (Encoding: UTF-8)records ... (len: 268)\n",
      "Finish: Get Page 29 (Encoding: UTF-8)records ... (len: 643)\n",
      "Finish: Get Page 30 (Encoding: UTF-8)records ... (len: 645)\n",
      "Finish: Get Page 31 (Encoding: UTF-8)records ... (len: 647)\n",
      "Finish: Get Page 32 (Encoding: UTF-8)records ... (len: 625)\n",
      "Finish: Get Page 33 (Encoding: UTF-8)records ... (len: 108)\n",
      "Finish: Get Page 34 (Encoding: UTF-8)records ... (len: 109)\n",
      "Finish: Get Page 35 (Encoding: UTF-8)records ... (len: 84)\n",
      "Finish: Get Page 36 (Encoding: UTF-8)records ... (len: 718)\n",
      "Finish: Get Page 37 (Encoding: UTF-8)records ... (len: 723)\n",
      "Finish: Get Page 38 (Encoding: UTF-8)records ... (len: 703)\n",
      "Finish: Get Page 39 (Encoding: UTF-8)records ... (len: 70)\n",
      "Finish: Get Page 40 (Encoding: UTF-8)records ... (len: 102)\n",
      "Finish: Get Page 41 (Encoding: UTF-8)records ... (len: 75)\n",
      "Finish: Get Page 42 (Encoding: UTF-8)records ... (len: 52)\n",
      "Finish: Get Page 43 (Encoding: UTF-8)records ... (len: 198)\n",
      "Finish: Get Page 44 (Encoding: UTF-8)records ... (len: 200)\n",
      "Finish: Get Page 45 (Encoding: UTF-8)records ... (len: 206)\n",
      "Finish: Get Page 46 (Encoding: UTF-8)records ... (len: 82)\n",
      "Finish: Get Page 47 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 48 (Encoding: UTF-8)records ... (len: 34)\n",
      "Finish: Get Page 49 (Encoding: UTF-8)records ... (len: 62)\n",
      "Finish: Get Page 50 (Encoding: UTF-8)records ... (len: 15)\n",
      "Finish: Get Page 51 (Encoding: UTF-8)records ... (len: 266)\n",
      "Finish: Get Page 52 (Encoding: UTF-8)records ... (len: 228)\n",
      "Finish: Get Page 53 (Encoding: UTF-8)records ... (len: 273)\n",
      "Finish: Get Page 54 (Encoding: cp1252)records ... (len: 283)\n",
      "Finish: Get Page 55 (Encoding: cp1252)records ... (len: 285)\n",
      "Finish: Get Page 56 (Encoding: cp1252)records ... (len: 520)\n",
      "Finish: Get Page 57 (Encoding: cp1252)records ... (len: 463)\n",
      "Finish: Get Page 58 (Encoding: UTF-8)records ... (len: 130)\n",
      "Finish: Get Page 59 (Encoding: UTF-8)records ... (len: 58)\n",
      "Finish: Get Page 60 (Encoding: UTF-8)records ... (len: 54)\n",
      "Finish: Get Page 61 (Encoding: UTF-8)records ... (len: 55)\n",
      "Finish: Get Page 62 (Encoding: UTF-8)records ... (len: 55)\n",
      "Finish: Get Page 63 (Encoding: UTF-8)records ... (len: 84)\n",
      "Finish: Get Page 64 (Encoding: UTF-8)records ... (len: 95)\n",
      "Finish: Get Page 65 (Encoding: iso8859-15)records ... (len: 702)\n",
      "Finish: Get Page 66 (Encoding: iso8859-15)records ... (len: 535)\n",
      "Finish: Get Page 67 (Encoding: iso8859-15)records ... (len: 538)\n",
      "Finish: Get Page 68 (Encoding: UTF-8)records ... (len: 37)\n",
      "Finish: Get Page 69 (Encoding: UTF-8)records ... (len: 312)\n",
      "Finish: Get Page 70 (Encoding: UTF-8)records ... (len: 127)\n",
      "Finish: Get Page 71 (Encoding: UTF-8)records ... (len: 104)\n",
      "Finish: Get Page 72 (Encoding: UTF-8)records ... (len: 92)\n",
      "Finish: Get Page 73 (Encoding: UTF-8)records ... (len: 149)\n",
      "Finish: Get Page 74 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 75 (Encoding: UTF-8)records ... (len: 386)\n",
      "Finish: Get Page 76 (Encoding: UTF-8)records ... (len: 319)\n",
      "Finish: Get Page 77 (Encoding: UTF-8)records ... (len: 114)\n",
      "Finish: Get Page 78 (Encoding: UTF-8)records ... (len: 118)\n",
      "Finish: Get Page 79 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 80 (Encoding: cp1251)records ... (len: 266)\n",
      "Finish: Get Page 81 (Encoding: cp1251)records ... (len: 274)\n",
      "Finish: Get Page 82 (Encoding: UTF-8)records ... (len: 139)\n",
      "Finish: Get Page 83 (Encoding: UTF-8)records ... (len: 408)\n",
      "Finish: Get Page 84 (Encoding: UTF-8)records ... (len: 94)\n",
      "Finish: Get Page 85 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 86 (Encoding: UTF-8)records ... (len: 230)\n",
      "Finish: Get Page 87 (Encoding: UTF-8)records ... (len: 106)\n",
      "Finish: Get Page 88 (Encoding: UTF-8)records ... (len: 110)\n",
      "Finish: Get Page 89 (Encoding: UTF-8)records ... (len: 448)\n",
      "Finish: Get Page 90 (Encoding: UTF-8)records ... (len: 452)\n",
      "Finish: Get Page 91 (Encoding: UTF-8)records ... (len: 2389)\n",
      "Finish: Get Page 92 (Encoding: UTF-8)records ... (len: 2379)\n",
      "Finish: Get Page 93 (Encoding: UTF-8)records ... (len: 160)\n",
      "Finish: Get Page 94 (Encoding: UTF-8)records ... (len: 74)\n",
      "Finish: Get Page 95 (Encoding: UTF-8)records ... (len: 143)\n",
      "Finish: Get Page 96 (Encoding: UTF-8)records ... (len: 133)\n",
      "Finish: Get Page 97 (Encoding: UTF-8)records ... (len: 163)\n",
      "Finish: Get Page 98 (Encoding: UTF-8)records ... (len: 378)\n",
      "Finish: Get Page 99 (Encoding: UTF-8)records ... (len: 120)\n",
      "Finish: Get Page 100 (Encoding: UTF-8)records ... (len: 124)\n",
      "Finish: Get Page 101 (Encoding: UTF-8)records ... (len: 122)\n",
      "Finish: Get Page 102 (Encoding: UTF-8)records ... (len: 430)\n",
      "Finish: Get Page 103 (Encoding: UTF-8)records ... (len: 260)\n",
      "Finish: Get Page 104 (Encoding: UTF-8)records ... (len: 252)\n",
      "Finish: Get Page 105 (Encoding: UTF-8)records ... (len: 439)\n",
      "Finish: Get Page 106 (Encoding: UTF-8)records ... (len: 422)\n",
      "Finish: Get Page 107 (Encoding: UTF-8)records ... (len: 423)\n",
      "Finish: Get Page 108 (Encoding: UTF-8)records ... (len: 233)\n",
      "Finish: Get Page 109 (Encoding: cp1252)records ... (len: 155)\n",
      "Finish: Get Page 110 (Encoding: cp1252)records ... (len: 161)\n",
      "Finish: Get Page 111 (Encoding: cp1252)records ... (len: 131)\n",
      "Finish: Get Page 112 (Encoding: cp1252)records ... (len: 131)\n",
      "Finish: Get Page 113 (Encoding: UTF-8)records ... (len: 72)\n",
      "Finish: Get Page 114 (Encoding: UTF-8)records ... (len: 70)\n",
      "Finish: Get Page 115 (Encoding: UTF-8)records ... (len: 126)\n",
      "Finish: Get Page 116 (Encoding: UTF-8)records ... (len: 90)\n",
      "Finish: Get Page 117 (Encoding: UTF-8)records ... (len: 89)\n",
      "Finish: Get Page 118 (Encoding: UTF-8)records ... (len: 79)\n",
      "Finish: Get Page 119 (Encoding: UTF-8)records ... (len: 133)\n",
      "Finish: Get Page 120 (Encoding: UTF-8)records ... (len: 89)\n",
      "Finish: Get Page 121 (Encoding: UTF-8)records ... (len: 81)\n",
      "Finish: Get Page 122 (Encoding: UTF-8)records ... (len: 167)\n",
      "Finish: Get Page 123 (Encoding: UTF-8)records ... (len: 165)\n",
      "Finish: Get Page 124 (Encoding: UTF-8)records ... (len: 124)\n",
      "Finish: Get Page 125 (Encoding: UTF-8)records ... (len: 16)\n",
      "Finish: Get Page 126 (Encoding: UTF-8)records ... (len: 8)\n",
      "Finish: Get Page 127 (Encoding: UTF-8)records ... (len: 154)\n",
      "Finish: Get Page 128 (Encoding: UTF-8)records ... (len: 51)\n",
      "Finish: Get Page 129 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 130 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 131 (Encoding: UTF-8)records ... (len: 53)\n",
      "Finish: Get Page 132 (Encoding: UTF-8)records ... (len: 306)\n",
      "Finish: Get Page 133 (Encoding: UTF-8)records ... (len: 309)\n",
      "Finish: Get Page 134 (Encoding: UTF-8)records ... (len: 103)\n",
      "Finish: Get Page 135 (Encoding: UTF-8)records ... (len: 88)\n",
      "Finish: Get Page 136 (Encoding: UTF-8)records ... (len: 159)\n",
      "Finish: Get Page 137 (Encoding: UTF-8)records ... (len: 35)\n",
      "Finish: Get Page 138 (Encoding: UTF-8)records ... (len: 112)\n",
      "Finish: Get Page 139 (Encoding: UTF-8)records ... (len: 117)\n",
      "Finish: Get Page 140 (Encoding: UTF-8)records ... (len: 142)\n",
      "Finish: Get Page 141 (Encoding: UTF-8)records ... (len: 116)\n",
      "Finish: Get Page 142 (Encoding: UTF-8)records ... (len: 31)\n",
      "Finish: Get Page 143 (Encoding: cp1252)records ... (len: 84)\n",
      "Finish: Get Page 144 (Encoding: cp1252)records ... (len: 134)\n",
      "Finish: Get Page 145 (Encoding: cp1252)records ... (len: 139)\n",
      "Finish: Get Page 146 (Encoding: cp1252)records ... (len: 95)\n",
      "Finish: Get Page 147 (Encoding: cp1252)records ... (len: 130)\n",
      "Finish: Get Page 148 (Encoding: UTF-8)records ... (len: 478)\n",
      "Finish: Get Page 149 (Encoding: UTF-8)records ... (len: 365)\n",
      "Finish: Get Page 150 (Encoding: UTF-8)records ... (len: 368)\n",
      "Finish: Get Page 151 (Encoding: UTF-8)records ... (len: 369)\n",
      "Finish: Get Page 152 (Encoding: cp1252)records ... (len: 294)\n",
      "Finish: Get Page 153 (Encoding: UTF-8)records ... (len: 271)\n",
      "Finish: Get Page 154 (Encoding: UTF-8)records ... (len: 300)\n",
      "Finish: Get Page 155 (Encoding: UTF-8)records ... (len: 314)\n",
      "Finish: Get Page 156 (Encoding: UTF-8)records ... (len: 278)\n",
      "Finish: Get Page 157 (Encoding: UTF-8)records ... (len: 288)\n",
      "Finish: Get Page 158 (Encoding: UTF-8)records ... (len: 178)\n",
      "Finish: Get Page 159 (Encoding: UTF-8)records ... (len: 108)\n",
      "Finish: Get Page 160 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 161 (Encoding: UTF-8)records ... (len: 101)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: Get Page 162 (Encoding: UTF-8)records ... (len: 279)\n",
      "Finish: Get Page 163 (Encoding: UTF-8)records ... (len: 269)\n",
      "Finish: Get Page 164 (Encoding: UTF-8)records ... (len: 259)\n",
      "Finish: Get Page 165 (Encoding: UTF-8)records ... (len: 133)\n",
      "Finish: Get Page 166 (Encoding: UTF-8)records ... (len: 181)\n",
      "Finish: Get Page 167 (Encoding: UTF-8)records ... (len: 94)\n",
      "Finish: Get Page 168 (Encoding: UTF-8)records ... (len: 99)\n",
      "Finish: Get Page 169 (Encoding: UTF-8)records ... (len: 103)\n",
      "Finish: Get Page 170 (Encoding: UTF-8)records ... (len: 210)\n",
      "Finish: Get Page 171 (Encoding: UTF-8)records ... (len: 208)\n",
      "Finish: Get Page 172 (Encoding: UTF-8)records ... (len: 179)\n",
      "Finish: Get Page 173 (Encoding: UTF-8)records ... (len: 461)\n",
      "Finish: Get Page 174 (Encoding: UTF-8)records ... (len: 340)\n",
      "Finish: Get Page 175 (Encoding: UTF-8)records ... (len: 188)\n",
      "Finish: Get Page 176 (Encoding: UTF-8)records ... (len: 195)\n",
      "Finish: Get Page 177 (Encoding: UTF-8)records ... (len: 40)\n",
      "Finish: Get Page 178 (Encoding: UTF-8)records ... (len: 256)\n",
      "Finish: Get Page 179 (Encoding: UTF-8)records ... (len: 258)\n",
      "Finish: Get Page 180 (Encoding: UTF-8)records ... (len: 256)\n",
      "Finish: Get Page 181 (Encoding: UTF-8)records ... (len: 227)\n",
      "Finish: Get Page 182 (Encoding: UTF-8)records ... (len: 229)\n",
      "Finish: Get Page 183 (Encoding: UTF-8)records ... (len: 189)\n",
      "Finish: Get Page 184 (Encoding: UTF-8)records ... (len: 401)\n",
      "Finish: Get Page 185 (Encoding: UTF-8)records ... (len: 954)\n",
      "Finish: Get Page 186 (Encoding: UTF-8)records ... (len: 427)\n",
      "Finish: Get Page 187 (Encoding: UTF-8)records ... (len: 426)\n",
      "Finish: Get Page 188 (Encoding: UTF-8)records ... (len: 191)\n",
      "Finish: Get Page 189 (Encoding: UTF-8)records ... (len: 129)\n",
      "Finish: Get Page 190 (Encoding: UTF-8)records ... (len: 254)\n",
      "Finish: Get Page 191 (Encoding: UTF-8)records ... (len: 247)\n",
      "Finish: Get Page 192 (Encoding: UTF-8)records ... (len: 243)\n",
      "Finish: Get Page 193 (Encoding: UTF-8)records ... (len: 89)\n",
      "Finish: Get Page 194 (Encoding: UTF-8)records ... (len: 57)\n",
      "Finish: Get Page 195 (Encoding: UTF-8)records ... (len: 95)\n",
      "Finish: Get Page 196 (Encoding: UTF-8)records ... (len: 238)\n",
      "Finish: Get Page 197 (Encoding: UTF-8)records ... (len: 182)\n",
      "Finish: Get Page 198 (Encoding: UTF-8)records ... (len: 183)\n",
      "Finish: Get Page 199 (Encoding: UTF-8)records ... (len: 148)\n",
      "Finish: Get Page 200 (Encoding: UTF-8)records ... (len: 347)\n",
      "Finish: Get Page 201 (Encoding: UTF-8)records ... (len: 199)\n",
      "Finish: Get Page 202 (Encoding: UTF-8)records ... (len: 200)\n",
      "Finish: Get Page 203 (Encoding: UTF-8)records ... (len: 361)\n",
      "Finish: Get Page 204 (Encoding: UTF-8)records ... (len: 356)\n",
      "Finish: Get Page 205 (Encoding: UTF-8)records ... (len: 360)\n",
      "Finish: Get Page 206 (Encoding: UTF-8)records ... (len: 331)\n",
      "Finish: Get Page 207 (Encoding: UTF-8)records ... (len: 498)\n",
      "Finish: Get Page 208 (Encoding: UTF-8)records ... (len: 499)\n",
      "Finish: Get Page 209 (Encoding: UTF-8)records ... (len: 497)\n",
      "Finish: Get Page 210 (Encoding: cp1252)records ... (len: 47)\n",
      "Finish: Get Page 211 (Encoding: UTF-8)records ... (len: 145)\n",
      "Finish: Get Page 212 (Encoding: UTF-8)records ... (len: 147)\n",
      "Finish: Get Page 213 (Encoding: UTF-8)records ... (len: 111)\n",
      "Finish: Get Page 214 (Encoding: UTF-8)records ... (len: 429)\n",
      "Finish: Get Page 215 (Encoding: UTF-8)records ... (len: 160)\n",
      "Finish: Get Page 216 (Encoding: UTF-8)records ... (len: 162)\n",
      "Finish: Get Page 217 (Encoding: UTF-8)records ... (len: 123)\n",
      "Finish: Get Page 218 (Encoding: UTF-8)records ... (len: 83)\n",
      "Finish: Get Page 219 (Encoding: UTF-8)records ... (len: 84)\n",
      "Finish: Get Page 220 (Encoding: UTF-8)records ... (len: 72)\n",
      "Finish: Get Page 221 (Encoding: UTF-8)records ... (len: 207)\n",
      "Finish: Get Page 222 (Encoding: UTF-8)records ... (len: 202)\n",
      "Finish: Get Page 223 (Encoding: UTF-8)records ... (len: 103)\n",
      "Finish: Get Page 224 (Encoding: cp1252)records ... (len: 59)\n",
      "Finish: Get Page 225 (Encoding: UTF-8)records ... (len: 58)\n",
      "Finish: Get Page 226 (Encoding: UTF-8)records ... (len: 346)\n",
      "Finish: Get Page 227 (Encoding: UTF-8)records ... (len: 281)\n",
      "Finish: Get Page 228 (Encoding: UTF-8)records ... (len: 164)\n",
      "Finish: Get Page 229 (Encoding: UTF-8)records ... (len: 66)\n",
      "Finish: Get Page 230 (Encoding: UTF-8)records ... (len: 69)\n",
      "Finish: Get Page 231 (Encoding: UTF-8)records ... (len: 613)\n",
      "Finish: Get Page 232 (Encoding: cp1252)records ... (len: 74)\n",
      "Finish: Get Page 233 (Encoding: UTF-8)records ... (len: 61)\n",
      "Finish: Get Page 234 (Encoding: UTF-8)records ... (len: 258)\n",
      "Finish: Get Page 235 (Encoding: UTF-8)records ... (len: 369)\n",
      "Finish: Get Page 236 (Encoding: UTF-8)records ... (len: 226)\n",
      "Finish: Get Page 237 (Encoding: UTF-8)records ... (len: 286)\n",
      "Finish: Get Page 238 (Encoding: UTF-8)records ... (len: 190)\n",
      "Finish: Get Page 239 (Encoding: cp1252)records ... (len: 256)\n",
      "Finish: Get Page 240 (Encoding: cp1252)records ... (len: 148)\n",
      "Finish: Get Page 241 (Encoding: UTF-8)records ... (len: 98)\n",
      "Finish: Get Page 242 (Encoding: UTF-8)records ... (len: 164)\n",
      "Finish: Get Page 243 (Encoding: UTF-8)records ... (len: 169)\n",
      "Finish: Get Page 244 (Encoding: UTF-8)records ... (len: 139)\n",
      "Finish: Get Page 245 (Encoding: UTF-8)records ... (len: 131)\n",
      "Finish: Get Page 246 (Encoding: cp1252)records ... (len: 528)\n",
      "Finish: Get Page 247 (Encoding: UTF-8)records ... (len: 217)\n",
      "Finish: Get Page 248 (Encoding: UTF-8)records ... (len: 119)\n",
      "Finish: Get Page 249 (Encoding: UTF-8)records ... (len: 61)\n",
      "Finish: Get Page 250 (Encoding: UTF-8)records ... (len: 107)\n",
      "Finish: Get Page 251 (Encoding: cp1252)records ... (len: 467)\n",
      "Finish: Get Page 252 (Encoding: cp1252)records ... (len: 469)\n",
      "Finish: Get Page 253 (Encoding: UTF-8)records ... (len: 144)\n",
      "Finish: Get Page 254 (Encoding: UTF-8)records ... (len: 43)\n",
      "Finish: Get Page 255 (Encoding: UTF-8)records ... (len: 44)\n",
      "Finish: Get Page 257 (Encoding: UTF-8)records ... (len: 385)\n",
      "Finish: Get Page 258 (Encoding: UTF-8)records ... (len: 355)\n",
      "Finish: Get Page 259 (Encoding: UTF-8)records ... (len: 72)\n",
      "Finish: Get Page 260 (Encoding: UTF-8)records ... (len: 73)\n",
      "Finish: Get Page 261 (Encoding: UTF-8)records ... (len: 306)\n",
      "Finish: Get Page 262 (Encoding: UTF-8)records ... (len: 129)\n",
      "Finish: Get Page 263 (Encoding: UTF-8)records ... (len: 171)\n",
      "Finish: Get Page 264 (Encoding: UTF-8)records ... (len: 53)\n",
      "Finish: Get Page 265 (Encoding: UTF-8)records ... (len: 199)\n",
      "Finish: Get Page 266 (Encoding: UTF-8)records ... (len: 86)\n",
      "Finish: Get Page 267 (Encoding: UTF-8)records ... (len: 131)\n",
      "Finish: Get Page 268 (Encoding: UTF-8)records ... (len: 175)\n",
      "Finish: Get Page 269 (Encoding: UTF-8)records ... (len: 176)\n",
      "Finish: Get Page 271 (Encoding: UTF-8)records ... (len: 302)\n",
      "Finish: Get Page 272 (Encoding: UTF-8)records ... (len: 304)\n",
      "Finish: Get Page 273 (Encoding: UTF-8)records ... (len: 180)\n",
      "Finish: Get Page 274 (Encoding: UTF-8)records ... (len: 529)\n",
      "Finish: Get Page 275 (Encoding: UTF-8)records ... (len: 310)\n",
      "Finish: Get Page 276 (Encoding: UTF-8)records ... (len: 80)\n",
      "Finish: Get Page 277 (Encoding: UTF-8)records ... (len: 165)\n",
      "Finish: Get Page 278 (Encoding: UTF-8)records ... (len: 337)\n",
      "Finish: Get Page 279 (Encoding: cp1252)records ... (len: 253)\n",
      "Finish: Get Page 280 (Encoding: cp1252)records ... (len: 239)\n",
      "Finish: Get Page 281 (Encoding: cp1252)records ... (len: 243)\n",
      "Finish: Get Page 282 (Encoding: UTF-8)records ... (len: 68)\n",
      "Finish: Get Page 283 (Encoding: UTF-8)records ... (len: 59)\n",
      "Finish: Get Page 284 (Encoding: cp1252)records ... (len: 130)\n",
      "Finish: Get Page 285 (Encoding: UTF-8)records ... (len: 139)\n",
      "Finish: Get Page 286 (Encoding: UTF-8)records ... (len: 166)\n",
      "Finish: Get Page 287 (Encoding: UTF-8)records ... (len: 82)\n",
      "Finish: Get Page 288 (Encoding: UTF-8)records ... (len: 140)\n",
      "Finish: Get Page 289 (Encoding: UTF-8)records ... (len: 44)\n",
      "Finish: Get Page 290 (Encoding: UTF-8)records ... (len: 330)\n",
      "Finish: Get Page 291 (Encoding: UTF-8)records ... (len: 331)\n",
      "Finish: Get Page 292 (Encoding: UTF-8)records ... (len: 280)\n",
      "Finish: Get Page 293 (Encoding: UTF-8)records ... (len: 74)\n",
      "Finish: Get Page 294 (Encoding: UTF-8)records ... (len: 63)\n",
      "Finish: Get Page 295 (Encoding: UTF-8)records ... (len: 65)\n",
      "Finish: Get Page 296 (Encoding: UTF-8)records ... (len: 20)\n",
      "Finish: Get Page 297 (Encoding: UTF-8)records ... (len: 367)\n",
      "Finish: Get Page 298 (Encoding: UTF-8)records ... (len: 371)\n",
      "Finish: Get Page 299 (Encoding: UTF-8)records ... (len: 361)\n",
      "Finish: Get Page 300 (Encoding: UTF-8)records ... (len: 361)\n",
      "Finish: Get Page 301 (Encoding: UTF-8)records ... (len: 364)\n",
      "Finish: Get Page 302 (Encoding: UTF-8)records ... (len: 170)\n",
      "Finish: Get Page 303 (Encoding: UTF-8)records ... (len: 154)\n",
      "Finish: Get Page 304 (Encoding: cp1252)records ... (len: 117)\n",
      "Finish: Get Page 305 (Encoding: UTF-8)records ... (len: 1987)\n",
      "Finish: Get Page 306 (Encoding: UTF-8)records ... (len: 59)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish: Get Page 307 (Encoding: UTF-8)records ... (len: 60)\n",
      "Finish: Get Page 308 (Encoding: UTF-8)records ... (len: 60)\n",
      "Finish: Get Page 309 (Encoding: UTF-8)records ... (len: 145)\n",
      "Finish: Get Page 310 (Encoding: UTF-8)records ... (len: 116)\n",
      "Finish: Get Page 311 (Encoding: UTF-8)records ... (len: 119)\n",
      "Finish: Get Page 312 (Encoding: cp1252)records ... (len: 136)\n",
      "Finish: Get Page 313 (Encoding: UTF-8)records ... (len: 383)\n",
      "Finish: Get Page 314 (Encoding: UTF-8)records ... (len: 317)\n",
      "Finish: Get Page 315 (Encoding: cp1252)records ... (len: 314)\n",
      "Finish: Get Page 316 (Encoding: cp1252)records ... (len: 357)\n",
      "Finish: Get Page 317 (Encoding: cp1252)records ... (len: 370)\n",
      "Finish: Get Page 318 (Encoding: UTF-8)records ... (len: 137)\n",
      "Finish: Get Page 319 (Encoding: UTF-8)records ... (len: 281)\n",
      "Finish: Get Page 320 (Encoding: UTF-8)records ... (len: 281)\n",
      "Finish: Get Page 321 (Encoding: UTF-8)records ... (len: 247)\n",
      "Finish: Get Page 322 (Encoding: UTF-8)records ... (len: 248)\n"
     ]
    }
   ],
   "source": [
    "X_raw, y = storage.get_Xy(contain_button = True, file_type='T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_page_seq = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_data(x, y):\n",
    "    new_tmp_x_array = []\n",
    "    new_tmp_y_array = []\n",
    "    for tmp_x, tmp_y in zip(x, y):\n",
    "        new_tmp_x_array.extend(chunks(tmp_x, max_page_seq))\n",
    "        new_tmp_y_array.extend(chunks(tmp_y, max_page_seq))\n",
    "    return new_tmp_x_array, new_tmp_y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_x, chunks_y = get_chunks_data(X_raw, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import bert\n",
    "from bert import tokenization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(f\"Token length more than max seq length! {len(tokens)} > {max_seq_length}\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    segments = []\n",
    "    current_segment_id = 0\n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            current_segment_id += 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "def get_bert_inputs_from_sequences(seqs, tokenizer, max_seq_length, Token):\n",
    "    if Token is False:\n",
    "        tokens_list = [tokenizer.tokenize(seq) for seq in seqs]\n",
    "    else:\n",
    "        tokens_list = seqs\n",
    "    ids = [ np.array(get_ids(tokens, tokenizer, max_seq_length)) for tokens in tokens_list ]\n",
    "    masks = [ np.array(get_masks(tokens, max_seq_length))  for tokens in tokens_list ]\n",
    "    segments = [ np.array(get_segments(tokens, max_seq_length))  for tokens in tokens_list ]\n",
    "    return np.array(ids), np.array(masks), np.array(segments)\n",
    "\n",
    "def page_list_to_bert_embedding_list(page_list, model, tokenizer, max_seq_length, Token = False):\n",
    "    print(f\"Use custom Token: {Token}\")\n",
    "    p = IntProgress(max=len(page_list))\n",
    "    p.description = '(Init)'\n",
    "    p.value = 0\n",
    "    display(p)\n",
    "    seq_list = []\n",
    "    for idx, page in enumerate(page_list):\n",
    "        p.description = f\"Task: {idx+1}\"\n",
    "        p.value = idx+1\n",
    "        page_idx, page_mask, page_seg = get_bert_inputs_from_sequences(page, tokenizer, max_seq_length, Token)\n",
    "        pooled_emb, _ = model.predict([ page_idx, page_mask, page_seg ])\n",
    "        seq_list.append(pooled_emb)\n",
    "    p.description = '(Done)'\n",
    "    return seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 256  # Your choice here.\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15 µs, sys: 0 ns, total: 15 µs\n",
      "Wall time: 18.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# XXX: these functions should be copy-pasted from autopager/model.py\n",
    "\n",
    "def _as_list(generator, limit=None):\n",
    "    \"\"\"\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2), 0)\n",
    "    []\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2), 2)\n",
    "    ['te', 'ex']\n",
    "    >>> _as_list(ngrams_wb(\"text\", 2, 2))\n",
    "    ['te', 'ex', 'xt']\n",
    "    \"\"\"\n",
    "    return list(generator if limit is None else islice(generator, 0, limit))\n",
    "\n",
    "def feat_to_tokens(feat, tokenizer):\n",
    "    if type(feat) == type([]):\n",
    "        feat = ' '.join(feat)\n",
    "    tokens = tokenizer.tokenize(feat)\n",
    "    return tokens\n",
    "\n",
    "def link_to_features(link, tokenizer):\n",
    "    text = normalize(get_link_text(link))\n",
    "    href = get_link_href(link)\n",
    "    if href is None:\n",
    "        href = \"\"\n",
    "    p = urlsplit(href)\n",
    "    query_parsed = parse_qsl(p.query) #parse query string from path\n",
    "    query_param_names = [k.lower() for k, v in query_parsed]\n",
    "    query_param_names_ngrams = _as_list(ngrams_wb(\n",
    "        \" \".join([normalize(name) for name in query_param_names]), 3, 5, True\n",
    "    ))\n",
    "\n",
    "    elem = get_selector_root(link)\n",
    "    elem_target = _elem_attr(elem, 'target')\n",
    "    elem_rel = _elem_attr(elem, 'rel')\n",
    "    # Classes of link itself and all its children.\n",
    "    # It is common to have e.g. span elements with fontawesome\n",
    "    # arrow icon classes inside <a> links.\n",
    "    self_and_children_classes = ' '.join(_as_list(link.xpath(\".//@class\").extract(), 5))\n",
    "    parent_classes = ' '.join(_as_list(link.xpath('../@class').extract(), 5))\n",
    "    css_classes = normalize(parent_classes + ' ' + self_and_children_classes)\n",
    "    token_feature = {\n",
    "        'text-before': '',\n",
    "        'text-exact': replace_digits(text.strip()[:40].strip()),\n",
    "        'text-after': '',\n",
    "        'elem-target': elem_target,\n",
    "        'elem-rel': elem_rel,\n",
    "        'class': css_classes,\n",
    "        'query': _as_list(query_param_names, 10)\n",
    "    }\n",
    "    tag_feature = {\n",
    "        'isdigit': 1 if text.isdigit() is True else 0,\n",
    "        'isalpha': 1 if text.isalpha() is True else 0,\n",
    "        'path-has-page': 1 if 'page' in p.path.lower() else 0,\n",
    "        'path-has-pageXX': 1 if re.search(r'[/-](?:p|page\\w?)/?\\d+', p.path.lower()) is not None else 0,\n",
    "        'path-has-number': 1 if any(part.isdigit() for part in p.path.split('/')) else 0,\n",
    "        'href-has-year': 1 if re.search('20\\d\\d', href) is not None else 0\n",
    "    }\n",
    "    return [token_feature, tag_feature]\n",
    "\n",
    "\n",
    "def page_to_features(xseq, tokenizer):\n",
    "    feat_list = [link_to_features(a, tokenizer) for a in xseq]\n",
    "    around = get_text_around_selector_list(xseq, max_length=15)\n",
    "    \n",
    "    # weight is less than 1 because there is a lot of duplicate information\n",
    "    # in these ngrams and so we want to regularize them stronger\n",
    "    # (as if they are a single feature, not many features)\n",
    "    k = 0.2\n",
    "    for feat, (before, after) in zip(feat_list, around):\n",
    "        feat[0]['text-before'] = normalize(before)\n",
    "        feat[0]['text-after'] = normalize(after)\n",
    "    return feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_tag_features_from_chunks(chunks):\n",
    "    token_features = []\n",
    "    tag_features = []\n",
    "    for page in chunks:\n",
    "        feat_list = page_to_features(page, tokenizer)\n",
    "        token_features.append([node[0] for node in feat_list])\n",
    "        tag_features.append([node[1] for node in feat_list])\n",
    "    return token_features, tag_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tokens_from_token_features(token_features):\n",
    "    train_tag_feature_token_list = []\n",
    "    for page in token_features:\n",
    "        tmp_page_list = []\n",
    "        for node in page: \n",
    "            tmp_list = []\n",
    "            for k, v in node.items():\n",
    "                if k == 'text-exact':\n",
    "                    continue\n",
    "                else:\n",
    "                    tmp_list.extend(v)\n",
    "            tmp_page_list.append(tmp_list)\n",
    "        train_tag_feature_token_list.append(tmp_page_list)\n",
    "    return train_tag_feature_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_features_bert_preprocessing(token_features, type = None, addNone = False):\n",
    "    train_token_features = []\n",
    "    if type == None or type not in ['single','multi','multi-two']:\n",
    "        print(\"Must Given a type of pre-processing\")\n",
    "        return\n",
    "    for page in token_features:\n",
    "        page_features = []\n",
    "        for node in page:\n",
    "            node_features = [\"[CLS]\"]\n",
    "            sep_two = False\n",
    "            for k,v in node.items():\n",
    "                value_tokens = feat_to_tokens(v, tokenizer)\n",
    "                if addNone == True and len(value_tokens) == 0:\n",
    "                    value_tokens = [\"None\"]\n",
    "                if type == 'single':\n",
    "                    node_features = node_features + value_tokens\n",
    "                    if k == 'text-after':\n",
    "                        node_features = node_features + [\"[SEP]\"]\n",
    "                elif type == 'multi':\n",
    "                    if k == 'text-after' or sep_two is True:\n",
    "                        sep_two = True\n",
    "                        node_features = node_features + value_tokens + [\"[SEP]\"]\n",
    "                    else:\n",
    "                        node_features = node_features + value_tokens\n",
    "                elif type == 'multi-two':\n",
    "                    node_features = node_features + value_tokens + [\"[SEP]\"]\n",
    "            if 'multi' in type:\n",
    "                node_features = node_features[:-1]\n",
    "            page_features.append(node_features)\n",
    "        train_token_features.append(page_features)\n",
    "    return train_token_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dicts_to_values(pages):\n",
    "    return [[[v for k,v in node.items()] for node in p ] for p in pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_features, tag_features = get_token_tag_features_from_chunks(chunks_x)\n",
    "# train_tag_feature_token_list = extract_tokens_from_token_features(token_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_token_features = token_features_bert_preprocessing(token_features,type = 'single')\n",
    "# train_token_features = token_features_bert_preprocessing(token_features,type = 'multi')\n",
    "train_token_features = token_features_bert_preprocessing(token_features,type = 'multi-two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tag_info_list = flatten_dicts_to_values(tag_features) #features which only have tag true/false information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '[SEP]',\n",
       " 'one',\n",
       " '##plus',\n",
       " 'X',\n",
       " '##t',\n",
       " '[SEP]',\n",
       " '[SEP]',\n",
       " '[SEP]',\n",
       " '[SEP]',\n",
       " 'page',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token_features[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  24.858043613697152\n",
      "Max_node:  215\n"
     ]
    }
   ],
   "source": [
    "max_node = -1\n",
    "page_sum = 0\n",
    "for page in train_token_features:\n",
    "    sum = 0\n",
    "    for node in page:\n",
    "        sum+=len(node)\n",
    "        if len(node) > max_node:\n",
    "            max_node = len(node)\n",
    "    page_sum+=sum/len(page)\n",
    "print(\"Average: \", page_sum/len(train_token_features))\n",
    "print(\"Max_node: \",max_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text-exact feature in token_features\n",
    "train_text_list = [[ data['text-exact'] for data in x] for x in token_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a9eafd8f8f4c1cb43c2bffae43ae51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=353)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_text_emb = page_list_to_bert_embedding_list(train_text_list, emb_model, tokenizer, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83ee9a6e513464f8f39a7cfd98fe15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=353)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tag_emb = page_list_to_bert_embedding_list(train_token_features, emb_model, tokenizer, max_seq_length, Token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 794,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tag_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 768)"
      ]
     },
     "execution_count": 795,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tag_emb[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split chunks into three type of training data\n",
    "    (1) Every chunks\n",
    "    (2) Chunks only have tag informations + Chunks only have Other\n",
    "\n",
    "### Feature List\n",
    "    * train_tag_feature_token_list => Tag Attributes tokens\n",
    "    * train_tag_info_list => Tag information\n",
    "    * train_text_emb => Only Text node => Bert Text embedding\n",
    "    * train_tag_emb => Text-before Text Text-after [SEP] Other Attributes => Bert Text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onlyHavaOther(y):\n",
    "    for tag in y:\n",
    "        if tag != 'O':\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_only_data_idx = [idx for idx, row_y in enumerate(chunks_y) if not onlyHavaOther(row_y)]\n",
    "chunks_only_other_idx = [idx for idx, row_y in enumerate(chunks_y) if onlyHavaOther(row_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags data:  228\n",
      "Other data:  125\n"
     ]
    }
   ],
   "source": [
    "print(\"Tags data: \", len(chunks_only_data_idx))\n",
    "print(\"Other data: \", len(chunks_only_other_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilterChunks(chunks, filterIdx):\n",
    "    # chunks: pages\n",
    "    # filterIdx: Idx of list which indicate the return data\n",
    "    return [page for idx, page in enumerate(chunks) if idx in filterIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingData(types = None):\n",
    "    if types == None:\n",
    "        if len(train_text_emb) != len(train_tag_info_list) or len(train_tag_info_list) != len(chunks_y) or len(train_tag_info_list) != len(train_tag_feature_token_list):\n",
    "            raise Exception('Every chunks should have equal size')\n",
    "        print(f\"return {len(chunks_y)} data.\")\n",
    "        return (train_text_emb, train_tag_feature_token_list, train_tag_info_list, chunks_y)\n",
    "    if types == 'Tags':\n",
    "        chunks_text_x = getFilterChunks(train_text_emb, chunks_only_data_idx)\n",
    "        chunks_token_x = getFilterChunks(train_tag_feature_token_list, chunks_only_data_idx)\n",
    "        chunks_tag_x = getFilterChunks(train_tag_info_list, chunks_only_data_idx)\n",
    "        chunks_filtered_y = getFilterChunks(chunks_y, chunks_only_data_idx)\n",
    "        if len(chunks_text_x) != len(chunks_tag_x) or len(chunks_tag_x) != len(chunks_filtered_y) or len(chunks_token_x) != len(chunks_tag_x):\n",
    "            raise Exception('Every chunks should have equal size')\n",
    "        print(f\"return {len(chunks_filtered_y)} data.\")\n",
    "        return (chunks_text_x, chunks_token_x, chunks_tag_x, chunks_filtered_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_text_emb = train_tag_emb\n",
    "chunks_tag_infos = train_tag_info_list\n",
    "chunks_filtered_y = chunks_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_text_emb = getFilterChunks(train_tag_emb, chunks_only_data_idx)\n",
    "chunks_tag_infos = getFilterChunks(train_tag_info_list, chunks_only_data_idx)\n",
    "chunks_filtered_y = getFilterChunks(chunks_y, chunks_only_data_idx)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "chunks_text_emb, chunks_tag_tokens, chunks_tag_infos, chunks_filtered_y = getTrainingData()\n",
    "# chunks_text_emb, chunks_tag_tokens, chunks_tag_infos, chunks_filtered_y = getTrainingData(types = 'Tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding to fixed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_pad_to_npdata(embedding):\n",
    "    dataset = Dataset.from_generator(lambda: iter(embedding), tf.float32)\n",
    "    dataset = dataset.padded_batch(1, padded_shapes= (max_page_seq, len(embedding[0][0])), padding_values=-1.,drop_remainder=False)\n",
    "    after_pad = np.array([ data[0] for data in list(dataset.as_numpy_iterator())])\n",
    "    return after_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tag_token = feature_pad_to_npdata(chunks_tag_tokens)\n",
    "train_tag_x = feature_pad_to_npdata(chunks_tag_infos)\n",
    "train_text_emb_x = feature_pad_to_npdata(chunks_text_emb)\n",
    "train_info_x = np.concatenate([train_text_emb_x, train_tag_x], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"O\", \"PREV\", \"PAGE\", \"NEXT\", \"[PAD]\"]\n",
    "tag2idx = { label:idx for idx,label in enumerate(labels)}\n",
    "idx2tag = { idx:label for idx,label in enumerate(labels)}\n",
    "num_tags = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pad_sequences([[tag2idx.get(l) for l in lab] for lab in chunks_filtered_y], maxlen=max_page_seq, padding=\"post\", truncating=\"post\", value=tag2idx[\"[PAD]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Shape:\n",
      "train_text_emb_x: (228, 512, 768)\n",
      "train_tag_x: (228, 512, 6)\n",
      "train_info_x: (228, 512, 774)\n",
      "train_y: (228, 512)\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Shape:\")\n",
    "print(f\"train_text_emb_x: {train_text_emb_x.shape}\")\n",
    "print(f\"train_tag_x: {train_tag_x.shape}\")\n",
    "print(f\"train_info_x: {train_info_x.shape}\")\n",
    "print(f\"train_y: {train_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = train_text_emb_x\n",
    "train_x = train_info_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build BERT-BiLSTM-CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.layers.crf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME_STAMP: 512\n",
      "HIDDEN_UNITS: 200\n",
      "DROPOUT_RATE: 0.1\n",
      "NUM_CLASS: 5\n"
     ]
    }
   ],
   "source": [
    "TIME_STAMPS = max_page_seq\n",
    "HIDDEN_UNITS = 200\n",
    "DROPOUT_RATE = 0.1\n",
    "# NUM_CLASS = 5\n",
    "NUM_CLASS = num_tags\n",
    "print(f\"TIME_STAMP: {TIME_STAMPS}\")\n",
    "print(f\"HIDDEN_UNITS: {HIDDEN_UNITS}\")\n",
    "print(f\"DROPOUT_RATE: {DROPOUT_RATE}\")\n",
    "print(f\"NUM_CLASS: {NUM_CLASS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BERT_BILSTM_CRF(SHAPE, numtags):\n",
    "    print(f\"SHAPE: {SHAPE}\")\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=SHAPE))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=HIDDEN_UNITS, return_sequences=True)))\n",
    "    crf=CRF(numtags,name='crf_layer')\n",
    "    model.add(crf)\n",
    "    model.compile('adam',loss={'crf_layer': crf.get_loss})\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_BERT_BILSTM_SOFTMAX(SHAPE, numtags):\n",
    "    print(f\"SHAPE: {SHAPE}\")\n",
    "    model=tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=SHAPE))\n",
    "#     model.add(tf.keras.layers.Masking(input_shape=SHAPE, mask_value=-1.))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=HIDDEN_UNITS, return_sequences=True)))\n",
    "    model.add(tf.keras.layers.Dense(units = numtags, activation='softmax'))\n",
    "    model.compile('adam',loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE: (512, 774)\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_47 (Bidirectio (None, 512, 400)          1560000   \n",
      "_________________________________________________________________\n",
      "crf_layer (CRF)              (None, 512)               2040      \n",
      "=================================================================\n",
      "Total params: 1,562,040\n",
      "Trainable params: 1,562,040\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CRF_model = get_BERT_BILSTM_CRF(train_x.shape[1:], num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE: (512, 774)\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_48 (Bidirectio (None, 512, 400)          1560000   \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 512, 5)            2005      \n",
      "=================================================================\n",
      "Total params: 1,562,005\n",
      "Trainable params: 1,562,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Softmax_model = get_BERT_BILSTM_SOFTMAX(train_x.shape[1:], num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 512, 774)"
      ]
     },
     "execution_count": 853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 2s 813ms/step - loss: 539.7711 - val_loss: 55.1894\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 54.6941 - val_loss: 56.6561\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 56.6650 - val_loss: 54.3063\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 53.8538 - val_loss: 48.2156\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 47.7400 - val_loss: 42.7297\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 42.2154 - val_loss: 43.2119\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 1s 383ms/step - loss: 42.7686 - val_loss: 46.0933\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 43.8994 - val_loss: 42.6119\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 40.8657 - val_loss: 40.1683\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 1s 394ms/step - loss: 39.7863 - val_loss: 40.0143\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 39.9091 - val_loss: 39.8674\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 39.7967 - val_loss: 39.2178\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 38.9751 - val_loss: 38.2833\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 1s 378ms/step - loss: 37.7399 - val_loss: 37.8504\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 37.0759 - val_loss: 38.3254\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 37.1078 - val_loss: 38.5978\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 36.9712 - val_loss: 37.6863\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 36.2279 - val_loss: 36.6258\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 35.7255 - val_loss: 36.1013\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 35.4761 - val_loss: 35.8072\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 35.2866 - val_loss: 35.4498\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 34.9139 - val_loss: 35.0476\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 34.3724 - val_loss: 34.8229\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 33.9167 - val_loss: 34.6455\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 33.5500 - val_loss: 34.2561\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 33.0866 - val_loss: 33.5829\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 1s 469ms/step - loss: 32.5631 - val_loss: 32.9298\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 32.0678 - val_loss: 32.4282\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 31.6074 - val_loss: 31.9382\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 31.1096 - val_loss: 31.5400\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 30.5716 - val_loss: 31.0873\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 1s 446ms/step - loss: 30.0853 - val_loss: 30.7033\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 29.6985 - val_loss: 30.3203\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 29.1814 - val_loss: 30.2777\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 1s 389ms/step - loss: 28.9212 - val_loss: 29.6265\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 28.3193 - val_loss: 29.1396\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 28.1432 - val_loss: 28.6507\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 27.6994 - val_loss: 28.3573\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 1s 374ms/step - loss: 27.0175 - val_loss: 27.8932\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 26.8991 - val_loss: 27.1949\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 26.4282 - val_loss: 26.7777\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 25.6124 - val_loss: 26.9341\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 25.8599 - val_loss: 25.8701\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 25.2531 - val_loss: 25.4790\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 1s 385ms/step - loss: 24.7215 - val_loss: 25.6152\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 24.6823 - val_loss: 24.9199\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 23.7306 - val_loss: 24.3761\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 23.1487 - val_loss: 24.3186\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 22.9860 - val_loss: 23.9081\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 22.5448 - val_loss: 23.2388\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 1s 385ms/step - loss: 21.8445 - val_loss: 23.2338\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 1s 440ms/step - loss: 21.8145 - val_loss: 23.0424\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 21.5834 - val_loss: 22.6463\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 21.3834 - val_loss: 21.8982\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 20.3238 - val_loss: 22.8336\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 1s 445ms/step - loss: 20.9699 - val_loss: 21.6546\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 1s 380ms/step - loss: 20.1578 - val_loss: 21.4175\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 19.7122 - val_loss: 20.7380\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 19.5927 - val_loss: 20.4640\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 19.1424 - val_loss: 20.4988\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 18.4747 - val_loss: 20.7475\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 19.1904 - val_loss: 20.6929\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 18.5227 - val_loss: 19.8933\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 17.7899 - val_loss: 19.6932\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 1s 463ms/step - loss: 17.7361 - val_loss: 19.8307\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 17.2554 - val_loss: 19.3848\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 16.9771 - val_loss: 19.3344\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 16.8460 - val_loss: 19.2640\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 16.6136 - val_loss: 19.2471\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 16.2039 - val_loss: 19.4894\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 1s 464ms/step - loss: 16.1302 - val_loss: 19.1615\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 15.8778 - val_loss: 19.3977\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 15.5558 - val_loss: 18.9951\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 15.5205 - val_loss: 20.6542\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 16.6069 - val_loss: 19.2070\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 15.1967 - val_loss: 19.2142\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 14.9955 - val_loss: 18.7264\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 14.8712 - val_loss: 19.1240\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 1s 453ms/step - loss: 14.5090 - val_loss: 19.2758\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 14.8460 - val_loss: 19.7711\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 14.7423 - val_loss: 18.7283\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 1s 449ms/step - loss: 13.6319 - val_loss: 18.8535\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 13.3622 - val_loss: 18.5454\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 13.2452 - val_loss: 18.0151\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 12.9457 - val_loss: 19.0717\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 13.3034 - val_loss: 18.6099\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 12.9316 - val_loss: 18.6231\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 12.5355 - val_loss: 18.3783\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 1s 370ms/step - loss: 12.7922 - val_loss: 18.3585\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 12.6411 - val_loss: 20.1069\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 13.3349 - val_loss: 18.4026\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 1s 375ms/step - loss: 12.2158 - val_loss: 18.5172\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 12.5116 - val_loss: 19.3404\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 12.8841 - val_loss: 19.3855\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 14.7546 - val_loss: 19.6452\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 14.2384 - val_loss: 19.2070\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 14.2769 - val_loss: 18.9131\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 14.1116 - val_loss: 20.1202\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 1s 373ms/step - loss: 14.0778 - val_loss: 18.2615\n"
     ]
    }
   ],
   "source": [
    "CRF_history = CRF_model.fit(train_x, train_y, batch_size=128, epochs=500, validation_split=0.2, verbose=1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 1s 342ms/step - loss: 1.1560 - val_loss: 0.1170\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.1135 - val_loss: 0.1163\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.1200 - val_loss: 0.1194\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.1218 - val_loss: 0.1109\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.1122 - val_loss: 0.0979\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0996 - val_loss: 0.0897\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0915 - val_loss: 0.0937\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0943 - val_loss: 0.0981\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0955 - val_loss: 0.0923\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0906 - val_loss: 0.0872\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0875 - val_loss: 0.0860\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.0872 - val_loss: 0.0858\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0872 - val_loss: 0.0852\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0863 - val_loss: 0.0842\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0848 - val_loss: 0.0835\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0836 - val_loss: 0.0838\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0834 - val_loss: 0.0846\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0832 - val_loss: 0.0843\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0827 - val_loss: 0.0831\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0819 - val_loss: 0.0819\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 0.0814 - val_loss: 0.0811\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0808 - val_loss: 0.0805\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 114ms/step - loss: 0.0803 - val_loss: 0.0801\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0798 - val_loss: 0.0797\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0794 - val_loss: 0.0794\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0788 - val_loss: 0.0790\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0784 - val_loss: 0.0784\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0778 - val_loss: 0.0779\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0772 - val_loss: 0.0773\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0766 - val_loss: 0.0768\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 107ms/step - loss: 0.0760 - val_loss: 0.0763\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0752 - val_loss: 0.0756\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0745 - val_loss: 0.0748\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0738 - val_loss: 0.0741\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 112ms/step - loss: 0.0730 - val_loss: 0.0730\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0720 - val_loss: 0.0717\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 113ms/step - loss: 0.0711 - val_loss: 0.0706\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0699 - val_loss: 0.0699\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0687 - val_loss: 0.0695\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0678 - val_loss: 0.0675\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0661 - val_loss: 0.0662\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0653 - val_loss: 0.0649\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0635 - val_loss: 0.0643\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0625 - val_loss: 0.0627\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0614 - val_loss: 0.0617\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0601 - val_loss: 0.0598\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0595 - val_loss: 0.0588\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 116ms/step - loss: 0.0580 - val_loss: 0.0571\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0562 - val_loss: 0.0565\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0558 - val_loss: 0.0557\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0544 - val_loss: 0.0534\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0525 - val_loss: 0.0525\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0518 - val_loss: 0.0509\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0502 - val_loss: 0.0496\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0490 - val_loss: 0.0484\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 0.0478 - val_loss: 0.0473\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0467 - val_loss: 0.0463\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0461 - val_loss: 0.0506\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 108ms/step - loss: 0.0489 - val_loss: 0.0457\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 115ms/step - loss: 0.0454 - val_loss: 0.0452\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0439 - val_loss: 0.0434\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0423 - val_loss: 0.0438\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0419 - val_loss: 0.0428\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0418 - val_loss: 0.0421\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0403 - val_loss: 0.0434\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0401 - val_loss: 0.0411\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0387 - val_loss: 0.0410\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0380 - val_loss: 0.0403\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0377 - val_loss: 0.0408\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 0.0374 - val_loss: 0.0395\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0370 - val_loss: 0.0390\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0357 - val_loss: 0.0408\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0362 - val_loss: 0.0384\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0344 - val_loss: 0.0393\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0342 - val_loss: 0.0379\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0339 - val_loss: 0.0387\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0331 - val_loss: 0.0382\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0334 - val_loss: 0.0411\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0335 - val_loss: 0.0384\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0324 - val_loss: 0.0384\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0313 - val_loss: 0.0397\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0315 - val_loss: 0.0381\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0311 - val_loss: 0.0391\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 0.0304 - val_loss: 0.0372\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0297 - val_loss: 0.0377\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0290 - val_loss: 0.0386\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0288 - val_loss: 0.0383\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0283 - val_loss: 0.0376\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 109ms/step - loss: 0.0286 - val_loss: 0.0391\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0285 - val_loss: 0.0377\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0280 - val_loss: 0.0383\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0273 - val_loss: 0.0397\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0285 - val_loss: 0.0365\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0272 - val_loss: 0.0383\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 0.0257 - val_loss: 0.0394\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0260 - val_loss: 0.0389\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0257 - val_loss: 0.0403\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0269 - val_loss: 0.0378\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0245 - val_loss: 0.0378\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0251 - val_loss: 0.0388\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 0.0243 - val_loss: 0.0375\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0256 - val_loss: 0.0368\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0245 - val_loss: 0.0379\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 0.0241 - val_loss: 0.0371\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0241 - val_loss: 0.0379\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0231 - val_loss: 0.0364\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0229 - val_loss: 0.0363\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0225 - val_loss: 0.0365\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 0.0220 - val_loss: 0.0362\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 0.0217 - val_loss: 0.0369\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0211 - val_loss: 0.0384\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0214 - val_loss: 0.0370\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0207 - val_loss: 0.0372\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 0.0205 - val_loss: 0.0398\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.0215 - val_loss: 0.0366\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0220 - val_loss: 0.0398\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 92ms/step - loss: 0.0220 - val_loss: 0.0375\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0215 - val_loss: 0.0389\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 0.0213 - val_loss: 0.0394\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0215 - val_loss: 0.0405\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 0.0218 - val_loss: 0.0388\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0207 - val_loss: 0.0374\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 0.0210 - val_loss: 0.0411\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0218 - val_loss: 0.0400\n"
     ]
    }
   ],
   "source": [
    "Softmax_history = Softmax_model.fit(train_x, train_y, batch_size=128, epochs=500, validation_split=0.2, verbose=1, callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[CRF]\n",
    "[All]\n",
    "[train_text_emb] \n",
    "[0]: epo165 loss: 16.5179 - val_loss: 20.5074\n",
    "[1]: epo183 loss: 13.5592 - val_loss: 16.4887\n",
    "[train_info_concat]\n",
    "[0]: epo153 loss: 10.0395 - val_loss: 14.2559\n",
    "[1]: epo128 loss: 10.1085 - val_loss: 14.6777\n",
    "[train_tag_emb]\n",
    "[0]: epo126 loss: 6.9019 - val_loss: 15.7222\n",
    "[train_tag_info_concat]\n",
    "[0]: epo114 loss: 5.3638 - val_loss: 12.2225\n",
    "[1]: epo116 loss: 4.9694 - val_loss: 11.9932\n",
    "[train_tag_two_info_concat]\n",
    "[0]: epo134 loss: 9.6451 - val_loss: 12.6221\n",
    "[1]: epo185 loss: 6.7996 - val_loss: 12.2248\n",
    "\n",
    "[TagOnly]\n",
    "[train_text_emb] \n",
    "[0]: epo185 loss: 34.0454 - val_loss: 38.6243\n",
    "[1]: epo149 loss: 26.8772 - val_loss: 30.1547\n",
    "[2]: epo167 loss: 25.4310 - val_loss: 31.7712\n",
    "[train_tag_info_concat]\n",
    "[0]: epo147 loss: 8.5670 - val_loss: 22.5889\n",
    "[1]: epo139 loss: 9.2346 - val_loss: 22.0598\n",
    "[train_tag_two_info_concat]\n",
    "[0]: epo172 loss: 11.0723 - val_loss: 16.6598\n",
    "[1]: epo116 loss: 16.7534 - val_loss: 19.8089\n",
    "loss: 13.1938 - val_loss: 19.0995"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[Softmax]\n",
    "[All]\n",
    "[train_text_emb] \n",
    "\n",
    "[train_info_concat]\n",
    "[0]: epo100 loss: 0.0267 - val_loss: 0.0290\n",
    "[train_tag_emb]\n",
    "[train_tag_info_concat]\n",
    "[0]: epo117 loss: 0.0107 - val_loss: 0.0273\n",
    "[1]: epo119 loss: 0.0108 - val_loss: 0.0280\n",
    "[train_tag_two_info_concat]\n",
    "[0]: epo146 loss: 0.0186 - val_loss: 0.0246\n",
    "[1]: epo143 loss: 0.0183 - val_loss: 0.0255\n",
    "\n",
    "[TagOnly]\n",
    "[train_text_emb] \n",
    "[train_info_concat]\n",
    "[train_tag_emb]\n",
    "[train_tag_info_concat]\n",
    "loss: 0.0180 - val_loss: 0.0467\n",
    "loss: 0.0154 - val_loss: 0.0474\n",
    "[train_tag_two_info_concat]\n",
    "loss: 0.0253 - val_loss: 0.0442\n",
    "loss: 0.0205 - val_loss: 0.0411\n",
    "loss: 0.0203 - val_loss: 0.0375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupKfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.36 ms, sys: 0 ns, total: 2.36 ms\n",
      "Wall time: 2.37 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "groups = [get_domain(url) for url in urls]\n",
    "N_SPLITs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_group_dataset(X_data,y_data, groups, n_splits):\n",
    "\n",
    "    def gen():\n",
    "        for train_index, test_index in GroupKFold(n_splits).split(X_data, y_data, groups):\n",
    "            X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "            y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "            yield X_train,y_train,X_test,y_test\n",
    "\n",
    "    return tf.data.Dataset.from_generator(gen, (tf.float64,tf.float64,tf.float64,tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = make_group_dataset(train_x, train_y, groups, N_SPLITs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_cross_val(dataset):\n",
    "    count = 1\n",
    "    for X_train,y_train,X_test,y_test in dataset:\n",
    "        print(f\"Start fold {count}\")\n",
    "        earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "        model = get_BILSTM_SOFTMAX(X_train.shape[1:], num_tags)\n",
    "        history = model.fit(X_train, y_train, batch_size=32, epochs=500, validation_split=0.2, verbose=0, callbacks=[earlyStopping])\n",
    "        predict_y = model.predict(X_test)\n",
    "        predict_y = label_distribution_to_label(predict_y)\n",
    "        predict_y = [[idx2tag.get(lab) for lab in page] for page in predict_y]\n",
    "        y_test = [[idx2tag.get(lab) for lab in page] for page in y_test.numpy()]\n",
    "        evaluate_labels = ['PREV', 'PAGE', 'NEXT']\n",
    "        print(flat_classification_report(y_test, predict_y, labels=evaluate_labels, digits=len(evaluate_labels)))\n",
    "        count+=1\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on split train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predict_y = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predict_y.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predict_y = label_distribution_to_label(predict_y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predict_y = [[idx2tag.get(lab) for lab in page] for page in predict_y]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.array(predict_y).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_Y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_Y = [[idx2tag.get(lab) for lab in page] for page in test_Y]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.array(test_Y).shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "evaluate_labels = ['PREV', 'PAGE', 'NEXT']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(flat_classification_report(test_Y, predict_y, labels=evaluate_labels, digits=len(evaluate_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distribution_to_label(predict_y):\n",
    "    if len(predict_y.shape) != 3:\n",
    "        return predict_y\n",
    "    label_y = list()\n",
    "    for page in predict_y:\n",
    "        tmp = list()\n",
    "        for lab in page:\n",
    "            lab = lab.tolist()\n",
    "            tmp.append(lab.index(max(lab)))\n",
    "        label_y.append(tmp)\n",
    "    return label_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_raw, test_y = storage.get_test_Xy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_urls = [rec['url'] for rec in storage.iter_test_records()]\n",
    "test_groups = set([get_domain(url) for url in test_urls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups exist: mobile01\n",
      "Groups exist: musicarts\n"
     ]
    }
   ],
   "source": [
    "for group in test_groups:\n",
    "    if group in train_groups_set:\n",
    "        print(f\"Groups exist: {group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_test_x, chunks_test_y = get_chunks_data(test_X_raw, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_features, test_tag_features = get_token_tag_features_from_chunks(chunks_test_x)\n",
    "# test_tag_emb_features = token_features_bert_preprocessing(test_token_features, 'single')\n",
    "# test_tag_emb_features = token_features_bert_preprocessing(test_token_features, 'multi')\n",
    "test_tag_emb_features = token_features_bert_preprocessing(test_token_features, 'multi-two')\n",
    "test_tag_info_list = flatten_dicts_to_values(test_tag_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6e0d34e57e4ce3a18478442e3c610e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=42)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_text_list = [[data[\"text-exact\"] for data in page ] for page in test_token_features]\n",
    "test_text_emb = page_list_to_bert_embedding_list(test_text_list, emb_model, tokenizer, max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use custom Token: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06470f8e1de46e9a45de29f9c7511be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='(Init)', max=42)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_tag_emb = page_list_to_bert_embedding_list(test_tag_emb_features, emb_model, tokenizer, max_seq_length, Token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average:  3.170013315689047\n",
      "Max_node:  134\n"
     ]
    }
   ],
   "source": [
    "max_node = -1\n",
    "page_sum = 0\n",
    "for page in test_tag_emb_features:\n",
    "    sum = 0\n",
    "    for node in page:\n",
    "        sum+=len(node)\n",
    "        if len(node) > max_node:\n",
    "            max_node = len(node)\n",
    "    page_sum+=sum/len(page)\n",
    "print(\"Average: \", page_sum/len(train_token_features))\n",
    "print(\"Max_node: \",max_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tag_token = feature_pad_to_npdata(test_tag_feature_token_list)\n",
    "test_tag_x = feature_pad_to_npdata(test_tag_info_list)\n",
    "test_text_emb_x = feature_pad_to_npdata(test_tag_emb)\n",
    "test_info_x = np.concatenate([test_text_emb_x, test_tag_x], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_info_x\n",
    "# x_test = test_text_emb_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 512, 774)"
      ]
     },
     "execution_count": 823,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pad_sequences([[tag2idx.get(l) for l in lab] for lab in chunks_test_y], maxlen=max_page_seq, padding=\"post\", truncating=\"post\", value=tag2idx[\"[PAD]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_crf_y = CRF_model.predict(x_test)\n",
    "predict_softmax_y = Softmax_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_crf_y = label_distribution_to_label(predict_crf_y)\n",
    "predict_softmax_y = label_distribution_to_label(predict_softmax_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_crf_y = np.asarray([[idx2tag.get(lab) for lab in page] for page in predict_crf_y])\n",
    "predict_softmax_y = np.asarray([[idx2tag.get(lab) for lab in page] for page in predict_softmax_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = [[idx2tag.get(lab) for lab in page] for page in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_labels = ['PREV', 'PAGE', 'NEXT', '[PAD]', 'O']\n",
    "evaluate_labels = ['PREV', 'PAGE', 'NEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PREV      0.000     0.000     0.000        13\n",
      "        PAGE      0.778     0.724     0.750       145\n",
      "        NEXT      0.667     0.074     0.133        27\n",
      "\n",
      "   micro avg      0.775     0.578     0.663       185\n",
      "   macro avg      0.481     0.266     0.294       185\n",
      "weighted avg      0.707     0.578     0.607       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(flat_classification_report(y_test, predict_crf_y, labels=evaluate_labels, digits=len(evaluate_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        PREV      0.000     0.000     0.000        13\n",
      "        PAGE      0.792     0.786     0.789       145\n",
      "        NEXT      1.000     0.074     0.138        27\n",
      "\n",
      "   micro avg      0.789     0.627     0.699       185\n",
      "   macro avg      0.597     0.287     0.309       185\n",
      "weighted avg      0.766     0.627     0.638       185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(flat_classification_report(y_test, predict_softmax_y, labels=evaluate_labels, digits=len(evaluate_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
